[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Notes",
    "section": "",
    "text": "Marginal Effects: A long-form document on using marginal effects (marginal means and marginal slopes) to improve interpretation of regression coefficients, especially in the presence of an interaction. Presents as a Rosetta stone for Stata’s margins command and R. Software: Stata and R.\nBenjamini–Hochberg Procedure: An interactive calculator to apply the Benjamini–Hochberg multiple comparison correction for a series of p-values.\nInterpreting log transformations in regression: Interpreting coefficients from a linear regression model (or linear mixed model) when the predictor and/or the outcome have been log transformed. Software: R, though concepts could transfer to other software.\nRandom intercepts in SPSS’s Mixed Model: There are two ways of specifying random intercepts in SPSS’s Mixed Model; this discusses their equivalence. Software: SPSS.\nModeration and Mediation via Regression: Although moderation and mediation typically arise in SEM/path analysis frameworks, moderation can be addressed in regression, and mediation can be conceptualized through regression.\nIRR and ICC: Some notes on IRR versus ICC, as well as how to obtain the ICC. Software: R, though concepts could transfer to other software.\nThe issue of collinear predictors: A visualization to see the potential negative effect of including highly collinear variables in a model. Software: R, though concepts could transfer to other software.\nStata, xt versus mixed model: Econometricians usually use the xt framework to address repeated measures. This document shows how to fit the equivalent of fixed effects regression, between effects regression, and random effects regression using linear regression (regress) and linear mixed models (mixed). Software: Stata.\nNested versus crossed random effects: Multiple random effects in a mixed model are typically defined as either “nested” or “crossed”. This document shows that this is a false dichotomy (nested random effects aren’t real!), as well as showing some nice visualization. Software: R, though concepts could transfer to other software.\nLinear splines versus interactions: Models such as interrupted time series or diff-in-diff are often considered special analyses. This document attempts to show that these are just regression models with particular interactions. Software: Stata, though concepts could transfer to other software.\nSelecting a random subset of the data, potentially within subgroups: An easy way to generate a random sample of your data of arbitrary size, including a stratified approach. Software: Stata, though concepts could transfer to other software.\nMediation with Survey data: To fit a mediation model in Stata using complex survey data requires using gsem. Unfortunately, the gsem command does not support directly estimating direct, indirect and total effects. This documents how to compute them. Software: Stata.\nThe “Divide by 4” rule: An additional tool in interpreting logistic regression coefficients is the “Divide by 4” rule. Software: Stata, though concepts could transfer to other software.\nResponse Surface Model Plotting: An example of visualization after running a “response surface model”, aka just regression with an interaction. Software: R.\nModifying built-in-Stata commands: A fun story about hacking one of Stata’s shipped-with commands to make some code work. A story rather than a guide or instructions. Software: Stata."
  },
  {
    "objectID": "divideby4.html",
    "href": "divideby4.html",
    "title": "The “Divide by 4” Rule",
    "section": "",
    "text": "In addition to the odds ratio interpretation of coefficients in a logistic regression model, the “divide by 4” rule can also help with interpretation.\n. sysuse auto\n(1978 automobile data)\n\n. logit foreign mpg\n\nIteration 0:  Log likelihood =  -45.03321  \nIteration 1:  Log likelihood = -39.380959  \nIteration 2:  Log likelihood = -39.288802  \nIteration 3:  Log likelihood =  -39.28864  \nIteration 4:  Log likelihood =  -39.28864  \n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(1)    =  11.49\n                                                        Prob &gt; chi2   = 0.0007\nLog likelihood = -39.28864                              Pseudo R2     = 0.1276\n\n------------------------------------------------------------------------------\n     foreign | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |   .1597621   .0525876     3.04   0.002     .0566922     .262832\n       _cons |  -4.378866   1.211295    -3.62   0.000    -6.752961   -2.004771\n------------------------------------------------------------------------------\nThe coefficient on mpg, .1597, is the log odds. The “divide by 4” rule says that the log odds divided by 4 (so \\(.1597/4 = .0399\\)) is the maximum difference in predicted probability for a 1-unit change in mpg.\nIn other words, consider the following plot of predicted probabilities.\n. quiet margins, at(mpg = (12(1)41)) nose\n\n. marginsplot, ///\n&gt;     recast(line) ///\n&gt;     addplot(pci .5 12 .5 41, lpattern(shortdash) lcolor(black)  || ///\n&gt;             pci 0 `=4.378866/.1597621' 1 `=4.378866/.1597621', ///\n&gt;             lpattern(shortdash) lcolor(black)) ///\n&gt;     legend(off)\n\nVariables that uniquely identify margins: mpg\n\nThe line represents the predicted probability of the outcome being 1 as mpg varies. In linear regression, this would be a straight line, and the difference in predicted probability between mpg = 14 versus mpg = 15 would be equivalent to the difference in predicted probability between mpg = 30 versus mpg = 31. However, since this is a logistic curve, the difference in predicted probability varies over mpg, being sharpest near the middle. Specifically, at the point when the predicted probability is exactly .5 (the dashed lines), the slope is the largest, and it recedes from there.\nThis means that at that middle point (around mpg = 27), the increasing mpg by 1 is increasing the predicted probability by .0399. For all other places on that curve, the increased probability is less than .0399."
  },
  {
    "objectID": "iccirr.html",
    "href": "iccirr.html",
    "title": "Inter-rater Reliability",
    "section": "",
    "text": "Cohen’s \\(\\kappa\\)\nCohen’s \\(\\kappa\\) can be used for agreement between two raters on categorical data. The basic calculation is\n\\[\n  \\kappa = \\frac{p_a - p_e}{1 - p_e},\n\\]\nwhere \\(p_a\\) is the percentage observed agreement and \\(p_e\\) is the percentage expected agreement by chance. Therefore \\(\\kappa\\) is what percentage of the agreement over chance is observed.\nFleiss’ \\(\\kappa\\) is an extension to more than two raters and has a similar form.\nA major flaw in either \\(\\kappa\\) is that for ordinal data, any disagreement is treated equal. E.g. on a Likert scale, ratings of 4 and 5 are just as disagreeable as ratings of 1 and 5. Weighted \\(\\kappa\\) addresses this by including a weight matrix which can be used to provide levels of disagreement.\nICC is used for continuous measurements. It can be used in place of weighted \\(\\kappa\\) with ordinal variables of course. The basic calculation is\n\\[\n  ICC = \\frac{\\sigma^2_w}{\\sigma^2_w + \\sigma^2_b},\n\\]\nwhere \\(\\sigma^2_w\\) and \\(\\sigma^2_b\\) represent within- and between- rater variability respectively. Since the denominator is the total variance of all ratings regardless of rater, this fraction represents the percent of total variation accounted for by within-variation.\nThe modern way to estimate the ICC is by a mixed model, extracting the \\(\\sigma\\)’s that are needed."
  },
  {
    "objectID": "iccirr.html#sources",
    "href": "iccirr.html#sources",
    "title": "Inter-rater Reliability",
    "section": "Sources",
    "text": "Sources\n\nhttps://en.wikipedia.org/wiki/Cohen%27s_kappa\nhttp://john-uebersax.com/stat/kappa.htm\nhttp://www.stata.com/manuals14/rkappa.pdf"
  },
  {
    "objectID": "iccirr.html#icc-in-r",
    "href": "iccirr.html#icc-in-r",
    "title": "Inter-rater Reliability",
    "section": "ICC in R",
    "text": "ICC in R\nUse the Orthodont data from nlme as our example. Look at distance measurements and look at correlation by Subject.\n\nlibrary(nlme)\nlibrary(lme4)\ndata(Orthodont)\n\n\nWith nlme\nUsing the nlme package, we fit the model:\n\nfm1 &lt;- lme(distance ~ 1, random = ~ 1 | Subject, data = Orthodont)\nsummary(fm1)\n\nLinear mixed-effects model fit by REML\n  Data: Orthodont \n       AIC      BIC    logLik\n  521.3618 529.3803 -257.6809\n\nRandom effects:\n Formula: ~1 | Subject\n        (Intercept) Residual\nStdDev:    1.937002 2.220312\n\nFixed effects:  distance ~ 1 \n               Value Std.Error DF  t-value p-value\n(Intercept) 24.02315 0.4296606 81 55.91192       0\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-3.2400448 -0.5277439 -0.1072888  0.4731815  2.7687301 \n\nNumber of Observations: 108\nNumber of Groups: 27 \n\n\nThe between-effect standard deviation is reported as the Residual StdDev. To obtain the ICC, we compute each \\(\\sigma\\):\n\ns2w &lt;- getVarCov(fm1)[[1]]\ns2b &lt;- fm1$s^2\nc(sigma2_w = s2w, sigma2_b = s2b, icc = s2w/(s2w + s2b))\n\n sigma2_w  sigma2_b       icc \n3.7519762 4.9297832 0.4321677 \n\n\n\n\nWith lme4\nUsing the lme4 package, we fit the model:\n\nfm2 &lt;- lmer(distance ~ (1 | Subject), data = Orthodont)\nsummary(fm2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ (1 | Subject)\n   Data: Orthodont\n\nREML criterion at convergence: 515.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2400 -0.5277 -0.1073  0.4732  2.7687 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 3.752    1.937   \n Residual             4.930    2.220   \nNumber of obs: 108, groups:  Subject, 27\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  24.0231     0.4297   55.91\n\n\nThe Variance column of the Random Effects table gives the within-subject (Subject) and between-subject (Residual) variances.\n\ns2w &lt;- summary(fm2)$varcor$Subject[1]\ns2b &lt;- summary(fm2)$sigma^2\nc(sigma2_w = s2w, sigma2_b = s2b, icc = s2w/(s2w + s2b))\n\n sigma2_w  sigma2_b       icc \n3.7519736 4.9297839 0.4321675"
  },
  {
    "objectID": "iccirr.html#sources-1",
    "href": "iccirr.html#sources-1",
    "title": "Inter-rater Reliability",
    "section": "Sources",
    "text": "Sources\n\nhttps://en.wikipedia.org/wiki/Intraclass_correlation\nhttp://stats.stackexchange.com/questions/14976/intraclass-correlation-coefficients-icc-with-multiple-variables\nhttp://john-uebersax.com/stat/icc.htm"
  },
  {
    "objectID": "logitinterpretation.html",
    "href": "logitinterpretation.html",
    "title": "Logistic Models and the Margins Command",
    "section": "",
    "text": "Introduction\nWhen looking at the results of a logistic model, there are several different measures of the relationship between the predictors and the probability of a positive outcome that can be used to interpret the model:\nIf you are unclear which you are looking at, confusion can abound. This is doubly-confounded in Stata (in my opinion) where certain margins commands will produce a different measure than perhaps expected.\nLet’s start simple and consider a model with only an intercept. We’ll use the “auto” data set, and fit a model predicting the probability of a car being foreign made.\nThe default output from the logit command are the log odds, -.86 and passing the or option gives the odds ratio of .423. The conversion between these values is straightforward:\n\\[\n    \\textrm{log odds} = log(\\textrm{odds})\n\\] \\[\n    \\textrm{odds} = \\exp^{\\textrm{log odds}}\n\\]\nLet’s backtrack to see how we arrive at those values.\nFirst, let’s look at the breakdown of foreign and domestic cars.\nWe see that \\(P(\\textrm{foreign})\\) = .2973 and \\(P(\\textrm{domestic})\\) = .7027. We can convert these probabilities into odds, using the formula\n\\[\n    \\textrm{Odds}(\\textrm{foreign}) = \\frac{P(\\textrm{foreign})}{1 - P(\\textrm{foreign})}.\n\\]\nTherefore we can easily see that\n\\[\n    \\textrm{Odds}(\\textrm{foreign}) = \\frac{    .2973}{1 -     .2973} =     .4231\n\\]\nand\n\\[\n    \\textrm{Odds}(\\textrm{domestic}) = \\frac{    .7027}{1 -     .7027} =     2.364.\n\\]\nFor completeness, to convert from odds to probability you can use\n\\[\n    P(\\textrm{foreign}) = \\frac{\\textrm{Odds}(\\textrm{foreign})}{1 + \\textrm{Odds}(\\textrm{foreign})}.\n\\]\nNote that the odds of a car being foreign is exactly the result we saw above from logit, or. So in a logistic model with only an intercept, the coefficient on the intercept is the odds of a positive outcome.\nRather than calculate these manually, Stata can produce these automatically.\n\\(P(\\textrm{foreign})\\):\n\\(\\textrm{Odds}(\\textrm{foreign})\\):\nThe expression(exp(xb())) is a bit odd, but the easiest way to obtain what we need. Think of it as just saying “give me the odds”.\nIn the intercept only example, we had no concept of an odds ratio. Let’s add a fixed effect, in this case a binary predictor, which will require interpreting an odds ratio.\nMoving from a binary predictor to a categorical predictor is fairly straightforward; instead of a single odds ratio, we have two.\nNow let’s replace the categorical predictor with a continuous one. Again, most interpretations stay the same.\nNow instead of talking about probability or odds in a level of a categorical predictor, it is instead at a specific level of headroom. The intercept is the odds of having a positive outcome when the headroom is identically 0, which in this case, as is often the case, is not interesting.\nLet’s consider interactions now. We’ll interact two binary variables for each."
  },
  {
    "objectID": "logitinterpretation.html#probabilities",
    "href": "logitinterpretation.html#probabilities",
    "title": "Logistic Models and the Margins Command",
    "section": "Probabilities",
    "text": "Probabilities\nLet’s look at the probabilities. Here we have conditional probabilities since we have a predictor. So we are interested in \\(P(\\textrm{foreign} | \\textrm{high mileage})\\) and \\(P(\\textrm{foreign} | \\textrm{low mileage})\\).\nFrom the table above, we can easily compute this:\n. tab foreign highmileage, col\n\n+-------------------+\n| Key               |\n|-------------------|\n|     frequency     |\n| column percentage |\n+-------------------+\n\n           |      highmileage\nCar origin | Low Milea  High Mile |     Total\n-----------+----------------------+----------\n  Domestic |        45          7 |        52 \n           |     75.00      50.00 |     70.27 \n-----------+----------------------+----------\n   Foreign |        15          7 |        22 \n           |     25.00      50.00 |     29.73 \n-----------+----------------------+----------\n     Total |        60         14 |        74 \n           |    100.00     100.00 |    100.00 \nWe see \\(P(\\textrm{foreign} | \\textrm{high mileage}) =  .25\\) and \\(P(\\textrm{foreign} | \\textrm{low mileage}) = .5\\).\nWe can also obtain these via margins1:\n. margins highmileage\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n highmileage |\nLow Mileage  |        .25   .0559017     4.47   0.000     .1404347    .3595653\nHigh Mile..  |         .5   .1336306     3.74   0.000     .2380888    .7619112\n------------------------------------------------------------------------------\nWe can also test for equality between these percentages:\n. margins highmileage, pwcompare(pv)\n\nPairwise comparisons of adjusted predictions                Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n\n---------------------------------------------------------------------\n                             |            Delta-method    Unadjusted\n                             |   Contrast   std. err.      z    P&gt;|z|\n-----------------------------+---------------------------------------\n                 highmileage |\nHigh Mileage vs Low Mileage  |        .25   .1448521     1.73   0.084\n---------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#odds",
    "href": "logitinterpretation.html#odds",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds",
    "text": "Odds\nWe can compute the odds using the formulas above, giving us\n\\[\n    \\textrm{Odds}(\\textrm{foreign} | \\textrm{high mileage}) =         1\n\\]\nand\n\\[\n    \\textrm{Odds}(\\textrm{foreign} | \\textrm{low mileage}) = =     .3333.\n\\]\nTo obtain with margins, we again pass the expression option:\n. margins highmileage, expression(exp(xb()))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: exp(xb())\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n highmileage |\nLow Mileage  |   .3333333   .0993808     3.35   0.001     .1385505    .5281161\nHigh Mile..  |          1   .5345225     1.87   0.061    -.0476448    2.047645\n------------------------------------------------------------------------------\nNote that we do not want to test if the odds are different using pwcompare as that’s what the odds ratio is for!"
  },
  {
    "objectID": "logitinterpretation.html#odds-ratio",
    "href": "logitinterpretation.html#odds-ratio",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds ratio",
    "text": "Odds ratio\nThe odds ratio is often very confusing to interpret, but is straightforward: An odds ratio predicts the number of positive outcomes we expect to see for every negative outcome. So an odds ratio of 2 would mean for every domestic car, we’d expect to see 2 foreign cars. An odds ratio of .25 would mean for every domestic car, we’d expect .25 foreign cars - or, for every 4 domestic cars, we’d expect 1 foreign car (since .25 = 1/4).\nThe odds ratio is literally the ratio of the odds.\n\\[\n    \\textrm{OR}(\\textrm{foreign} | \\textrm{high mileage}) = \\frac{\\textrm{odds}(\\textrm{foreign} | \\textrm{high mileage})}{\\textrm{odds}(\\textrm{foreign} | \\textrm{low mileage})} = 1/.333 = 3\n\\]\nLooking at the regression results again:\n. logit, or\n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(1)    =   3.18\n                                                        Prob &gt; chi2   = 0.0746\nLog likelihood = -43.444169                             Pseudo R2     = 0.0353\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n highmileage |\nHigh Mile..  |          3   1.836145     1.79   0.073     .9039507    9.956295\n       _cons |   .3333333   .0993808    -3.68   0.000      .185823    .5979406\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\nThe intercept is \\(\\textrm{odds}(\\textrm{foreign} | \\textrm{low mileage})\\), the odds of a positive outcome in the baseline group, and the coefficient on highmileage is the odds ratio!\nNote that we cannot use the margins command to obtain the odds ratio2. Instead, we use lincom:\n. lincom _b[1.highmileage], or\n\n ( 1)  [foreign]1.highmileage = 0\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |          3   1.836145     1.79   0.073     .9039507    9.956295\n------------------------------------------------------------------------------\n(I obtained the _b[1.highmileage] name by running logit, coeflegend to obtain the legend.) Note the or option, without it we obtain the log odds."
  },
  {
    "objectID": "logitinterpretation.html#probabilities-1",
    "href": "logitinterpretation.html#probabilities-1",
    "title": "Logistic Models and the Margins Command",
    "section": "Probabilities",
    "text": "Probabilities\n. tab foreign pricecat, col\n\n+-------------------+\n| Key               |\n|-------------------|\n|     frequency     |\n| column percentage |\n+-------------------+\n\n           |             pricecat\nCar origin |         1          2          3 |     Total\n-----------+---------------------------------+----------\n  Domestic |        42          2          8 |        52 \n           |     71.19      40.00      80.00 |     70.27 \n-----------+---------------------------------+----------\n   Foreign |        17          3          2 |        22 \n           |     28.81      60.00      20.00 |     29.73 \n-----------+---------------------------------+----------\n     Total |        59          5         10 |        74 \n           |    100.00     100.00     100.00 |    100.00 \n. margins pricecat\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    pricecat |\n          1  |   .2881356   .0589618     4.89   0.000     .1725725    .4036987\n          2  |         .6    .219089     2.74   0.006     .1705934    1.029407\n          3  |         .2   .1264911     1.58   0.114     -.047918     .447918\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#odds-1",
    "href": "logitinterpretation.html#odds-1",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds",
    "text": "Odds\nThe intercept is the odds of a foreign car in pricecat 1, or .2881/.7119 = .4048. We can obtain the odds of each pricecat in the typical way.\n. margins pricecat, expression(exp(xb()))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: exp(xb())\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    pricecat |\n          1  |   .4047619   .1163527     3.48   0.001     .1767148     .632809\n          2  |        1.5   1.369306     1.10   0.273    -1.183791    4.183791\n          3  |        .25   .1976424     1.26   0.206    -.1373719    .6373719\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#odds-ratios",
    "href": "logitinterpretation.html#odds-ratios",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds ratios",
    "text": "Odds ratios\nFinally, the two coefficients in the model are the odds ratios of being in pricecat 2 or 3 versus 1. Again we can use lincom to obtain these.\n. lincom _b[2.pricecat], or\n\n ( 1)  [foreign]2.pricecat = 0\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |   3.705882   3.546757     1.37   0.171     .5678577    24.18487\n------------------------------------------------------------------------------\n\n. lincom _b[3.pricecat], or\n\n ( 1)  [foreign]3.pricecat = 0\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |   .6176471   .5195704    -0.57   0.567     .1187686    3.212026\n------------------------------------------------------------------------------\nNote here that multiplying the odds ratios by the odds in pricecat 1 (the intercept) gives the odds in the other group. E.g. 3.705*.4047 = 1.5."
  },
  {
    "objectID": "logitinterpretation.html#probabilities-2",
    "href": "logitinterpretation.html#probabilities-2",
    "title": "Logistic Models and the Margins Command",
    "section": "Probabilities",
    "text": "Probabilities\nWe cannot look at crosstabs as we did before the compute probabilities, but the margins command still works.\n. margins, at(headroom = (2.5 5))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n1._at: headroom = 2.5\n2._at: headroom =   5\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at |\n          1  |   .3674795   .0655664     5.60   0.000     .2389717    .4959873\n          2  |   .0659516    .050366     1.31   0.190    -.0327639     .164667\n------------------------------------------------------------------------------\nThese are the predicted probabilties of a positive outcome at the referenced levels of headroom, i.e. \\(P(\\textrm{foreign} | \\textrm{headroom} = 2.5)\\) and \\(P(\\textrm{foreign} | \\textrm{headroom} = 5)\\)."
  },
  {
    "objectID": "logitinterpretation.html#odds-2",
    "href": "logitinterpretation.html#odds-2",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds",
    "text": "Odds\nWe can directly compute the odds given the probabilities above, but it’s easier to continue using margins.\n. margins, at(headroom = (2.5 5)) expression(exp(xb()))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: exp(xb())\n1._at: headroom = 2.5\n2._at: headroom =   5\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at |\n          1  |   .5809764   .1638824     3.55   0.000     .2597729    .9021799\n          2  |   .0706083   .0577296     1.22   0.221    -.0425396    .1837562\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#odds-ratio-1",
    "href": "logitinterpretation.html#odds-ratio-1",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds ratio",
    "text": "Odds ratio\nThe coefficient in the logistic regression is interpreted as the odds ratio when increasing headroom by 1. In other words, if we had a collection of cars with headroom of \\(x\\) and magically change their headroom to \\(x + 1\\), we would expect for every one additional domestic car, we’d see &lt;&lt;dd_display: %9.4f exp(_b[headroom])&gt;&gt; additional foreign cars.\nWe can obtain this odds ratio by again using lincom.\n. lincom _b[headroom], or\n\n ( 1)  [foreign]headroom = 0\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |   .4304066   .1490474    -2.43   0.015     .2183295    .8484873\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#probabilities-3",
    "href": "logitinterpretation.html#probabilities-3",
    "title": "Logistic Models and the Margins Command",
    "section": "Probabilities",
    "text": "Probabilities\nBecause we have two categorical predictors, we can return to looking at crosstabs as a way of obtaining probabilities. The margins call will also return them.\n. table foreign highmileage highprice\n\n----------------------------------------------------\n                   |             highprice          \n                   |  Low Price   High Price   Total\n-------------------+--------------------------------\nCar origin         |                                \n  Domestic         |                                \n    highmileage    |                                \n      Low Mileage  |         23           22      45\n      High Mileage |          6            1       7\n      Total        |         29           23      52\n  Foreign          |                                \n    highmileage    |                                \n      Low Mileage  |          2           13      15\n      High Mileage |          6            1       7\n      Total        |          8           14      22\n  Total            |                                \n    highmileage    |                                \n      Low Mileage  |         25           35      60\n      High Mileage |         12            2      14\n      Total        |         37           37      74\n----------------------------------------------------\n\n. margins highprice#highmileage\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   highprice#|\n highmileage |\n  Low Price #|\nLow Mileage  |        .08   .0542586     1.47   0.140     -.026345     .186345\n  Low Price #|\nHigh Mile..  |         .5   .1443376     3.46   0.001     .2171036    .7828964\n High Price #|\nLow Mileage  |   .3714286   .0816735     4.55   0.000     .2113515    .5315056\n High Price #|\nHigh Mile..  |         .5   .3535534     1.41   0.157    -.1929519    1.192952\n------------------------------------------------------------------------------\nThis is similar to the categorical predictor, where there are four groups. For example, low price and low mileage, 2 out of 25 cars are foreign, so the probability is 2/25 = .08."
  },
  {
    "objectID": "logitinterpretation.html#odds-3",
    "href": "logitinterpretation.html#odds-3",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds",
    "text": "Odds\n. margins highprice#highmileage, expression(exp(xb()))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: exp(xb())\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   highprice#|\n highmileage |\n  Low Price #|\nLow Mileage  |   .0869565   .0641052     1.36   0.175    -.0386874    .2126004\n  Low Price #|\nHigh Mile..  |          1   .5773503     1.73   0.083    -.1315857    2.131586\n High Price #|\nLow Mileage  |   .5909091   .2067148     2.86   0.004     .1857554    .9960628\n High Price #|\nHigh Mile..  |          1   1.414214     0.71   0.480    -1.771808    3.771808\n------------------------------------------------------------------------------\nIf you look at the logistic results above, the baseline categories are low mileage and low price. So, as before, the intercept is the odds of a foreign car in that subcategory, which we see here.\nWe do not obtain the odds for the other categories in the regression output."
  },
  {
    "objectID": "logitinterpretation.html#odds-ratios-1",
    "href": "logitinterpretation.html#odds-ratios-1",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds ratios",
    "text": "Odds ratios\nThe odds ratios reported in the regression output only present part of the story. Let’s take a look at them again.\n. logit, or\n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(3)    =  10.54\n                                                        Prob &gt; chi2   = 0.0145\nLog likelihood = -39.763201                             Pseudo R2     = 0.1170\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   highprice |\n High Price  |   6.795455    5.54509     2.35   0.019     1.372897    33.63558\n             |\n highmileage |\nHigh Mile..  |       11.5    10.7684     2.61   0.009      1.83505    72.06888\n             |\n   highprice#|\n highmileage |\n High Price #|\nHigh Mile..  |   .1471572   .2548493    -1.11   0.269     .0049392    4.384364\n             |\n       _cons |   .0869565   .0641052    -3.31   0.001     .0205016    .3688215\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\nThe coefficient on highprice is the odds ratio of being foreign between high price and low price cars, in the low mileage category.\n\\[\n    \\frac{\\textrm{OR}(\\textrm{foreign}|\\textrm{high price, low mileage})}{\\textrm{OR}(\\textrm{foreign}|\\textrm{low price, low mileage})}\n\\]\nThe coefficient on highmileage is the odds ratio of being foreign between high mileage and low mileage cars, in the low price category.\n\\[\n    \\frac{\\textrm{OR}(\\textrm{foreign}|\\textrm{low price, high mileage})}{\\textrm{OR}(\\textrm{foreign}|\\textrm{low price, low mileage})}\n\\]\nThe interaction can be interpreted in one of two ways."
  },
  {
    "objectID": "logitinterpretation.html#footnotes",
    "href": "logitinterpretation.html#footnotes",
    "title": "Logistic Models and the Margins Command",
    "section": "",
    "text": "Note that if you do not flag highmileage as a categorical with i., you can use instead margins, over(highmileage). If you pass a continuous variable, it will compute the probability at each discrete value of the continuous variable.↩︎\nIf I’m wrong, please let me know!↩︎"
  },
  {
    "objectID": "logtransform.html",
    "href": "logtransform.html",
    "title": "Log Transform Interpretation",
    "section": "",
    "text": "Introduction\nLog transformations can be useful when a variable is very right-skewed, or multiplicative effects are desired over additive. However, interpretation can be challenging.\nWe are always discussing the natural log (ln), i.e. log base e.\nA 1 unit change in a predictor is associated with a \\(\\textrm{exp}(\\hat{\\beta})\\) mulitplicative change in \\(Y\\), or a \\(100(\\textrm{exp}(\\hat{\\beta}) - 1)\\%\\) change in \\(Y\\).\nExamples:\nA \\(k\\%\\) change in a predictor is associated with \\(\\hat{\\beta}\\log\\left(1 + \\frac{k}{100}\\right)\\) change in the outcome.\nExamples:\nA \\(k\\%\\) change in a predictor is associated with a \\(\\left(1 + \\frac{k}{100}\\right)^{\\hat{\\beta}}\\) multiplicative change in the outcome.\nExamples:"
  },
  {
    "objectID": "logtransform.html#multiplicative-vs-percent-change",
    "href": "logtransform.html#multiplicative-vs-percent-change",
    "title": "Log Transform Interpretation",
    "section": "Multiplicative vs Percent Change",
    "text": "Multiplicative vs Percent Change\nNote that multiplicative changes can be expressed as percent changes and vice-versa.\nIf we multiply \\(X\\) by 1.1, the resultant \\(1.1X\\) is 10% larger than \\(X\\). E.g. 16.5 is 10% larger than 15.\nIf we multiply \\(X\\) by .7, the resultant \\(.7X\\) is 30% lower than \\(X\\). E.g. 7 is 30% smaller than 10."
  },
  {
    "objectID": "logtransform.html#theory",
    "href": "logtransform.html#theory",
    "title": "Log Transform Interpretation",
    "section": "Theory",
    "text": "Theory\nAssume our regression equation is\n\\[\n  E(Y|X = x) = \\beta_0 + \\beta_1x.\n\\]\nIf we regress on the log of \\(Y\\) instead,\n\\[\n  E(\\log(Y)|X = x) = \\beta_0 + \\beta_1x.\n\\]\nBy Taylor expansion,\n\\[\n  \\log(E(X)) \\approx E(\\log(X)).\n\\]\nTherefore we can write \\[\\begin{align*}\n  E(Y|X = x + 1) & = \\textrm{exp}\\left(\\beta_0 + \\beta_1(x + 1)\\right) \\\\\n  & = \\textrm{exp}\\left(\\beta_0 + \\beta_1x + \\beta_1\\right) \\\\\n  & = \\textrm{exp}\\left(\\beta_0 + \\beta_1x\\right)\\textrm{exp}(\\beta_1) \\\\\n  & = E(Y|X = x)\\textrm{exp}(\\beta_1)\n\\end{align*}\\]"
  },
  {
    "objectID": "logtransform.html#example",
    "href": "logtransform.html#example",
    "title": "Log Transform Interpretation",
    "section": "Example",
    "text": "Example\n\ndata(mtcars)\n(m &lt;- lm(log(disp) ~ drat, data = mtcars))\n\n\nCall:\nlm(formula = log(disp) ~ drat, data = mtcars)\n\nCoefficients:\n(Intercept)         drat  \n     8.2782      -0.8323  \n\n\nTherefore a 1-unit increase in drat is associated with an \\(\\textrm{exp}(`r round(m\\)coef[2], 3)) = 0.435$ multiplicative change indisp`, corresponding to a 56.5% decrease.\nTo test this, we predict the ratio in predicted outcome with some values of drat, and that value increased by 1. Note: We exponentiate the predicted values to get them on the outcome scale.\n\nexp(predict(m, newdata = data.frame(drat = 5)))/exp(predict(m, newdata = data.frame(drat = 4)))\n\n        1 \n0.4350567 \n\nexp(predict(m, newdata = data.frame(drat = 30)))/exp(predict(m, newdata = data.frame(drat = 29)))\n\n        1 \n0.4350567"
  },
  {
    "objectID": "logtransform.html#theory-1",
    "href": "logtransform.html#theory-1",
    "title": "Log Transform Interpretation",
    "section": "Theory",
    "text": "Theory\nAssume our regression equation is\n\\[\n  E(Y|X = x) = \\beta_0 + \\beta_1x.\n\\]\nIf we include \\(\\log(X)\\) instead, we have\n\\[\n  E(Y|X = x) = \\beta_0 + \\beta_1\\log(x).\n\\]\nConsider when \\(X = cX\\) where \\(c\\) is some constant (e.g. 2 for a doubling of \\(X\\) or 1.3 for a 30% increase in \\(X\\)).\n\\[\n  E(Y|X = cx) = \\beta_0 + \\beta_1\\log(cx).\n\\]\nTherefore if we look at the difference in expectation,\n\\[\n  E(Y|X = cx) - E(Y|X = x)  = \\beta_1(\\log(cx) - \\log(x)) = \\beta_1\\log(c).\n\\]\n\nApproximation\nIf your percent change is small (e.g. a few percent) then you can approximate the change. This is because\n\\[\n  log(1 + x) \\approx x,\n\\]\nwhen \\(x\\) is close to 0. So to approximate what effect a 1% change in \\(X\\)` would have, simply multiple \\(\\hat{\\beta}\\) by that value; \\(0.1\\hat{\\beta}\\). This works reliably well up to \\(\\pm3\\%\\), moderately up to \\(\\pm5\\%\\) and gets much worse beyond that."
  },
  {
    "objectID": "logtransform.html#example-1",
    "href": "logtransform.html#example-1",
    "title": "Log Transform Interpretation",
    "section": "Example",
    "text": "Example\n\ndata(mtcars)\n(m &lt;- lm(drat ~ log(disp), data = mtcars))\n\n\nCall:\nlm(formula = drat ~ log(disp), data = mtcars)\n\nCoefficients:\n(Intercept)    log(disp)  \n     7.2301      -0.6875  \n\n\nTherefore a 25% increase in disp is associated with a \\(-0.688\\log(1.25) = -0.153\\) change in drat.\nTo test this, we predict the difference in predicted outcome with some values of disp, and that value increaed by 25%.\n\npredict(m, newdata = data.frame(disp = 5)) - predict(m, newdata = data.frame(disp = 5*1.25))\n\n        1 \n0.1534182 \n\npredict(m, newdata = data.frame(disp = 30)) - predict(m, newdata = data.frame(disp = 30*1.25))\n\n        1 \n0.1534182"
  },
  {
    "objectID": "logtransform.html#theory-2",
    "href": "logtransform.html#theory-2",
    "title": "Log Transform Interpretation",
    "section": "Theory",
    "text": "Theory\nTo-do."
  },
  {
    "objectID": "logtransform.html#example-2",
    "href": "logtransform.html#example-2",
    "title": "Log Transform Interpretation",
    "section": "Example",
    "text": "Example\n\ndata(mtcars)\n(m &lt;- lm(log(drat) ~ log(disp), data = mtcars))\n\n\nCall:\nlm(formula = log(drat) ~ log(disp), data = mtcars)\n\nCoefficients:\n(Intercept)    log(disp)  \n     2.2763      -0.1905  \n\n\nTherefore a 25% increase in disp is associated with a \\(1.25^{`r round(m\\)coef[2], 3)} = 0.958$ multiplicative change indrat`, corresponding to a 4.2% decrease.\nTo test this, we predict the difference in predicted outcome with some values of disp, and that value increaed by 25%.\n\npredict(m, newdata = data.frame(disp = 5)) - predict(m, newdata = data.frame(disp = 5*1.25))\n\n         1 \n0.04251857 \n\npredict(m, newdata = data.frame(disp = 30)) - predict(m, newdata = data.frame(disp = 30*1.25))\n\n         1 \n0.04251857"
  },
  {
    "objectID": "mixedModelsSPSS.html",
    "href": "mixedModelsSPSS.html",
    "title": "Two Ways of Specifying Random Intercepts in SPSS’s Mixed Model",
    "section": "",
    "text": "Introduction\nTo do: Screenshots of SPSS dialogues.\nIn SPSS’s Mixed Models dialogue, there are two ways to enter random intercepts, either by the Subjects and Repeated measures dialogue (the first window upon opening the dialogue) or the Random subdialogue. This document shows how to generate identical results using either option.\nWe’ll use R to get the data, demostrate the model we’re trying to fit, and export the data for SPSS. The data comes from Brady West.\nlibrary(tidyverse)\nratpup &lt;- readr::read_delim(\"http://www-personal.umich.edu/~bwest/rat_pup.dat\", \"\\t\")\nMake the treatment binary, and convert both it and sex to numeric.\nratpup &lt;- ratpup %&gt;%\n  mutate(treat = (treatment != \"Control\") + 0) %&gt;%\n  mutate(female = (sex == \"Female\") + 0) %&gt;%\n  select(-c(treatment, sex)) %&gt;%\n  as.data.frame()\nWrite the data into SPSS\nlibrary(foreign)\nwrite.foreign(ratpup, datafile = \"ratpup.sav\",\n              codefile = \"ratpup.sps\", package = \"SPSS\")\nThe mixed model we’ll be fitting has a random intercept per litter:\nlibrary(lme4)\nlmer(weight ~ treat*female + litsize + (1 | litter), data = ratpup, REML = TRUE)\nOpen the file ratpup.sps and run the code to load the data.\nWe’ll run two variations using the dialogue Analyze -&gt; Mixed Models -&gt; Linear.\nFirst, ignore the first screen. Place weight in Dependent variable; treat, female and litter into Factors, and litsize into Covariates. In the Fixed subdialogue, enter the main effect for litsize and the main and interaction of treat and female. In the Random subdialogue, place litter into the Model. Do not check the box “Include intercept”.\nThis dialogue should produce the following syntax:\nIn this model, litter was chosen as “subject” in the first screen. Random Intercept was included but not litter, and litter was included in “Subject groupings”\nNext, we’ll make use of the first screen. In the first screen, place little into Subjects. Enter the variables as described above.1 In the Random subdialogue, check “Include intercept”. Do not enter litter into Model, but at the bottom under Subject Groups, enter it into Combinations.\nThe syntax is:\nThe difference is\nversus\nbut both models are identical."
  },
  {
    "objectID": "mixedModelsSPSS.html#footnotes",
    "href": "mixedModelsSPSS.html#footnotes",
    "title": "Two Ways of Specifying Random Intercepts in SPSS’s Mixed Model",
    "section": "",
    "text": "litter does not have to be placed in Factors anymore, but it will not affect anything if it is.↩︎"
  },
  {
    "objectID": "moderationMediation.html",
    "href": "moderationMediation.html",
    "title": "Moderation and Mediation",
    "section": "",
    "text": "Introduction\nModeration and mediation are terms for measuring conditional effects. They are frequently discussed in the context of SEMs. However, both can be estimated in standard regression settings."
  },
  {
    "objectID": "moderationMediation.html#definition",
    "href": "moderationMediation.html#definition",
    "title": "Moderation and Mediation",
    "section": "Definition",
    "text": "Definition\nLet \\(X\\) be a predictor, \\(Y\\) be a response and we are studying whether \\(Z\\) moderates the relationship between \\(X\\) and \\(Y\\). In other words, we want to know whether \\(Z\\) modifies the strength (or direction) of the relationship between \\(X\\) and \\(Y\\). For example, we could study the relationship between socio-economic status (\\(X\\)) and frequency of breast exams (\\(Y\\)). Among younger women, breast exams are rare because current recommendations are for women age 40 and over to get exams. However, in older women, there is a relationship because those with higher SES tend to be more concerned with health. Age (\\(Z\\)) moderates the relationship between SES and frequency of breast exams."
  },
  {
    "objectID": "moderationMediation.html#in-regression",
    "href": "moderationMediation.html#in-regression",
    "title": "Moderation and Mediation",
    "section": "In Regression",
    "text": "In Regression\nModeration is represented in a regression model with nothing more than an interaction term. There is a causal aspect to it, in that there needs to be the assumption that the predictor (\\(X\\)) has a causal relationship on the response (\\(Y\\)). If \\(X\\) is randomized, this assumption is satisfied. If the \\(X\\) is not randomized, this assumption must be based on theory and domain specific knowledge.\nFit the interaction model,\n\\[\n  Y = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3XZ + \\epsilon.\n\\]\nTherefore, \\(\\beta_3\\) measures the moderation effect."
  },
  {
    "objectID": "moderationMediation.html#sources",
    "href": "moderationMediation.html#sources",
    "title": "Moderation and Mediation",
    "section": "Sources",
    "text": "Sources\nhttp://davidakenny.net/cm/moderation.htm"
  },
  {
    "objectID": "moderationMediation.html#definition-1",
    "href": "moderationMediation.html#definition-1",
    "title": "Moderation and Mediation",
    "section": "Definition",
    "text": "Definition\nAgain, let \\(X\\) be a predictor and \\(Y\\) be a response. We are studying whether \\(Z\\) mediates the relationship between \\(X\\) and \\(Y\\). Mediating variables are more complicated that moderators. A variable (\\(Z\\)) is said to be a mediator if it partially or fully explains the relationship between the predictor (\\(X\\)) and the response (\\(Y\\)). The easiest way to explain this is visually.\nThe unmediated and mediated model follow.\n\n\n\n\n\nIn the first model, the unmediated model, the total relationship between \\(X\\) and \\(Y\\) is captured by \\(c\\).\nIn the second model, the total effect is split into direct and indirect effects. \\(d\\) is the direct effect and \\(ab\\). If the direct effect (\\(d\\)) is zero, then the relationship is fully mediated. In other words, without the existence of \\(Z\\), there is no relationship between \\(X\\) and \\(Y\\).\nIf the indirect effect is zero (either \\(a\\) or \\(b\\) are zero), then there is no indirect effect and no mediation occurs.\nNote: If fitting with linear regression, \\(c = d + ab\\) and \\(c\\) is the total effect. Otherwise (e.g. mixed model, logistic, etc) that relationship may not be precise, so \\(d + ab\\) is a better estimate of the total effect."
  },
  {
    "objectID": "moderationMediation.html#in-regression-1",
    "href": "moderationMediation.html#in-regression-1",
    "title": "Moderation and Mediation",
    "section": "In Regression",
    "text": "In Regression\n\n\n\n\n\n\nImportant\n\n\n\nWhile we can fit mediation via regression (called the Baron-Kenny approach), it is not recommended. Use either SEM or the mediation package in R or causal mediation in Stata.\n\n\nTo test, we fit three regression models, testing all paths above.\n\nTest \\(c\\) to establish a relationship between predictor (\\(X\\)) and response (\\(Y\\)):\n\n\\[\nY = \\beta_0^{(1)} + cX + \\epsilon.\n\\]\n\nTest \\(a\\) to establish correlation between predictor (\\(X\\)) and moderator (\\(Z\\)):\n\n\\[\nZ = \\beta_0^{(2)} + aX + \\epsilon.\n\\]\n\nTest \\(b\\) to establish that the mediator (\\(Z\\)) affects the response (\\(Y\\)). Include both mediator (\\(Z\\)) and predictor (\\(X\\)) to establish that the relationship exists when controlling for \\(X\\).\n\n\\[\nY = \\beta_0^{(3)} + dX + bM + \\epsilon.\n\\]\nIf \\(c\\), \\(a\\) and \\(b\\) are shown to be significant in each step above, we have established partial mediation.\nIf additionally \\(d\\) is zero, we have established full mediation."
  },
  {
    "objectID": "moderationMediation.html#sources-1",
    "href": "moderationMediation.html#sources-1",
    "title": "Moderation and Mediation",
    "section": "Sources",
    "text": "Sources\nhttp://davidakenny.net/cm/mediate.htm\nhttp://data.library.virginia.edu/introduction-to-mediation-analysis/"
  },
  {
    "objectID": "modifying_stata_builtin.html",
    "href": "modifying_stata_builtin.html",
    "title": "Modifying a built-in Stata command",
    "section": "",
    "text": "I recently came across an issue in which modifying a shipped-with-Stata command was the easier way to address. I’m writing this to document the approach I took in case it comes up again and proves useful to remember the steps I took.\n(Note: If you see ... in Stata code snippets below, it indicates portions of code I excluded for brevity.)\n\nThe Problem\nA client was working with survival data and using the sts test command to show that the number of observed and expected events was similar. The sts test command produces a p-value, as shown below, but the journal the client was submitting to rightly prefers confidence intervals over p-values. So the client was hoping to bootstrap sts test to generate confidence intervals for the expected number of events. The problem was that sts test returns very little information and thus bootstrap wouldn’t work as normal (as it requires return’d or ereturn’d values to operate).\n. webuse stan3\n(Heart transplant data)\n\n. sts test posttran\n\n        Failure _d: died\n  Analysis time _t: t1\n       ID variable: id\n\nEquality of survivor functions\nLog-rank test\n\n         |  Observed       Expected\nposttran |    events         events\n---------+-------------------------\n       0 |        30          31.20\n       1 |        45          43.80\n---------+-------------------------\n   Total |        75          75.00\n\n                   chi2(1) =   0.13\n                   Pr&gt;chi2 = 0.7225\n\n. return list\n\nscalars:\n               r(chi2) =  .1261354821436887\n                 r(df) =  1\nWhile I could probably have figured out the math that Stata uses to calculate those expected events, it seemed probably easier and definitely more fun to hack the sts test command.\n\n\nFinding where to modify\nThe which command will tell you whether a command exists in an ado file, or is “built-in”, which cannot be modified as easily. Since the “test” of sts test is a subcommand, we can find where sts is defined.\n. which sts\n/Applications/Stata/ado/base/s/sts.ado\n*! version 8.8.0  23apr2022\nOpening that file, I quickly found the test subcommand:\nprogram define Test, rclass\n...\nSearching the file for “Expected” or “events” or any other text in the sts test output came up blank, so the actual calcuations must take place elsewhere.\nThe sts test command runs a variety of different tests, and I noticed that a good chunk of the code was dedicated to determining which test the user request, and setting the cmd macro.\n...\n    else if \"`tware'\"~=\"\" {\n        local cmd \"tware_st\"\n    }\n...\n    else if \"`peto'\"~=\"\" {\n        local cmd \"peto_st\"\n    }\n...\n    else    local cmd \"logrank\"\n...\nFinally, near the bottom of the command, the cmd macro is used:\n...\n    `vv' `cmd' _t _d `w' if `touse', strata(`strata') /*\n        */ t0(_t0) `id' `by' `options' `detail' `trend' `p' `q'\n...\nIgnore the vv macro (which handles version if applicable), it’s clear that the cmd macro must be an actual command. And the particular test the client was working with was the logrank test, therefore I was looking for the logrank command.\n. which logrank\n/Applications/Stata/ado/base/l/logrank.ado\n*! version 7.1.17  10nov2021\nInside I found the following code:\n...\n  di in smcl in gr _n _col(`len') `\" {c |}  Observed       Expected\"'\n    local pad = `len' - `len1'\n    if `\"`strata'\"'==`\"\"' { local dup `\"     events\"' }\n    else    local dup `\"     events*\"'\n    di in smcl in gr `\"`ttl'\"' _skip(`pad') `\"{c |}    events    `dup'\"'\n    di in smcl in gr \"{hline `len'}{c +}{hline 25}\"\n\n\n    local sum 0\n    local i 1\n    local gstr = (bsubstr(\"`:type `grp''\", 1, 3)==\"str\")\n    while `i' &lt;= _N {\n        if (`gstr') {\n            local x : di udsubstr(`grp'[`i'], 1, 255)\n        }\n        else {\n            local x = `grp'[`i']\n        }\n        local pad = `len' - udstrlen(`\"`x'\"')-1\n        di in smcl in gr _skip(`pad') `\"`x' \"' \"{c |}\" in ye /*\n            */ %10.0g `wo'[1,`i'] `\"     \"' %10.2f `w'[1,`i']\n        local sum = `sum' + `wo'[1,`i']\n        local i = `i' + 1\n    }\n    di in smcl in gr \"{hline `len'}{c +}{hline 25}\"\n        local pad = `len' - 6\n    di in smcl in gr _skip(`pad') `\"Total \"' `\"{c |}\"' in ye /*\n            */ %10.0g `sum' `\"     \"' %10.2f `sum'\n...\nThis is very confusing code to look at at first (for some reason all shipped-with-Stata code uses the shortest possible versions of the command names, making things even more obtuse) but we can tell that this is producing the table output we’re looking for. di is display, so each di line is printing something out. The first couple are printing “Observed”, “Expected”, and “events”, so that’s the head of the table, and the last di is printing “Total” which is the end of the table. The sts test (and logrank) command takes in a categorical variable and prints a row per level, so the while loop must be going through each level of the variable. Inside the loop, there is only a single di statement:\n...\n        di in smcl in gr _skip(`pad') `\"`x' \"' \"{c |}\" in ye /*\n            */ %10.0g `wo'[1,`i'] `\"     \"' %10.2f `w'[1,`i']\n...\nNote the reference to two matrix extractions: wo[1, i] and w[1, i]. These insert the observed (wo) and expected (w) number of events!\n\n\nMaking the modification\nSo ultimately, we just need to return the w matrix. Finding the other returns,\n...\n    ret scalar df = colsof(`w') - 1\n    ret scalar chi2 = `V'[1,1]\n...\nwe can add our own return that stores each expected value into a scalar:\n  matrix events=`w'\n  local i 1\n  while `i' &lt;= _N {\n    return scalar e`i' = events[1,`i']\n    local i = `i' + 1\n  }\nNow we can save this, re-open Stata, and it works!\n. webuse stan3\n(Heart transplant data)\n\n. bootstrap e1=r(e1) e2=r(e2), reps(10): sts test posttran\n(running sts on estimation sample)\n\nwarning: sts does not set e(sample), so no observations will be excluded from\n         the resampling because of missing values or other reasons. To\n         exclude observations, press Break, save the data, drop any\n         observations that are to be excluded, and rerun bootstrap.\n\nBootstrap replications (10): .........10 done\n\nBootstrap results                                          Number of obs = 172\n                                                           Replications  =  10\n\n      Command: sts test posttran\n           e1: r(e1)\n           e2: r(e2)\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          e1 |   31.19955   7.380891     4.23   0.000     16.73327    45.66583\n          e2 |   43.80045   5.779255     7.58   0.000     32.47332    55.12758\n------------------------------------------------------------------------------\n\n\nAn Added Complication\nThe client was working on a virtual Windows machine which did not have permission to overwrite the logrank.ado file. It is easy enough to make a copy of logrank.ado, rename logrank to logrank2 and treat it as a user-written ado file, but we’d have to also create a custom version of sts test which might be slightly messy due to it being a subcommand.\nInstead, we can use the sts.ado to figure out to get the comparable logrank command for a given sts test command.\n...\n    `vv' `cmd' _t _d `w' if `touse', strata(`strata') /*\n        */ t0(_t0) `id' `by' `options' `detail' `trend' `p' `q'\n...\nAs mentioned above, vv is just version control, so here we have some variables passed to logrank (_t, _d, and whatever is inside w), and a bunch of options. We can modify our version of logrank to print out all these, to determine what is actually being passed.\nprogram define logrank /* timevar [deadvar] [, by(group) t0(t0) id(tvid)] */, rclass\n    version 6.0, missing\n    syntax varlist(min=1 max=2) [if] [in] [fw iw] [, /*\n        */ BY(varlist) CHECK Detail ID(varname) LOGRANK /*\n        */ MAT(string) T0(varname) noTItle /*\n        */ STrata(varlist) TVid(varname) trend DINOTE]\n\ndisplay \"varlist: `varlist'\"\ndisplay \"t0: `t0'\"\ndisplay \"id: `id'\"\netc...\nUltimately it ended up that the command was:\n. logrank _t _d, by(posttran) id(id) t0(_t0)\n\nEquality of survivor functions\nLog-rank test\n\n         |  Observed       Expected\nposttran |    events         events\n---------+-------------------------\n       0 |        30          31.20\n       1 |        45          43.80\n---------+-------------------------\n   Total |        75          75.00\n\n                   chi2(1) =   0.13\n                   Pr&gt;chi2 = 0.7225"
  },
  {
    "objectID": "poissonapprox.html",
    "href": "poissonapprox.html",
    "title": "When can Poisson Regression approximate Logistic?",
    "section": "",
    "text": "An older idea in Epidemiology is to use a Poisson regression model in place of a logistic regression model. This idea has some validity because with a low mean, the Poisson distribution approximates the binary distribution.\n\nSimulation\nLet’s examine this. First, we define a program which generates some data according to a logistic model, then fits both logistic and Poisson regression models against it.\nThis program, defined below, takes in three arguments\n\nn - Sample size\np - Baseline probability of success\nb1 - Coefficient of interest.\n\nThe model is simply\n\\[\n    logit(P(Y = 1 | X)) = logit(p) + b_1x\n\\]\n. program def binsim, rclass\n  1.     drop _all\n  2.     args n p b1\n  3.     set obs `n'\n  4.     gen x = rnormal()\n  5.     gen y = rbinomial(1, invlogit(logit(`p') + `b1'*x))\n  6.     * Return P(success) to ensure everything is working\n.     mean y\n  7.     mat b = e(b)\n  8.     scalar pp = b[1,1]\n  9.     return scalar pp=pp\n 10. \n.     * Poisson model\n.     poisson y x\n 11.     mat b = e(b)\n 12.     scalar b_pois = b[1,1]\n 13.     return scalar b_pois=b_pois\n 14. \n.     * Logistic model\n.     logistic y x\n 15.     mat b = e(b)\n 16.     scalar b_logit = b[1,1]\n 17.     return scalar b_logit=b_logit\n 18. end\n\n\nResults\nNow we can run it with a few different settings. Specifically, we’re interested in how close to 0 the mean must be for the Poisson coefficient to approximate the logistic coefficient.\nSet a few parameters\n. local beta1 .4\n\n. local reps 1000\n\n10% success5% success3% success1% success\n\n\n. simulate pp=r(pp) b_pois=r(b_pois) b_logit=r(b_logit), ///\n&gt;     reps(`reps') nodots: binsim 10000 .1 `beta1'\n\n      Command: binsim 10000 .1 .4\n           pp: r(pp)\n       b_pois: r(b_pois)\n      b_logit: r(b_logit)\n\nFirst we’ll ensure the code is working and that P(success) is 10% as requested.\n. mean pp\n\nMean estimation                          Number of obs = 1,000\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n          pp |   .1057365   .0000978      .1055445    .1059285\n--------------------------------------------------------------\nNow we can look at the kernel densities\n. twoway kdensity b_logit || kdensity b_pois, ///\n&gt;     xline(`beta1') legend(label(1 \"Logistic\") label(2 \"Poisson\"))\n\nThe Poisson coefficient is strongly negatively biased. We can estimate the bias as a percent of the true coefficient.\n. gen error = abs(b_logit - b_pois)/b_logit\n\n. mean error\n\nMean estimation                          Number of obs = 1,000\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       error |   .1195278   .0001314      .1192699    .1197857\n--------------------------------------------------------------\n\n\n. simulate pp=r(pp) b_pois=r(b_pois) b_logit=r(b_logit), ///\n&gt;     reps(`reps') nodots: binsim 10000 .05 `beta1'\n\n      Command: binsim 10000 .05 .4\n           pp: r(pp)\n       b_pois: r(b_pois)\n      b_logit: r(b_logit)\n\n\n. mean pp\n\nMean estimation                          Number of obs = 1,000\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n          pp |   .0535497   .0000704      .0534115    .0536879\n--------------------------------------------------------------\n\n. twoway kdensity b_logit || kdensity b_pois, ///\n&gt;     xline(`beta1') legend(label(1 \"Logistic\") label(2 \"Poisson\"))\n\n. gen error = abs(b_logit - b_pois)/b_logit\n\n. mean error\n\nMean estimation                          Number of obs = 1,000\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       error |   .0616367   .0001023      .0614361    .0618374\n--------------------------------------------------------------\n\nStill see a negative bias.\n\n\n. simulate pp=r(pp) b_pois=r(b_pois) b_logit=r(b_logit), ///\n&gt;     reps(`reps') nodots: binsim 10000 .03 `beta1'\n\n      Command: binsim 10000 .03 .4\n           pp: r(pp)\n       b_pois: r(b_pois)\n      b_logit: r(b_logit)\n\n\n. mean pp\n\nMean estimation                          Number of obs = 1,000\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n          pp |   .0322263   .0000573      .0321139    .0323387\n--------------------------------------------------------------\n\n. twoway kdensity b_logit || kdensity b_pois, ///\n&gt;     xline(`beta1') legend(label(1 \"Logistic\") label(2 \"Poisson\"))\n\n. gen error = abs(b_logit - b_pois)/b_logit\n\n. mean error\n\nMean estimation                          Number of obs = 1,000\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       error |   .0375324   .0000839      .0373677    .0376971\n--------------------------------------------------------------\n\nThe bias is minimal but still present.\n\n\n. simulate pp=r(pp) b_pois=r(b_pois) b_logit=r(b_logit), ///\n&gt;     reps(`reps') nodots: binsim 10000 .01 `beta1'\n\n      Command: binsim 10000 .01 .4\n           pp: r(pp)\n       b_pois: r(b_pois)\n      b_logit: r(b_logit)\n\n\n. mean pp\n\nMean estimation                          Number of obs = 1,000\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n          pp |   .0108073   .0000342      .0107402    .0108744\n--------------------------------------------------------------\n\n. twoway kdensity b_logit || kdensity b_pois, ///\n&gt;     xline(`beta1') legend(label(1 \"Logistic\") label(2 \"Poisson\"))\n\n. gen error = abs(b_logit - b_pois)/b_logit\n\n. mean error\n\nMean estimation                          Number of obs = 1,000\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       error |   .0127062   .0000505      .0126072    .0128053\n--------------------------------------------------------------\n\nThe bias has all but disappeared.\n\n\n\n\n\nConclusion\nI wouldn’t recommend using Poisson over Logistic unless P(Success) was 1% or less.\nThere’s an interesting artifact to explore, namely that the percent bias is consistently about 15-20% larger than the true beta."
  },
  {
    "objectID": "randomselection.html",
    "href": "randomselection.html",
    "title": "Choosing a Random Sample",
    "section": "",
    "text": "Introduction\nA common task is to select a random subset of rows from your data set. This document discusses an easy way to do this, including sampling from subsamples.\n\n\nData\nWe’ll load up the “auto” data set and shrink it down substantially in order to be able to print out the results.\n. sysuse auto\n(1978 automobile data)\n\n. keep make foreign\n\n. bysort foreign: gen row = _n\n\n. keep if row &lt;= 4\n(66 observations deleted)\n\n. drop row\n\n. list, sep(0)\n\n     +--------------------------+\n     | make             foreign |\n     |--------------------------|\n  1. | AMC Concord     Domestic |\n  2. | AMC Pacer       Domestic |\n  3. | AMC Spirit      Domestic |\n  4. | Buick Century   Domestic |\n  5. | Audi 5000        Foreign |\n  6. | Audi Fox         Foreign |\n  7. | BMW 320i         Foreign |\n  8. | Datsun 200       Foreign |\n     +--------------------------+\n\n\nSimple Random Sample\nLet’s say we want to select 4 rows, as a simple random sample. That is, the probability of any row being included in the sample is equal.\nFirst, we’ll generate a random number per row. You can use any distribution you want; uniform or normal are common.\n. generate rand = rnormal()\n\n. list, sep(0)\n\n     +--------------------------------------+\n     | make             foreign        rand |\n     |--------------------------------------|\n  1. | AMC Concord     Domestic   -.4705035 |\n  2. | AMC Pacer       Domestic   -.3938664 |\n  3. | AMC Spirit      Domestic   -.2524172 |\n  4. | Buick Century   Domestic   -1.404408 |\n  5. | Audi 5000        Foreign   -.8082101 |\n  6. | Audi Fox         Foreign   -.0387205 |\n  7. | BMW 320i         Foreign    1.185362 |\n  8. | Datsun 200       Foreign   -.2958094 |\n     +--------------------------------------+\nrnormal takes in 2 optional arguments of a mean and standard deviation; the defaults are 0 and 1 respectively.\nIf you prefer uniform, you call generate rand = runiform(a, b) where a and b are upper and lower bounds, e.g. generate rand = runiform(0, 1).\nNow we simply sort by this new variable.\n. sort rand\n\n. list, sep(0)\n\n     +--------------------------------------+\n     | make             foreign        rand |\n     |--------------------------------------|\n  1. | Buick Century   Domestic   -1.404408 |\n  2. | Audi 5000        Foreign   -.8082101 |\n  3. | AMC Concord     Domestic   -.4705035 |\n  4. | AMC Pacer       Domestic   -.3938664 |\n  5. | Datsun 200       Foreign   -.2958094 |\n  6. | AMC Spirit      Domestic   -.2524172 |\n  7. | Audi Fox         Foreign   -.0387205 |\n  8. | BMW 320i         Foreign    1.185362 |\n     +--------------------------------------+\nFinally, we can identify our sample.\n. gen insample = _n &lt;= 4\n\n. list, sep(0)\n\n     +-------------------------------------------------+\n     | make             foreign        rand   insample |\n     |-------------------------------------------------|\n  1. | Buick Century   Domestic   -1.404408          1 |\n  2. | Audi 5000        Foreign   -.8082101          1 |\n  3. | AMC Concord     Domestic   -.4705035          1 |\n  4. | AMC Pacer       Domestic   -.3938664          1 |\n  5. | Datsun 200       Foreign   -.2958094          0 |\n  6. | AMC Spirit      Domestic   -.2524172          0 |\n  7. | Audi Fox         Foreign   -.0387205          0 |\n  8. | BMW 320i         Foreign    1.185362          0 |\n     +-------------------------------------------------+\nRecall that _n refers to the current row number, so this is just flagging all rows 4 and below!\n\n\nSample by Subgroup\nConsider the sample we obtained above, and notice that we sampled 3 domestic cars and 1 foreign car. Since it was a simple random sample, that split is random; we could have just as easily obtained all foreign cars or any other combination. Perhaps we want to force some balance, for example, that our random sample is exactly 2 foreign and 2 domestic.\nWe’ll generate a new random number first just as before.\n. drop rand insample\n\n. generate rand = rnormal()\n\n. list, sep(0)\n\n     +--------------------------------------+\n     | make             foreign        rand |\n     |--------------------------------------|\n  1. | Buick Century   Domestic   -1.179235 |\n  2. | Audi 5000        Foreign    1.503948 |\n  3. | AMC Concord     Domestic    .0767283 |\n  4. | AMC Pacer       Domestic    -.627642 |\n  5. | Datsun 200       Foreign   -1.122534 |\n  6. | AMC Spirit      Domestic   -1.491838 |\n  7. | Audi Fox         Foreign    .0291835 |\n  8. | BMW 320i         Foreign   -.7714012 |\n     +--------------------------------------+\nNow when we sort, we’ll sort by foreign first.\n. sort foreign rand\n\n. list, sep(0)\n\n     +--------------------------------------+\n     | make             foreign        rand |\n     |--------------------------------------|\n  1. | AMC Spirit      Domestic   -1.491838 |\n  2. | Buick Century   Domestic   -1.179235 |\n  3. | AMC Pacer       Domestic    -.627642 |\n  4. | AMC Concord     Domestic    .0767283 |\n  5. | Datsun 200       Foreign   -1.122534 |\n  6. | BMW 320i         Foreign   -.7714012 |\n  7. | Audi Fox         Foreign    .0291835 |\n  8. | Audi 5000        Foreign    1.503948 |\n     +--------------------------------------+\nSo we have two separate randomly sorted lists here. To select a fixed number from each, we can use the bysort prefix.\n. bysort foreign (rand): gen rownumber = _n\n\n. gen insample = rownumber &lt;= 2\n\n. list, sep(0)\n\n     +------------------------------------------------------------+\n     | make             foreign        rand   rownum~r   insample |\n     |------------------------------------------------------------|\n  1. | AMC Spirit      Domestic   -1.491838          1          1 |\n  2. | Buick Century   Domestic   -1.179235          2          1 |\n  3. | AMC Pacer       Domestic    -.627642          3          0 |\n  4. | AMC Concord     Domestic    .0767283          4          0 |\n  5. | Datsun 200       Foreign   -1.122534          1          1 |\n  6. | BMW 320i         Foreign   -.7714012          2          1 |\n  7. | Audi Fox         Foreign    .0291835          3          0 |\n  8. | Audi 5000        Foreign    1.503948          4          0 |\n     +------------------------------------------------------------+\n(Recall that when calling bysort, any argument in parentheses is used for sorting, not for by’ing. Since I sorted by foreign and rand above I probably could have just used the prefix by foreign:, however, I prefer always using bysort with full sorting just to avoid any issues.)\nWe could have also enforced an unequal split in foreign:\n. gen insample2 = rownumber &lt;= 3 if foreign == 0\n(4 missing values generated)\n\n. replace insample2 = rownumber &lt;= 1 if foreign == 1\n(4 real changes made)\n\n. list, sep(0)\n\n     +-----------------------------------------------------------------------+\n     | make             foreign        rand   rownum~r   insample   insamp~2 |\n     |-----------------------------------------------------------------------|\n  1. | AMC Spirit      Domestic   -1.491838          1          1          1 |\n  2. | Buick Century   Domestic   -1.179235          2          1          1 |\n  3. | AMC Pacer       Domestic    -.627642          3          0          1 |\n  4. | AMC Concord     Domestic    .0767283          4          0          0 |\n  5. | Datsun 200       Foreign   -1.122534          1          1          1 |\n  6. | BMW 320i         Foreign   -.7714012          2          1          0 |\n  7. | Audi Fox         Foreign    .0291835          3          0          0 |\n  8. | Audi 5000        Foreign    1.503948          4          0          0 |\n     +-----------------------------------------------------------------------+"
  },
  {
    "objectID": "responsesurfaceplot.html",
    "href": "responsesurfaceplot.html",
    "title": "Response Surface Plot",
    "section": "",
    "text": "Introduction\nResponse surface analysis is just regression with an interaction. Typically the model fit is\n\\[\n    y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2 + \\beta_4x_1^2 + \\beta_5x_2^2,\n\\]\nbut in general any interaction should work for the below visualization.\n\n\nUsing rsm::persp.lm\nThe “rsm” package allegedly does response surface modeling automatically, but we can use it just for the rsm::persp.lm function (which can just be referred to as persp after loading the package). Note that stats::persp does not work directly with a model object; you’d need to do a few extra steps to generate a matrix first.\n\nlibrary(rsm)\n\ndata(mtcars)\n\nmod1 &lt;- lm(mpg ~ disp + qsec + disp:qsec + I(disp^2) + I(qsec^2), data = mtcars)\n\nrsm::persp.lm(mod1, disp ~ qsec, zlab = \"mpg\", theta = 120, phi = 10, r = 10)\n\n\n\n\n(Note: persp.lm does not support other types of models such as mixed models from lmer, however plotly below should support all types of models that emmeans supports.)\n\n\nUsing plotly\nAlternately, plotly can be used to create an interactive plot. The data needs to be set up in a bit of an odd fashion, instead of a matrix of x-y-z triads, a list is created with the unique x values, the unique y values, and a matrix of predicted values of the outcome containing all pairwise values of x and y. We use emmeans to generate these predicted values.\n\nlibrary(plotly)\nlibrary(emmeans)\n\n\n# Set up grid\nq &lt;- 14:23\nd &lt;- (1:10)*50\n\nem &lt;- as.data.frame(emmeans(mod1, ~ qsec*disp, at = list(qsec = q, disp = d)))\n\n# x & y are unique values, z is len(x) x len(y) matrix\ndd &lt;- list(q = q,\n           d = d,\n           m = matrix(em$emmean,\n                      nrow = length(q),\n                      ncol = length(d),\n                      byrow = TRUE))\n\nplot_ly(x = dd$q, y = dd$d, z = dd$m, type = \"surface\") |&gt;\n  layout(scene = list(xaxis=list(title = \"qsec\"),\n                      yaxis=list(title = \"disp\"),\n                      zaxis=list(title = \"mpg\")))\n\n\n\n\n\n\n\nA Warning about Interpretation\nResponse surface plots can be extremely misleading due to extrapolation. Consider the plots above. One thing we would be tempted to interpret from the plot is to predict that the lowest mileage cars are around 5 mpg, and this occurs when disp and qsec are at their maximum. However, this is complete extrapolation. We can look at the predicted values for the cars in the data:\n\nsummary(predict(mod1))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  13.10   15.36   19.53   20.09   24.28   30.13 \n\n\nThe lowest predicted mileage is 13.1, yet the response surface dips below 5! What’s happening is that there are no cars in the car with both high disp and high qsec, so that area of the response curve is entirely extrapolated - we have no idea what should be occurring there.\n\nplot(mtcars$disp ~ mtcars$qsec)\ntext(21, 400, \"There's no data in this region!\", col = \"red\")\n\n\n\n\nFinally, let’s add the predicted values onto the response surface to further visualize the areas of the response surface that is pure extrapolation. (Note that I add one to the predicted values (predict(mod1) + 1) to move the points slightly above the surface, improving visualization.)\n\ndf &lt;- data.frame(qsec = mtcars$qsec,\n                 disp = mtcars$disp,\n                 predict = predict(mod1) + 1)\n\n\nplot_ly(x = dd$q, y = dd$d, z = dd$m, type = \"surface\") |&gt;\n  layout(scene = list(xaxis=list(title = \"qsec\"),\n                      yaxis=list(title = \"disp\"),\n                      zaxis=list(title = \"mpg\"))) |&gt;\n  add_trace(x = df$qsec, y = df$disp, z = df$predict, mode = \"markers\",\n            type = \"scatter3d\",\n            marker = list(size = 10, color = \"red\", symbol = 104))"
  },
  {
    "objectID": "splinesvsinteraction.html",
    "href": "splinesvsinteraction.html",
    "title": "Splines vs Interaction models",
    "section": "",
    "text": "Introduction\nLinear splines are sometimes used when looking at interrupted time series models. For example, consider the scatter plot below.\nThe slope amongst the red points (x &lt; 5) is clearly different from the slope amongst the blue points (x &gt; 5). The best fit line fails to capture this at all.\nImagine that x is time, and at x = 5, some intervention took place. The goal is to capture the change in slope that occurs after the intervention. One easy approach would be to fit separate pre and post models, and test for equality of coefficients. However, we can also address this with a single model.\nA linear spline model (as fit by Stata’s mkspline) can capture that change in trend. Including an indicator for pre/post even allows a discontinuity at x = 5 instead of the typical continuous spline. However, splines can be harder to interpret and more complicated to work with. This document will demonstrate that an interaction model is equivalent to the linear spline model, and with a simple re-scaling, easier to interpret.\nLet’s create a slightly more general data set where there is a “jump” (discontinuity) at intervention in addition to the change in trend.\nNow there’s a drop of around 4 at the intervention addition to a flattening of the slope.\nFor comparison purposes, let’s obtain the slopes in each time period.\nSo the pre slope is 0.998 and the post slope is 0.121. Their difference is -0.877.\nThe “intervention” takes place at x = 5, so let’s create the spline with a knot there.\nWith the marginal option, x0’s coefficient will represent the pre-intervention slop and x1’s coefficient the difference between the pre- and post-intervention slopes (similar to an interaction). Without marginal, x1’s coefficient is the post-intervention slope. Note that this will not change the model, but is a simple reparameterization.\nIf we fit a simple interaction model here, we obtain the same model.\nThe coefficient for x and the interaction capture the pre-slope and the change in slope after intervention, but the coefficent on z is capturing the difference in y-intercepts at x = 0 - a meaningless value. This greatly harms the interpretability of this model."
  },
  {
    "objectID": "splinesvsinteraction.html#spline-model-1---continuous-at-intervention",
    "href": "splinesvsinteraction.html#spline-model-1---continuous-at-intervention",
    "title": "Splines vs Interaction models",
    "section": "Spline Model 1 - Continuous at intervention",
    "text": "Spline Model 1 - Continuous at intervention\nFirst, we’ll predict y using only the splines. This forces a continuity at intervention.\n. reg y x0 x1\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(2, 97)        =      9.48\n       Model |  47.3187332         2  23.6593666   Prob &gt; F        =    0.0002\n    Residual |  242.134186        97  2.49622872   R-squared       =    0.1635\n-------------+----------------------------------   Adj R-squared   =    0.1462\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.5799\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x0 |   .4201304   .1111038     3.78   0.000     .1996201    .6406408\n          x1 |  -.9088959   .2087659    -4.35   0.000    -1.323238   -.4945534\n       _cons |   .7302661   .3768498     1.94   0.056    -.0176763    1.478209\n------------------------------------------------------------------------------\n\n. est store spline1\n\n. predict y_spline1\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_spline1 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_spline1 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\nThe continuity1 at x = 5 makes this a poor fit."
  },
  {
    "objectID": "splinesvsinteraction.html#spline-model-2---discontinuous-at-intervention",
    "href": "splinesvsinteraction.html#spline-model-2---discontinuous-at-intervention",
    "title": "Splines vs Interaction models",
    "section": "Spline Model 2 - Discontinuous at intervention",
    "text": "Spline Model 2 - Discontinuous at intervention\nSimply adding z to the model will allow a discontinuity.\n. reg y x0 x1 z\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(3, 96)        =     38.83\n       Model |  158.673925         3  52.8913084   Prob &gt; F        =    0.0000\n    Residual |  130.778994        96  1.36228119   R-squared       =    0.5482\n-------------+----------------------------------   Adj R-squared   =    0.5341\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.1672\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x0 |   .9983998   .1040552     9.59   0.000      .791852    1.204948\n          x1 |  -.8769945   .1542639    -5.69   0.000    -1.183206   -.5707831\n           z |  -4.057394   .4487716    -9.04   0.000    -4.948199    -3.16659\n       _cons |  -.1003115   .2931596    -0.34   0.733    -.6822287    .4816058\n------------------------------------------------------------------------------\n\n. est store spline2\n\n. predict y_spline2\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_spline2 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_spline2 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\nWe capture the model much better here. Note that the coefficient on x0 is the marginal slope we obtained before and x1 is the difference between the slopes.\nAdditionally (and one of the major benefits that linear spline proponents point to) is that the coefficient on z, -4.06, captures the drop that occurs at x = 5 - in the pre-period, the best fit line is approaching ~5, and in the post-period, the best fit line is approaching ~1."
  },
  {
    "objectID": "splinesvsinteraction.html#without-marginal",
    "href": "splinesvsinteraction.html#without-marginal",
    "title": "Splines vs Interaction models",
    "section": "Without marginal",
    "text": "Without marginal\nLet’s generate the splines without the marginal option to show the results are the same.\n. mkspline x0a 5 x1a = x\n. reg y x0a x1a z\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(3, 96)        =     38.83\n       Model |  158.673925         3  52.8913084   Prob &gt; F        =    0.0000\n    Residual |  130.778994        96  1.36228119   R-squared       =    0.5482\n-------------+----------------------------------   Adj R-squared   =    0.5341\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.1672\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         x0a |   .9983998   .1040552     9.59   0.000      .791852    1.204948\n         x1a |   .1214053   .1138854     1.07   0.289    -.1046554    .3474659\n           z |  -4.057394   .4487716    -9.04   0.000    -4.948199    -3.16659\n       _cons |  -.1003115   .2931596    -0.34   0.733    -.6822287    .4816058\n------------------------------------------------------------------------------\n\n. est store spline3\n\n. predict y_spline3\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_spline3 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_spline3 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\nThe model is identical, but the coefficient on x1a is now the slope in the post period."
  },
  {
    "objectID": "splinesvsinteraction.html#interaction-model-1---continuity-at-intervention",
    "href": "splinesvsinteraction.html#interaction-model-1---continuity-at-intervention",
    "title": "Splines vs Interaction models",
    "section": "Interaction model 1 - Continuity at intervention",
    "text": "Interaction model 1 - Continuity at intervention\nIf we use a version of x which is re-centered around the intervention point (a linear transformation, not affecting the model fit), we can instead obtain a coefficient on the interaction that’s interpretable.\n. gen xc = x - 5\nFirst we’ll fit the model forcing continuity at the intervention. We fit this model by including a main effect for xc, the interaction of xc and z, but crucially, not a main effect for z.\n. reg y c.xc c.xc#i.z\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(2, 97)        =      9.48\n       Model |  47.3187335         2  23.6593667   Prob &gt; F        =    0.0002\n    Residual |  242.134186        97  2.49622872   R-squared       =    0.1635\n-------------+----------------------------------   Adj R-squared   =    0.1462\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.5799\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          xc |   .4201304   .1111038     3.78   0.000     .1996201    .6406408\n             |\n      z#c.xc |\n          1  |  -.9088959   .2087659    -4.35   0.000    -1.323238   -.4945535\n             |\n       _cons |   2.830918   .3037036     9.32   0.000     2.228151    3.433686\n------------------------------------------------------------------------------\n\n. est store int1\n\n. predict y_int1\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_int1 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_int1 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\n. est table spline1 int1\n\n----------------------------------------\n    Variable |  spline1        int1     \n-------------+--------------------------\n          x0 |  .42013044               \n          x1 | -.90889588               \n          xc |               .42013044  \n             |\n      z#c.xc |\n          1  |              -.90889589  \n             |\n       _cons |   .7302661    2.8309183  \n----------------------------------------\nAs you can see, we get identical results. (The y-intercept differs - in the spline model, it is the value estimated when x = 0; in the interaction model, it is the value estimated when x approaches 5 from the left.)"
  },
  {
    "objectID": "splinesvsinteraction.html#interaction-model-2---discontinuous-at-intervention",
    "href": "splinesvsinteraction.html#interaction-model-2---discontinuous-at-intervention",
    "title": "Splines vs Interaction models",
    "section": "Interaction Model 2 - Discontinuous at intervention",
    "text": "Interaction Model 2 - Discontinuous at intervention\nNow, relax the continuity assumption.\n. reg y c.xc##i.z\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(3, 96)        =     38.83\n       Model |  158.673926         3  52.8913088   Prob &gt; F        =    0.0000\n    Residual |  130.778993        96  1.36228118   R-squared       =    0.5482\n-------------+----------------------------------   Adj R-squared   =    0.5341\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.1672\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          xc |   .9983998   .1040552     9.59   0.000      .791852    1.204948\n         1.z |  -4.057394   .4487716    -9.04   0.000    -4.948199    -3.16659\n             |\n      z#c.xc |\n          1  |  -.8769945   .1542639    -5.69   0.000    -1.183206   -.5707831\n             |\n       _cons |   4.891688    .319828    15.29   0.000     4.256834    5.526541\n------------------------------------------------------------------------------\n\n. est store int2\n\n. predict y_int2\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_int2 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_int2 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\n. est table spline2 int2\n\n----------------------------------------\n    Variable |  spline2        int2     \n-------------+--------------------------\n          x0 |  .99839981               \n          x1 | -.87699452               \n           z | -4.0573944               \n          xc |               .99839982  \n             |\n           z |\n          1  |              -4.0573945  \n             |\n      z#c.xc |\n          1  |              -.87699454  \n             |\n       _cons | -.10031147    4.8916876  \n----------------------------------------\nAgain, we get the same results."
  },
  {
    "objectID": "splinesvsinteraction.html#obtaining-both-slopes",
    "href": "splinesvsinteraction.html#obtaining-both-slopes",
    "title": "Splines vs Interaction models",
    "section": "Obtaining both slopes",
    "text": "Obtaining both slopes\nAs mentioned before, the one downside of the interaction model is that we don’t directly get the post-slope, instead obtaining the pre-slope and and the difference in slopes. This is easily remedied:\n. margins z, dydx(xc)\n\nAverage marginal effects                                   Number of obs = 100\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  xc\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxc           |\n           z |\n          0  |   .9983998   .1040552     9.59   0.000      .791852    1.204948\n          1  |   .1214053   .1138854     1.07   0.289    -.1046554    .3474659\n------------------------------------------------------------------------------\nOnce again, agreeing with the slopes obtained before of 0.998 and 0.121."
  },
  {
    "objectID": "splinesvsinteraction.html#footnotes",
    "href": "splinesvsinteraction.html#footnotes",
    "title": "Splines vs Interaction models",
    "section": "",
    "text": "The visual discontinity is due the way the plot is generated and is not real.↩︎"
  },
  {
    "objectID": "svy_gsem_teffects.html",
    "href": "svy_gsem_teffects.html",
    "title": "Mediation with svyset data",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\nStata Versioning\n\n\n\nCurrent versions of Stata support svy: sem and estat teffects without complaint. There is no longer a need to use gsem and manual calculation. However, this document may still be useful as either a guide to using estat teffects or manually calculating them if you need to use gsem for another reason.\n\n\nMediation models in Stata are fit with the sem command. sem does not support svyset data, so instead you use gsem (e.g. svy: gsem ...). However, gsem does not support estat teffects which calculates direct, indirect and total effects.\nThis document shows how to manually calculate these effects using nlcom.\nNote that this is a case where all variables are continuous and all models are linear - we are only using gsem for it’s support of svy:, not it’s support of GLMs. Indirect effects are a more complicated topic in those models which we do not address here. Additionally, we’ll trust Stata to compute standard errors rather than getting into any sticky issues of bootstrapping.\n\n\nStandard Mediation\nFirst, let’s estimate the direct, indirect and total effects without the use of the survey design to show equivalence.\n. webuse gsem_multmed\n(Fictional job-performance data)\nThe model we’ll be fitting is\n\n\n\nmediation\n\n\nHere, “satis” is a potential mediator between “support” and “perform”. The direct effect is the arrow between “support” and “perform”, the indirect effect is the arrows from “support” to “perform” which passes through “satis”, and the total effect is the sum of the direct and indirect effects.\n. sem (perform &lt;- satis support) (satis &lt;- support)\n\nEndogenous variables\n  Observed: perform satis\n\nExogenous variables\n  Observed: support\n\nFitting target model:\nIteration 0:  Log likelihood = -3779.9224  \nIteration 1:  Log likelihood = -3779.9224  \n\nStructural equation model                                Number of obs = 1,500\nEstimation method: ml\n\nLog likelihood = -3779.9224\n\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .6161077   .0303143    20.32   0.000     .5566927    .6755227\n       _cons |   4.981054   .0150589   330.77   0.000     4.951539    5.010569\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n       _cons |    .019262   .0154273     1.25   0.212    -.0109749    .0494989\n-------------+----------------------------------------------------------------\nvar(e.perf~m)|   .3397087   .0124044                      .3162461     .364912\n var(e.satis)|   .3569007   .0130322                      .3322507    .3833795\n------------------------------------------------------------------------------\nLR test of model vs. saturated: chi2(0) = 0.00                 Prob &gt; chi2 = .\nThe direct, indirect and total effects can be estimated via estat teffects.\n. estat teffects\n\n\nDirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .6161077   .0303143    20.32   0.000     .5566927    .6755227\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n------------------------------------------------------------------------------\n\n\nIndirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |          0  (no path)\n     support |    .205648   .0280066     7.34   0.000      .150756      .26054\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |          0  (no path)\n------------------------------------------------------------------------------\n\n\nTotal effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .8217557   .0404579    20.31   0.000     .7424597    .9010516\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n------------------------------------------------------------------------------\nLet’s calculate them manually. First we’ll re-display the SEM results with the coeflegend to obtain the names to access the coefficients.\n. sem, coeflegend\n\nStructural equation model                                Number of obs = 1,500\nEstimation method: ml\n\nLog likelihood = -3779.9224\n\n------------------------------------------------------------------------------\n             | Coefficient  Legend\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401  _b[perform:satis]\n     support |   .6161077  _b[perform:support]\n       _cons |   4.981054  _b[perform:_cons]\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945  _b[satis:support]\n       _cons |    .019262  _b[satis:_cons]\n-------------+----------------------------------------------------------------\nvar(e.perf~m)|   .3397087  _b[/var(e.perform)]\n var(e.satis)|   .3569007  _b[/var(e.satis)]\n------------------------------------------------------------------------------\nLR test of model vs. saturated: chi2(0) = 0.00                 Prob &gt; chi2 = .\nThe main effects are directly from the model, but for completeness let’s obtain it.\n. estat teffects, noindirect nototal\n\n\nDirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .6161077   .0303143    20.32   0.000     .5566927    .6755227\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n------------------------------------------------------------------------------\n\n. nlcom _b[perform:support]\n\n       _nl_1: _b[perform:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .6161077   .0303143    20.32   0.000     .5566927    .6755227\n------------------------------------------------------------------------------\nFor the indirect effect, we’ll simply multiply the path from “support” to “satis” and from “satis” to “perform”.\n. estat teffects, nodirect nototal\n\n\nIndirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |          0  (no path)\n     support |    .205648   .0280066     7.34   0.000      .150756      .26054\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |          0  (no path)\n------------------------------------------------------------------------------\n\n. nlcom _b[perform:satis]*_b[satis:support]\n\n       _nl_1: _b[perform:satis]*_b[satis:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |    .205648   .0280066     7.34   0.000      .150756      .26054\n------------------------------------------------------------------------------\nFinally, we can sum those for the direct effect.\n. estat teffects, nodirect noindirect\n\n\nTotal effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .8217557   .0404579    20.31   0.000     .7424597    .9010516\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n------------------------------------------------------------------------------\n\n. nlcom _b[perform:satis]*_b[satis:support] + _b[perform:support]\n\n       _nl_1: _b[perform:satis]*_b[satis:support] + _b[perform:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .8217557   .0404579    20.31   0.000     .7424597    .9010516\n------------------------------------------------------------------------------\n\n\nWith survey data\nWe’ll reproduce the above results with survey set data. The actually svyset here is nonsense, this test data is not actual survey data.\n. svyset branch [pweight = perform]\n\nSampling weights: perform\n             VCE: linearized\n     Single unit: missing\n        Strata 1: &lt;one&gt;\n Sampling unit 1: branch\n           FPC 1: &lt;zero&gt;\nTo use the svy prefix, we switch from sem to gsem.\n. svy: gsem (perform &lt;- satis support) (satis &lt;- support)\n(running gsem on estimation sample)\n\nSurvey: Generalized structural equation model\n\nNumber of strata =  1                              Number of obs   =     1,500\nNumber of PSUs   = 75                              Population size = 7,507.976\n                                                   Design df       =        74\n\nResponse: perform \nFamily:   Gaussian\nLink:     Identity\n\nResponse: satis   \nFamily:   Gaussian\nLink:     Identity\n\n------------------------------------------------------------------------------\n             |             Linearized\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nperform      |\n       satis |   .8768337   .0478296    18.33   0.000      .781531    .9721363\n     support |   .6105411   .0276199    22.11   0.000     .5555072     .665575\n       _cons |   5.051254   .0414936   121.74   0.000     4.968576    5.133932\n-------------+----------------------------------------------------------------\nsatis        |\n     support |    .212986   .0261195     8.15   0.000     .1609418    .2650301\n       _cons |   .0841272   .0583215     1.44   0.153     -.032081    .2003353\n-------------+----------------------------------------------------------------\nvar(e.perf~m)|   .3284831   .0241911                      .2836511    .3804009\n var(e.satis)|   .3570689   .0353162                      .2931998    .4348508\n------------------------------------------------------------------------------\nFinally, all three effects can be calculated via the same nlcom commands.\n. * main effect\n. nlcom _b[perform:support]\n\n       _nl_1: _b[perform:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .6105411   .0276199    22.11   0.000      .556407    .6646751\n------------------------------------------------------------------------------\n\n. \n. * Indirect effect\n. nlcom _b[perform:satis]*_b[satis:support]\n\n       _nl_1: _b[perform:satis]*_b[satis:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .1867533   .0253151     7.38   0.000     .1371366    .2363699\n------------------------------------------------------------------------------\n\n. \n. * Total effect\n. nlcom _b[perform:satis]*_b[satis:support] + _b[perform:support]\n\n       _nl_1: _b[perform:satis]*_b[satis:support] + _b[perform:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .7972943   .0308642    25.83   0.000     .7368017     .857787\n------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "visualizeCollinearity.html",
    "href": "visualizeCollinearity.html",
    "title": "Visualizing the issue with Collinear Predictors",
    "section": "",
    "text": "Introduction\nWhile there are many resources out there to describe the issues arising with multicollinearity in independent variables, there’s a visualization for the problem that I came across in a book once and haven’t found replicated online. This replicates the visualization.\n\n\nSet-up\nLet \\(Y\\) be a response and \\(X_1\\) and \\(X_2\\) be the predictors, such that\n\\[\n    Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\epsilon_i\n\\]\nfor individual \\(i\\).\nFor simplicity, let’s say that\n\\[\n    \\beta_0 = 0,\\\\\n    \\beta_1 = 1,\\\\\n    \\beta_2 = 1.\n\\]\nI carry out a simulation by generating 1,000 simulations with a given correlation between predictors and obtain their coefficients.\n\nreps &lt;- 1000\nn &lt;- 100\nsave &lt;- matrix(nrow = reps, ncol = 3)\n\nfor (i in 1:reps) {\n  x1 &lt;- rnorm(n)\n  x2 &lt;- rnorm(n)\n  y &lt;- x1 + x2 + rnorm(n)\n\n  mod &lt;- lm(y ~ x1 + x2)\n  save[i,] &lt;- c(coef(mod)[-1], cor(x1, x2))\n}\n\nThe line x2 &lt;- rnorm(n) gets replaced with x2 &lt;- x1 + rnorm(n, sd = _), where the _ is replaced with difference values to induce more correlation between x1 and x2.\nFollowing these simulations, the coefficients are plotted against each out.\n\n\nSimulation\n\nNo CollinearityModerate CollinearityHigh CollinearityExtremely High Collinearity\n\n\nThe average correlation between \\(X_1\\) and \\(X_2\\) is 0.002.\n\n\n\n\n\nThe red dot represents the true coefficients. We see no relationship between the estimated coefficients, and each are well centered around the truth.\n\n\nThe average correlation between \\(X_1\\) and \\(X_2\\) is 0.553.\n\n\n\n\n\nWhile there is some semblance of a relationship, it is not very strong.\n\n\nThe average correlation between \\(X_1\\) and \\(X_2\\) is 0.893.\n\n\n\n\n\nThere’s a strong relationship now, such that if one coefficient is incorrectly high, the other is incorrectly low.\n\n\nThe average correlation between \\(X_1\\) and \\(X_2\\) is 0.995.\n\n\n\n\n\nWe now see a very strong relationship between the coefficients.\n\n\n\n\n\nWhy is this a problem?\nConsider the “extremely high correlation” results. With such high correlation, essentially \\(X_1 = X_2\\), which is represented by the red line (slope of -1). We can approximate our model:\n\\[\\begin{aligned}\n  Y_i &= \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\epsilon_i\\\\\n  &\\approx \\beta_0 + (\\beta_1 + \\beta_2)X_{1i} + \\epsilon_i\\\\\n  &\\approx \\beta_0 + (\\beta_1 + \\beta_2)X_{2i} + \\epsilon_i\n\\end{aligned}\\]\nIn other words, the model has that \\(\\beta_1 + \\beta_2 = 2\\) (since we assumed above that both coefficients have values of 1.. So while all of those models would have the same predictive power for \\(Y\\), they would have drastically different interpretations depending on where along that red line they fall."
  },
  {
    "objectID": "vizrandomeffects.html",
    "href": "vizrandomeffects.html",
    "title": "Visualizing Nested and Cross Random Effects",
    "section": "",
    "text": "Random Effects\nOne way to think about random intercepts in a mixed models is the impact they will have on the residual covariance matrix. Of course, in a model with only fixed effects (e.g. lm), the residual covariance matrix is diagonal as each observation is assumed independent. In mixed models, there is a dependence structure across observations, so the residual covariance matrix will no longer be diagonal.\nThis function extracts components of the mixed model and constructs the covariance matrix. From https://stackoverflow.com/a/45655597/905101\nrescov &lt;- function(model, data) {\n  var.d &lt;- crossprod(getME(model,\"Lambdat\"))\n  Zt &lt;- getME(model,\"Zt\")\n  vr &lt;- sigma(model)^2\n  var.b &lt;- vr*(t(Zt) %*% var.d %*% Zt)\n  sI &lt;- vr * Diagonal(nrow(data))\n  var.y &lt;- var.b + sI\n  invisible(var.y)\n}\nThe data is Penicillin from the “lmer” package.\nlibrary(lme4)\ndata(Penicillin)\nhead(Penicillin, 10)\n\n   diameter plate sample\n1        27     a      A\n2        23     a      B\n3        26     a      C\n4        23     a      D\n5        23     a      E\n6        21     a      F\n7        27     b      A\n8        23     b      B\n9        26     b      C\n10       23     b      D\nThis data is measuring Penicillin over a number of different trials. There are 24 plates and 6 samples, each plate having 1 replicate from each sample. (This is a fully crossed design, but it need not be.)\nFor the time being, let’s ignore the plate level effects, and fit a model with a random intercept only for sample.\nIn this data there are no covariates to enter as fixed effects, but their existence would not impede things.\nmod1 &lt;- lmer(diameter ~ 1 + (1 | sample), data = Penicillin %&gt;% arrange(sample))\nrc1 &lt;- rescov(mod1, Penicillin)\nimage(rc1)\n(The data is re-ordered by sample to improve visualization. You generally want the data sorted first by the higher level, then within that level, the next highest level, etc.)\nYou see that this is block diagonal, with 6 blocks, each corresponding to one of the samples. This implies that the repeated measurements within each sample is correlated, but between samples are not correlated (as we expect).\nNow let’s refit the above model, including the crossed random effets. In lmer, we simply add a second random intercept.\nmod2 &lt;- lmer(diameter ~ 1 + (1 | sample) + (1 | plate), data = Penicillin)\nrc2 &lt;- rescov(mod2, Penicillin)\nimage(rc2)\nWe see an additional pattern here. It can be hard to interpret at such a high level, so let’s zoom in.\nimage(rc2[1:12, 1:12])\nThe diagonal blocks represents the correlation across plates - here we see the first 2 (of total 24). The diagonal lines represent the sample correlations. This subset of covariance matrix is represented by this data:\nhead(Penicillin, 12)\n\n   diameter plate sample\n1        27     a      A\n2        23     a      B\n3        26     a      C\n4        23     a      D\n5        23     a      E\n6        21     a      F\n7        27     b      A\n8        23     b      B\n9        26     b      C\n10       23     b      D\n11       23     b      E\n12       21     b      F\nWe see that observations 1 and 7 share the same sample “A”, 2 and 8 share the same sample “B”, etc. These entries are non-zero in the covariance matrix.\nNow lets view a nested random effect. We’ll switch to the Oxide data from “nlme”.\ndata(Oxide, package = \"nlme\")\nhead(Oxide, 12)\n\nGrouped Data: Thickness ~ 1 | Lot/Wafer\n   Source Lot Wafer Site Thickness\n1       1   1     1    1      2006\n2       1   1     1    2      1999\n3       1   1     1    3      2007\n4       1   1     2    1      1980\n5       1   1     2    2      1988\n6       1   1     2    3      1982\n7       1   1     3    1      2000\n8       1   1     3    2      1998\n9       1   1     3    3      2007\n10      1   2     1    1      1991\n11      1   2     1    2      1990\n12      1   2     1    3      1988\nWe’ll ignore the Source (there are only two) and instead focus on lots and wafers. There are 8 different lots, and within each lot there are 3 wafers. Three measurements are made on each Wafer (the Site variable) of the Thickness.\nHere Wafer is nested inside Lot.\nmod3 &lt;- lmer(Thickness ~ 1 + (1|Lot/Wafer), data = Oxide)\nrc3 &lt;- rescov(mod3, Oxide)\nimage(rc3)\nThis is much cleaner as opposed to the crossed example. Each of the 8 larger blocks represents the correlations within each Lot, and the 3 smaller darker blocks within represent the additional correlation within each Wafer.\nWhat would the covariance matrix look like if we had crossed effects rather than nested?\nmod3b &lt;- lmer(Thickness ~ 1 + (1|Lot) + (1|Wafer), data = Oxide)\nrc3b &lt;- rescov(mod3b, Oxide)\nimage(rc3b)\nBecause the wafers within each lot are named the same, we have spurious correlations.\nAs mentioned above, nested effects are an attribute of the data, not the model. We can include nested random effects using the cross effects syntax. In other words, for a nested structure, there is an equivalent crossed structure. (The reverse is not true.)\nThe key is that the Wafer levels must be unique in the data, not just within each lot. Let’s generate a unique version of wafer.\nOxide &lt;- mutate(Oxide, Wafer2 = as.numeric(paste0(Lot, Wafer)))\nhead(Oxide, 12)\n\nGrouped Data: Thickness ~ 1 | Lot/Wafer\n   Source Lot Wafer Site Thickness Wafer2\n1       1   1     1    1      2006     11\n2       1   1     1    2      1999     11\n3       1   1     1    3      2007     11\n4       1   1     2    1      1980     12\n5       1   1     2    2      1988     12\n6       1   1     2    3      1982     12\n7       1   1     3    1      2000     13\n8       1   1     3    2      1998     13\n9       1   1     3    3      2007     13\n10      1   2     1    1      1991     21\n11      1   2     1    2      1990     21\n12      1   2     1    3      1988     21\nLet’s check that this didn’t break the nested structure.\nmod4 &lt;- lmer(Thickness ~ 1 + (1|Lot/Wafer2), data = Oxide)\nsummary(mod3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Thickness ~ 1 + (1 | Lot/Wafer)\n   Data: Oxide\n\nREML criterion at convergence: 454\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8746 -0.4991  0.1047  0.5510  1.7922 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n Wafer:Lot (Intercept)  35.87    5.989  \n Lot       (Intercept) 129.91   11.398  \n Residual               12.57    3.545  \nNumber of obs: 72, groups:  Wafer:Lot, 24; Lot, 8\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 2000.153      4.232   472.6\n\nsummary(mod4)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Thickness ~ 1 + (1 | Lot/Wafer2)\n   Data: Oxide\n\nREML criterion at convergence: 454\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8746 -0.4991  0.1047  0.5510  1.7922 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n Wafer2:Lot (Intercept)  35.87    5.989  \n Lot        (Intercept) 129.91   11.398  \n Residual                12.57    3.545  \nNumber of obs: 72, groups:  Wafer2:Lot, 24; Lot, 8\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 2000.153      4.232   472.6\nWe saw before that adding crossed random effects with Lot and Wafer was not the same as nested. However, with Lot and Wafer2…\nmod4b &lt;- lmer(Thickness ~ 1 + (1|Lot) + (1|Wafer2), data = Oxide)\nsummary(mod4b)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Thickness ~ 1 + (1 | Lot) + (1 | Wafer2)\n   Data: Oxide\n\nREML criterion at convergence: 454\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8746 -0.4991  0.1047  0.5510  1.7922 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Wafer2   (Intercept)  35.87    5.989  \n Lot      (Intercept) 129.91   11.398  \n Residual              12.57    3.545  \nNumber of obs: 72, groups:  Wafer2, 24; Lot, 8\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 2000.153      4.232   472.6\n\nrc4b &lt;- rescov(mod4b, Oxide)\nimage(rc4b)\nAs long as the the levels of the nested variable are unique across the data as opposed to unique within each of the nesting variable, nested effects and crossed effects are identical. There’s a very nice visualization of this found in this thread: https://stats.stackexchange.com/questions/228800/crossed-vs-nested-random-effects-how-do-they-differ-and-how-are-they-specified\nWith this naming convention, nested and fixed effects will be different:\nWith this naming convention, nested and fixed effects will be equivalent:"
  },
  {
    "objectID": "vizrandomeffects.html#nested-versus-crossed",
    "href": "vizrandomeffects.html#nested-versus-crossed",
    "title": "Visualizing Nested and Cross Random Effects",
    "section": "Nested versus Crossed",
    "text": "Nested versus Crossed\nWhether random effects are nested or crossed1 is a property of the data, not the model. However, when fitting the model, effects can be included as either nested or crossed.\nNested random effects are when each member of one group is contained entirely within a single unit of another group. The canonical example is students in classrooms; you may have repeated measures per student, but each student belongs to a single classroom (assuming no reassignments).\nCrossed random effects are when this nesting is not true. An example would be different seeds and different fields used for planting crops. Seeds of the same type can be planted in different fields, and each field can have multiple seeds in it."
  },
  {
    "objectID": "vizrandomeffects.html#footnotes",
    "href": "vizrandomeffects.html#footnotes",
    "title": "Visualizing Nested and Cross Random Effects",
    "section": "",
    "text": "“Crossed” simply means non-nested.↩︎"
  },
  {
    "objectID": "xtsetvsmixed.html",
    "href": "xtsetvsmixed.html",
    "title": "Stata’s mixed versus xtreg",
    "section": "",
    "text": "Introduction\nIn Stata, panel data (repeated measures) can be modeled using mixed (and its siblings e.g. melogit, mepoisson) or using the xt toolkit, including xtset and xtreg.\nThis document is an attempt to show the equivalency of the models between the two commands. There will be slight differences due to the algorithms used in the backend but the results should generally be equivalent.\nWe’ll use the “nlswork” data:\nidcode represents each individual, data is measured over the years. Lets set it up using xtset:\nWithin the panel/xt framework, there are three separate models:\nEach is fitted via\nThe short version of how to fit each model using mixed is:\nThe xtsum command can be used to estimate the variance of a variable within versus between.\nWe can replicate all these results without xt. As a sidenote, “T-bar” represents the average number of measures per individual, or N/n.\nLet’s use the following as our model"
  },
  {
    "objectID": "xtsetvsmixed.html#the-math",
    "href": "xtsetvsmixed.html#the-math",
    "title": "Stata’s mixed versus xtreg",
    "section": "The Math",
    "text": "The Math\nPicture a typical mixed model setup:\n\\[\n    y\\_{it} = \\alpha + x\\_{it}\\beta + \\nu\\_i + \\epsilon\\_{it}.\n\\]\nHere \\(i\\) is an index for individuals, \\(t\\) is an index for time. \\(y\\_{it}\\) and \\(x\\_{it}\\) are some outcome and predictor which are both time and individual varying. \\(\\nu\\_{i}\\) is an error associated with each individual and \\(\\epsilon\\_{it}\\) is an additional error per observation.\nIf this model is true, then the following must be true:\n\\[\n    \\overline{y}\\_i = \\alpha + \\overline{x}\\_i\\beta + \\nu\\_i + \\overline{\\epsilon\\_i}.\n\\]\nEach bar’d variable is average over each individual. In this model, \\(\\nu\\_i\\) and \\(\\overline{\\epsilon}\\_i\\) are indistinguishable, so this is just a linear model.\nSince we have that both models are equivalent, if we difference them, we remain equivalent:\n\\[\n    (y\\_{it} - \\overline{y}\\_i) = (x\\_{it} - \\overline{x}\\_i)\\beta + (\\epsilon\\_{it} - \\overline{\\epsilon}\\_i).\n\\]\nAgain, we have just a linear model.\nFinally, the random effects model doesn’t add much clarity, but it essentially is a weighted combination of the other two, with the weight being a function of the variance of \\(\\nu\\_i\\) and \\(\\epsilon\\_i\\). If the variance of \\(\\nu\\_i\\) is 0, then there’s no individual level effect and the first model can be fit lineally (because \\(\\nu\\_i\\) is constant and folds into the intercept)."
  },
  {
    "objectID": "xtsetvsmixed.html#assumptions",
    "href": "xtsetvsmixed.html#assumptions",
    "title": "Stata’s mixed versus xtreg",
    "section": "Assumptions",
    "text": "Assumptions\nThere is one key different assumption between the models:\nThe random effects model assumes that unobservable variables are uncorrelated with other covariates. The other models don’t.\nThe between effects and random effects models assume that \\(\\nu\\_i\\) and \\(\\overline{x}\\_i\\) are uncorrelated (individual intercepts are independent of predictors)."
  },
  {
    "objectID": "xtsetvsmixed.html#overall-variation",
    "href": "xtsetvsmixed.html#overall-variation",
    "title": "Stata’s mixed versus xtreg",
    "section": "Overall variation",
    "text": "Overall variation\nEasy:\n. summ ln_wage\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     ln_wage |     28,534    1.674907    .4780935          0   5.263916"
  },
  {
    "objectID": "xtsetvsmixed.html#within-variation",
    "href": "xtsetvsmixed.html#within-variation",
    "title": "Stata’s mixed versus xtreg",
    "section": "Within variation",
    "text": "Within variation\nTaking our cue from the notes in the theory, to obtain within variation we will center the variable by individual.\n. egen meanln_wage = mean(ln_wage), by(idcode)\n\n. gen cln_wage = ln_wage - meanln_wage\n\n. summ cln_wage\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n    cln_wage |     28,534   -2.83e-10      .29266  -2.082629   3.108763\nNote that the mean is 0 (within rounding error), as we’d expect. To get the mean/min/max back into the same scale as the raw data we can re-add the overall mean to\n. egen overallmean = mean(ln_wage)\n\n. gen cln_wage2 = cln_wage + overallmean\n\n. summ cln_wage2\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   cln_wage2 |     28,534    1.674907      .29266  -.4077221   4.783669\nThis works because each individual has mean of 0 or in the second case, overallmean, in either case, since the means are constant, we’ve removed any between variance and isolated the within variance."
  },
  {
    "objectID": "xtsetvsmixed.html#between-variation",
    "href": "xtsetvsmixed.html#between-variation",
    "title": "Stata’s mixed versus xtreg",
    "section": "Between variation",
    "text": "Between variation\nWe simply collapse by id.\n. preserve\n\n. collapse (mean) ln_wage, by(idcode)\n\n. summ ln_wage\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     ln_wage |      4,711    1.650605     .424569          0   3.912023\n\n. restore\nDoing this works because each subject now has a single observation, hence the within variance is identically 0, so the remaining variance is between-variance."
  },
  {
    "objectID": "xtsetvsmixed.html#xtreg-fe-fixed-effect-model-within-variance",
    "href": "xtsetvsmixed.html#xtreg-fe-fixed-effect-model-within-variance",
    "title": "Stata’s mixed versus xtreg",
    "section": "xtreg, fe: Fixed Effect model (Within variance)",
    "text": "xtreg, fe: Fixed Effect model (Within variance)\nThe fixed effects results are\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, fe\nnote: grade omitted because of collinearity.\n\nFixed-effects (within) regression               Number of obs     =     28,091\nGroup variable: idcode                          Number of groups  =      4,697\n\nR-squared:                                      Obs per group:\n     Within  = 0.1491                                         min =          1\n     Between = 0.3526                                         avg =        6.0\n     Overall = 0.2517                                         max =         15\n\n                                                F(5, 23389)       =     819.94\ncorr(u_i, Xb) = 0.2348                          Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |          0  (omitted)\n         age |  -.0026787    .000863    -3.10   0.002    -.0043703   -.0009871\n     ttl_exp |   .0287709   .0014474    19.88   0.000     .0259339    .0316079\n      tenure |   .0114355   .0009229    12.39   0.000     .0096265    .0132445\n    not_smsa |  -.0921689   .0096641    -9.54   0.000    -.1111112   -.0732266\n       south |  -.0633396   .0110819    -5.72   0.000    -.0850608   -.0416184\n       _cons |   1.591678   .0186849    85.19   0.000     1.555054    1.628302\n-------------+----------------------------------------------------------------\n     sigma_u |  .36167618\n     sigma_e |  .29477563\n         rho |  .60086475   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(4696, 23389) = 6.63                 Prob &gt; F = 0.0000\nTo replicate, we’ll include idcode as a categorical variable. One of the benefits of xtreg ..., fe is efficiency; since there are over 4000 idcode, the regression model will fail to run. Consequently, we’ll demostrate on a subset of the data\n. preserve\n\n. keep if idcode &lt; 100\n(27,959 observations deleted)\n\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, fe\nnote: grade omitted because of collinearity.\n\nFixed-effects (within) regression               Number of obs     =        567\nGroup variable: idcode                          Number of groups  =         89\n\nR-squared:                                      Obs per group:\n     Within  = 0.1895                                         min =          1\n     Between = 0.2312                                         avg =        6.4\n     Overall = 0.1900                                         max =         15\n\n                                                F(5, 473)         =      22.12\ncorr(u_i, Xb) = 0.1420                          Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |          0  (omitted)\n         age |  -.0045718   .0054274    -0.84   0.400    -.0152366    .0060931\n     ttl_exp |   .0313655   .0091803     3.42   0.001     .0133263    .0494046\n      tenure |   .0149681   .0061975     2.42   0.016     .0027901    .0271462\n    not_smsa |   .0003284   .0743637     0.00   0.996    -.1457957    .1464524\n       south |   .1353543    .170972     0.79   0.429    -.2006042    .4713129\n       _cons |   1.770705   .1125176    15.74   0.000     1.549609    1.991801\n-------------+----------------------------------------------------------------\n     sigma_u |  .35036238\n     sigma_e |  .26779328\n         rho |   .6312319   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(88, 473) = 9.01                     Prob &gt; F = 0.0000\n\n. reg ln_wage grade age ttl_exp tenure not_smsa south i.idcode, noconstant\n\n      Source |       SS           df       MS      Number of obs   =       567\n-------------+----------------------------------   F(94, 473)      =    305.03\n       Model |  2056.18728        94  21.8743328   Prob &gt; F        =    0.0000\n    Residual |  33.9203619       473  .071713239   R-squared       =    0.9838\n-------------+----------------------------------   Adj R-squared   =    0.9805\n       Total |  2090.10764       567  3.68625687   Root MSE        =    .26779\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .1664948   .0111166    14.98   0.000     .1446507    .1883389\n         age |  -.0045718   .0054274    -0.84   0.400    -.0152366    .0060931\n     ttl_exp |   .0313655   .0091803     3.42   0.001     .0133263    .0494046\n      tenure |   .0149681   .0061975     2.42   0.016     .0027901    .0271462\n    not_smsa |   .0003284   .0743637     0.00   0.996    -.1457957    .1464524\n       south |   .1353543    .170972     0.79   0.429    -.2006042    .4713129\n             |\n      idcode |\n          2  |  -.4226575   .1098026    -3.85   0.000    -.6384187   -.2068963\n          3  |  -.5725985    .104336    -5.49   0.000    -.7776178   -.3675791\n          4  |  -.9436944   .1389879    -6.79   0.000    -1.216805   -.6705843\n          5  |   -.222494   .1118964    -1.99   0.047    -.4423696   -.0026184\n          6  |  -.4007454   .1096849    -3.65   0.000    -.6162754   -.1852154\n          7  |  -.6883352   .1225737    -5.62   0.000    -.9291914   -.4474789\n          9  |  -.2376803   .1165492    -2.04   0.042    -.4666986    -.008662\n         10  |  -.3435751   .1106681    -3.10   0.002    -.5610371   -.1261131\n         12  |  -.5649688   .2342601    -2.41   0.016    -1.025288   -.1046496\n         13  |  -.1909402   .1248158    -1.53   0.127    -.4362023    .0543219\n         14  |   -.455463   .2143522    -2.12   0.034    -.8766633   -.0342628\n         15  |  -.4232663   .1481695    -2.86   0.004    -.7144181   -.1321146\n         16  |  -.5371115   .1264239    -4.25   0.000    -.7855334   -.2886895\n         17  |  -.3603291   .1480851    -2.43   0.015    -.6513151   -.0693431\n         18  |  -1.048504   .1680767    -6.24   0.000    -1.378773   -.7182346\n         19  |  -.6236019    .126136    -4.94   0.000    -.8714581   -.3757457\n         20  |  -.5473341   .1259303    -4.35   0.000    -.7947861   -.2998822\n         21  |  -.7002773   .1569423    -4.46   0.000    -1.008668   -.3918869\n         22  |  -.4564266    .124073    -3.68   0.000    -.7002291   -.2126242\n         23  |  -.8079256    .166224    -4.86   0.000    -1.134554   -.4812968\n         24  |  -.4104822   .1151672    -3.56   0.000    -.6367848   -.1841795\n         25  |  -.3723879   .1189486    -3.13   0.002     -.606121   -.1386548\n         26  |  -.5195303   .1469161    -3.54   0.000    -.8082192   -.2308414\n         27  |  -.2435413    .162567    -1.50   0.135    -.5629841    .0759016\n         29  |  -.3157487   .1642662    -1.92   0.055    -.6385305    .0070331\n         30  |    .080972   .1171126     0.69   0.490    -.1491533    .3110972\n         33  |  -.3586822   .2790194    -1.29   0.199    -.9069531    .1895887\n         35  |  -.6745962    .279494    -2.41   0.016      -1.2238   -.1253927\n         36  |  -.3269244   .1273912    -2.57   0.011    -.5772471   -.0766016\n         37  |  -.6471198   .1735604    -3.73   0.000    -.9881646   -.3060751\n         38  |  -.4596228   .1274658    -3.61   0.000    -.7100921   -.2091535\n         39  |  -.4201852    .163985    -2.56   0.011    -.7424145    -.097956\n         40  |  -.5339167   .2840895    -1.88   0.061     -1.09215    .0243168\n         41  |  -.3757769    .122508    -3.07   0.002     -.616504   -.1350497\n         43  |  -1.311499   .2226841    -5.89   0.000    -1.749072   -.8739265\n         44  |  -.2583422   .1631146    -1.58   0.114     -.578861    .0621766\n         45  |  -.1339062   .1198129    -1.12   0.264    -.3693376    .1015253\n         46  |  -1.736985   .2252408    -7.71   0.000    -2.179582   -1.294389\n         47  |  -.6256071   .2058861    -3.04   0.003    -1.030172   -.2210425\n         48  |  -.9859271   .1824752    -5.40   0.000     -1.34449   -.6273647\n         49  |  -.3247521   .1438057    -2.26   0.024     -.607329   -.0421751\n         50  |  -.2738516   .1866851    -1.47   0.143    -.6406863    .0929831\n         51  |  -.6200922   .1274217    -4.87   0.000    -.8704748   -.3697096\n         53  |  -.6259007   .1428582    -4.38   0.000     -.906616   -.3451854\n         54  |  -.5365555   .2812581    -1.91   0.057    -1.089225    .0161143\n         55  |  -.4445126   .1231406    -3.61   0.000    -.6864829   -.2025422\n         56  |   .3443307   .1570771     2.19   0.029     .0356754    .6529859\n         57  |  -.6570438    .114101    -5.76   0.000    -.8812514   -.4328363\n         58  |   -.435127   .1799124    -2.42   0.016    -.7886535   -.0816005\n         59  |   -.628225   .1364025    -4.61   0.000    -.8962548   -.3601951\n         60  |   -.475028   .1440273    -3.30   0.001    -.7580406   -.1920155\n         61  |  -.7261986   .1510088    -4.81   0.000     -1.02293   -.4294675\n         62  |  -.3044535   .1221368    -2.49   0.013    -.5444515   -.0644556\n         63  |   .0037803   .1285151     0.03   0.977    -.2487508    .2563115\n         64  |  -.5483459   .1340122    -4.09   0.000    -.8116787   -.2850131\n         65  |  -.3321434    .141554    -2.35   0.019    -.6102959    -.053991\n         66  |  -.8689924   .1212238    -7.17   0.000    -1.107196   -.6307886\n         67  |  -.3807511   .1543163    -2.47   0.014    -.6839814   -.0775208\n         68  |  -.6198262   .1756128    -3.53   0.000    -.9649039   -.2747485\n         69  |  -.4311974   .1330101    -3.24   0.001    -.6925613   -.1698336\n         70  |   .0557999     .23286     0.24   0.811    -.4017682    .5133679\n         71  |   -.541011   .1282886    -4.22   0.000    -.7930971   -.2889249\n         72  |  -.4935751   .1234947    -4.00   0.000    -.7362413    -.250909\n         73  |  -.6795435   .1474628    -4.61   0.000    -.9693067   -.3897804\n         75  |  -.0205057    .126925    -0.16   0.872    -.2699124     .228901\n         76  |  -.0420092   .2920378    -0.14   0.886    -.6158611    .5318427\n         77  |   -.284598   .1677175    -1.70   0.090    -.6141614    .0449655\n         78  |   .1533827   .1098172     1.40   0.163    -.0624072    .3691726\n         79  |   .4001081   .2748834     1.46   0.146    -.1400355    .9402517\n         80  |   .0210369   .1955429     0.11   0.914    -.3632033     .405277\n         81  |  -.2832579   .2152753    -1.32   0.189    -.7062723    .1397564\n         82  |   .0315568   .1432313     0.22   0.826    -.2498916    .3130052\n         83  |  -.4661398   .1293009    -3.61   0.000     -.720215   -.2120646\n         84  |   .0525394   .2789779     0.19   0.851      -.49565    .6007288\n         85  |    .206903   .1590657     1.30   0.194      -.10566    .5194659\n         86  |  -.7807072   .1429434    -5.46   0.000     -1.06159   -.4998246\n         87  |  -.5240905   .2246382    -2.33   0.020    -.9655027   -.0826783\n         88  |  -.1660402   .2047806    -0.81   0.418    -.5684324    .2363519\n         89  |   .0577245   .2107879     0.27   0.784     -.356472    .4719209\n         91  |   .1126238   .2082437     0.54   0.589    -.2965734     .521821\n         92  |    .488832   .2785641     1.75   0.080    -.0585442    1.036208\n         93  |  -.3047289   .2808968    -1.08   0.279    -.8566889    .2472311\n         94  |  -.2315187   .1288964    -1.80   0.073    -.4847991    .0217617\n         95  |  -.5423296   .1395023    -3.89   0.000    -.8164505   -.2682088\n         96  |  -.0991001   .2025628    -0.49   0.625    -.4971343    .2989342\n         97  |   .0848356   .1679777     0.51   0.614    -.2452392    .4149104\n         98  |  -.0877863   .1183023    -0.74   0.458    -.3202494    .1446767\n         99  |   -.127322   .1556316    -0.82   0.414    -.4331369    .1784928\n------------------------------------------------------------------------------\n\n. restore\ngrade is not estimated in the fixed effects model because it is time-invarying; within each individual it is constant. In the regress model, we are able to estimate it.\nxtreg reports 3 R-squared statistics; this is a within variance model so we can use that value (which agrees with the regression R-squared). Note that the regress R-squared estimate is artificially inflated due to the massive amount of predictors."
  },
  {
    "objectID": "xtsetvsmixed.html#xtreg-be-between-effect-model-between-variance",
    "href": "xtsetvsmixed.html#xtreg-be-between-effect-model-between-variance",
    "title": "Stata’s mixed versus xtreg",
    "section": "xtreg, be: Between Effect model (Between variance)",
    "text": "xtreg, be: Between Effect model (Between variance)\nThe between effects results are\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, be\n\nBetween regression (regression on group means)  Number of obs     =     28,091\nGroup variable: idcode                          Number of groups  =      4,697\n\nR-squared:                                      Obs per group:\n     Within  = 0.1427                                         min =          1\n     Between = 0.4787                                         avg =        6.0\n     Overall = 0.3562                                         max =         15\n\n                                                F(6,4690)         =     717.89\nsd(u_i + avg(e_i.)) = .3068161                  Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |    .064188   .0019539    32.85   0.000     .0603575    .0680185\n         age |  -.0041071   .0010618    -3.87   0.000    -.0061886   -.0020255\n     ttl_exp |   .0287514    .002033    14.14   0.000     .0247658    .0327369\n      tenure |   .0286782   .0022174    12.93   0.000      .024331    .0330254\n    not_smsa |   -.175568   .0111952   -15.68   0.000    -.1975159   -.1536202\n       south |  -.1086271   .0098438   -11.04   0.000    -.1279256   -.0893287\n       _cons |   .8066724   .0329873    24.45   0.000     .7420017     .871343\n------------------------------------------------------------------------------\nTo replicate, collapse over idcode and run a regression:\n. preserve\n\n. collapse (mean) ln_wage grade age ttl_exp tenure not_smsa south, by(idcode)\n\n. reg ln_wage grade age ttl_exp tenure not_smsa south\n\n      Source |       SS           df       MS      Number of obs   =     4,697\n-------------+----------------------------------   F(6, 4690)      =    724.96\n       Model |  406.076398         6  67.6793996   Prob &gt; F        =    0.0000\n    Residual |  437.838178     4,690  .093355688   R-squared       =    0.4812\n-------------+----------------------------------   Adj R-squared   =    0.4805\n       Total |  843.914575     4,696  .179709237   Root MSE        =    .30554\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0642015   .0019467    32.98   0.000     .0603849     .068018\n         age |  -.0039009   .0010611    -3.68   0.000    -.0059812   -.0018206\n     ttl_exp |   .0285569   .0020267    14.09   0.000     .0245836    .0325301\n      tenure |   .0288392   .0022077    13.06   0.000      .024511    .0331673\n    not_smsa |  -.1758847   .0111494   -15.78   0.000    -.1977428   -.1540266\n       south |  -.1080377   .0098055   -11.02   0.000    -.1272611   -.0888142\n       _cons |   .8000955   .0328792    24.33   0.000     .7356368    .8645542\n------------------------------------------------------------------------------\n\n. restore\nAgain, the coefficients agree to three decimals and the between R-square agrees.\nAll predictors here are estimated; if we had any time-variant by individual-invariant predictors (e.g. time), they would not be estimable here."
  },
  {
    "objectID": "xtsetvsmixed.html#xtreg-re-random-effect-model-both-variances",
    "href": "xtsetvsmixed.html#xtreg-re-random-effect-model-both-variances",
    "title": "Stata’s mixed versus xtreg",
    "section": "xtreg, re: Random Effect model (Both variances)",
    "text": "xtreg, re: Random Effect model (Both variances)\nThe random effects results are\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, re\n\nRandom-effects GLS regression                   Number of obs     =     28,091\nGroup variable: idcode                          Number of groups  =      4,697\n\nR-squared:                                      Obs per group:\n     Within  = 0.1483                                         min =          1\n     Between = 0.4701                                         avg =        6.0\n     Overall = 0.3569                                         max =         15\n\n                                                Wald chi2(6)      =    8304.62\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0691836   .0017689    39.11   0.000     .0657166    .0726506\n         age |  -.0038386   .0006544    -5.87   0.000    -.0051212   -.0025559\n     ttl_exp |   .0301313   .0011215    26.87   0.000     .0279331    .0323294\n      tenure |   .0134656   .0008442    15.95   0.000      .011811    .0151202\n    not_smsa |   -.128591   .0072246   -17.80   0.000     -.142751    -.114431\n       south |  -.0932646    .007231   -12.90   0.000     -.107437   -.0790921\n       _cons |   .7544109   .0273445    27.59   0.000     .7008168    .8080051\n-------------+----------------------------------------------------------------\n     sigma_u |  .26027808\n     sigma_e |  .29477563\n         rho |  .43808743   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nJust fit a regular mixed model:\n. mixed ln_wage grade age ttl_exp tenure not_smsa south || idcode:\n\nPerforming EM optimization ...\n\nPerforming gradient-based optimization: \nIteration 0:  Log likelihood = -9218.9773  \nIteration 1:  Log likelihood = -9218.9773  \n\nComputing standard errors ...\n\nMixed-effects ML regression                         Number of obs    =  28,091\nGroup variable: idcode                              Number of groups =   4,697\n                                                    Obs per group:\n                                                                 min =       1\n                                                                 avg =     6.0\n                                                                 max =      15\n                                                    Wald chi2(6)     = 8496.81\nLog likelihood = -9218.9773                         Prob &gt; chi2      =  0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0691186   .0017231    40.11   0.000     .0657414    .0724959\n         age |   -.003869   .0006491    -5.96   0.000    -.0051411   -.0025969\n     ttl_exp |    .030151   .0011135    27.08   0.000     .0279687    .0323334\n      tenure |    .013591   .0008441    16.10   0.000     .0119365    .0152454\n    not_smsa |  -.1299789    .007154   -18.17   0.000    -.1440004   -.1159575\n       south |  -.0941264   .0071291   -13.20   0.000    -.1080991   -.0801537\n       _cons |   .7566548   .0267655    28.27   0.000     .7041954    .8091142\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n  Random-effects parameters  |   Estimate   Std. err.     [95% conf. interval]\n-----------------------------+------------------------------------------------\nidcode: Identity             |\n                  var(_cons) |   .0626522   .0017678      .0592815    .0662147\n-----------------------------+------------------------------------------------\n               var(Residual) |    .087569    .000811      .0859938    .0891732\n------------------------------------------------------------------------------\nLR test vs. linear model: chibar2(01) = 7277.75       Prob &gt;= chibar2 = 0.0000\nThe results are very close; we can get even closer by fitting the xtreg model with the mle option, which uses a different estimation strategy.\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, re mle\n\nFitting constant-only model:\nIteration 0:  Log likelihood = -12663.954\nIteration 1:  Log likelihood = -12649.756\nIteration 2:  Log likelihood = -12649.614\nIteration 3:  Log likelihood = -12649.614\n\nFitting full model:\nIteration 0:  Log likelihood = -9271.8615\nIteration 1:  Log likelihood = -9219.1214\nIteration 2:  Log likelihood = -9218.9773\nIteration 3:  Log likelihood = -9218.9773\n\nRandom-effects ML regression                        Number of obs    =  28,091\nGroup variable: idcode                              Number of groups =   4,697\n\nRandom effects u_i ~ Gaussian                       Obs per group:\n                                                                 min =       1\n                                                                 avg =     6.0\n                                                                 max =      15\n\n                                                    LR chi2(6)       = 6861.27\nLog likelihood = -9218.9773                         Prob &gt; chi2      =  0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0691186   .0017233    40.11   0.000     .0657411    .0724962\n         age |   -.003869   .0006491    -5.96   0.000    -.0051413   -.0025967\n     ttl_exp |    .030151   .0011135    27.08   0.000     .0279687    .0323334\n      tenure |    .013591   .0008454    16.08   0.000     .0119341    .0152478\n    not_smsa |  -.1299789   .0071711   -18.13   0.000     -.144034   -.1159239\n       south |  -.0941264   .0071356   -13.19   0.000    -.1081119   -.0801409\n       _cons |   .7566548   .0267773    28.26   0.000     .7041722    .8091374\n-------------+----------------------------------------------------------------\n    /sigma_u |   .2503043   .0035313                      .2434779    .2573221\n    /sigma_e |   .2959207   .0013704                       .293247    .2986188\n         rho |   .4170663   .0074745                      .4024774    .4317704\n------------------------------------------------------------------------------\nLR test of sigma_u=0: chibar2(01) = 7277.75            Prob &gt;= chibar2 = 0.000"
  }
]