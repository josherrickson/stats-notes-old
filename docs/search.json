[
  {
    "objectID": "logitinterpretation.html",
    "href": "logitinterpretation.html",
    "title": "Logistic Models and the Margins Command",
    "section": "",
    "text": "When looking at the results of a logistic model, there are several different measures of the relationship between the predictors and the probability of a positive outcome that can be used to interpret the model:\nIf you are unclear which you are looking at, confusion can abound. This is doubly-confounded in Stata (in my opinion) where certain margins commands will produce a different measure than perhaps expected."
  },
  {
    "objectID": "logitinterpretation.html#probabilities",
    "href": "logitinterpretation.html#probabilities",
    "title": "Logistic Models and the Margins Command",
    "section": "Probabilities",
    "text": "Probabilities\nLet’s look at the probabilities. Here we have conditional probabilities since we have a predictor. So we are interested in \\(P(\\textrm{foreign} | \\textrm{high mileage})\\) and \\(P(\\textrm{foreign} | \\textrm{low mileage})\\).\nFrom the table above, we can easily compute this:\n. tab foreign highmileage, col\n\n+-------------------+\n| Key               |\n|-------------------|\n|     frequency     |\n| column percentage |\n+-------------------+\n\n           |      highmileage\nCar origin | Low Milea  High Mile |     Total\n-----------+----------------------+----------\n  Domestic |        45          7 |        52 \n           |     75.00      50.00 |     70.27 \n-----------+----------------------+----------\n   Foreign |        15          7 |        22 \n           |     25.00      50.00 |     29.73 \n-----------+----------------------+----------\n     Total |        60         14 |        74 \n           |    100.00     100.00 |    100.00 \nWe see \\(P(\\textrm{foreign} | \\textrm{high mileage}) =  .25\\) and \\(P(\\textrm{foreign} | \\textrm{low mileage}) = .5\\).\nWe can also obtain these via margins1:\n. margins highmileage\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n highmileage |\nLow Mileage  |        .25   .0559017     4.47   0.000     .1404347    .3595653\nHigh Mile..  |         .5   .1336306     3.74   0.000     .2380888    .7619112\n------------------------------------------------------------------------------\nWe can also test for equality between these percentages:\n. margins highmileage, pwcompare(pv)\n\nPairwise comparisons of adjusted predictions                Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n\n---------------------------------------------------------------------\n                             |            Delta-method    Unadjusted\n                             |   Contrast   std. err.      z    P&gt;|z|\n-----------------------------+---------------------------------------\n                 highmileage |\nHigh Mileage vs Low Mileage  |        .25   .1448521     1.73   0.084\n---------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#odds",
    "href": "logitinterpretation.html#odds",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds",
    "text": "Odds\nWe can compute the odds using the formulas above, giving us\n\\[\n    \\textrm{Odds}(\\textrm{foreign} | \\textrm{high mileage}) =         1\n\\]\nand\n\\[\n    \\textrm{Odds}(\\textrm{foreign} | \\textrm{low mileage}) = =     .3333.\n\\]\nTo obtain with margins, we again pass the expression option:\n. margins highmileage, expression(exp(xb()))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: exp(xb())\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n highmileage |\nLow Mileage  |   .3333333   .0993808     3.35   0.001     .1385505    .5281161\nHigh Mile..  |          1   .5345225     1.87   0.061    -.0476448    2.047645\n------------------------------------------------------------------------------\nNote that we do not want to test if the odds are different using pwcompare as that’s what the odds ratio is for!"
  },
  {
    "objectID": "logitinterpretation.html#odds-ratio",
    "href": "logitinterpretation.html#odds-ratio",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds ratio",
    "text": "Odds ratio\nThe odds ratio is often very confusing to interpret, but is straightforward: An odds ratio predicts the number of positive outcomes we expect to see for every negative outcome. So an odds ratio of 2 would mean for every domestic car, we’d expect to see 2 foreign cars. An odds ratio of .25 would mean for every domestic car, we’d expect .25 foreign cars - or, for every 4 domestic cars, we’d expect 1 foreign car (since .25 = 1/4).\nThe odds ratio is literally the ratio of the odds.\n\\[\n    \\textrm{OR}(\\textrm{foreign} | \\textrm{high mileage}) = \\frac{\\textrm{odds}(\\textrm{foreign} | \\textrm{high mileage})}{\\textrm{odds}(\\textrm{foreign} | \\textrm{low mileage})} = 1/.333 = 3\n\\]\nLooking at the regression results again:\n. logit, or\n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(1)    =   3.18\n                                                        Prob &gt; chi2   = 0.0746\nLog likelihood = -43.444169                             Pseudo R2     = 0.0353\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n highmileage |\nHigh Mile..  |          3   1.836145     1.79   0.073     .9039507    9.956295\n       _cons |   .3333333   .0993808    -3.68   0.000      .185823    .5979406\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\nThe intercept is \\(\\textrm{odds}(\\textrm{foreign} | \\textrm{low mileage})\\), the odds of a positive outcome in the baseline group, and the coefficient on highmileage is the odds ratio!\nNote that we cannot use the margins command to obtain the odds ratio2. Instead, we use lincom:\n. lincom _b[1.highmileage], or\n\n ( 1)  [foreign]1.highmileage = 0\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |          3   1.836145     1.79   0.073     .9039507    9.956295\n------------------------------------------------------------------------------\n(I obtained the _b[1.highmileage] name by running logit, coeflegend to obtain the legend.) Note the or option, without it we obtain the log odds."
  },
  {
    "objectID": "logitinterpretation.html#probabilities-1",
    "href": "logitinterpretation.html#probabilities-1",
    "title": "Logistic Models and the Margins Command",
    "section": "Probabilities",
    "text": "Probabilities\n. tab foreign pricecat, col\n\n+-------------------+\n| Key               |\n|-------------------|\n|     frequency     |\n| column percentage |\n+-------------------+\n\n           |             pricecat\nCar origin |         1          2          3 |     Total\n-----------+---------------------------------+----------\n  Domestic |        42          2          8 |        52 \n           |     71.19      40.00      80.00 |     70.27 \n-----------+---------------------------------+----------\n   Foreign |        17          3          2 |        22 \n           |     28.81      60.00      20.00 |     29.73 \n-----------+---------------------------------+----------\n     Total |        59          5         10 |        74 \n           |    100.00     100.00     100.00 |    100.00 \n. margins pricecat\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    pricecat |\n          1  |   .2881356   .0589618     4.89   0.000     .1725725    .4036987\n          2  |         .6    .219089     2.74   0.006     .1705934    1.029407\n          3  |         .2   .1264911     1.58   0.114     -.047918     .447918\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#odds-1",
    "href": "logitinterpretation.html#odds-1",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds",
    "text": "Odds\nThe intercept is the odds of a foreign car in pricecat 1, or .2881/.7119 = .4048. We can obtain the odds of each pricecat in the typical way.\n. margins pricecat, expression(exp(xb()))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: exp(xb())\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    pricecat |\n          1  |   .4047619   .1163527     3.48   0.001     .1767148     .632809\n          2  |        1.5   1.369306     1.10   0.273    -1.183791    4.183791\n          3  |        .25   .1976424     1.26   0.206    -.1373719    .6373719\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#odds-ratios",
    "href": "logitinterpretation.html#odds-ratios",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds ratios",
    "text": "Odds ratios\nFinally, the two coefficients in the model are the odds ratios of being in pricecat 2 or 3 versus 1. Again we can use lincom to obtain these.\n. lincom _b[2.pricecat], or\n\n ( 1)  [foreign]2.pricecat = 0\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |   3.705882   3.546757     1.37   0.171     .5678577    24.18487\n------------------------------------------------------------------------------\n\n. lincom _b[3.pricecat], or\n\n ( 1)  [foreign]3.pricecat = 0\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |   .6176471   .5195704    -0.57   0.567     .1187686    3.212026\n------------------------------------------------------------------------------\nNote here that multiplying the odds ratios by the odds in pricecat 1 (the intercept) gives the odds in the other group. E.g. 3.705*.4047 = 1.5."
  },
  {
    "objectID": "logitinterpretation.html#probabilities-2",
    "href": "logitinterpretation.html#probabilities-2",
    "title": "Logistic Models and the Margins Command",
    "section": "Probabilities",
    "text": "Probabilities\nWe cannot look at crosstabs as we did before the compute probabilities, but the margins command still works.\n. margins, at(headroom = (2.5 5))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n1._at: headroom = 2.5\n2._at: headroom =   5\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at |\n          1  |   .3674795   .0655664     5.60   0.000     .2389717    .4959873\n          2  |   .0659516    .050366     1.31   0.190    -.0327639     .164667\n------------------------------------------------------------------------------\nThese are the predicted probabilties of a positive outcome at the referenced levels of headroom, i.e. \\(P(\\textrm{foreign} | \\textrm{headroom} = 2.5)\\) and \\(P(\\textrm{foreign} | \\textrm{headroom} = 5)\\)."
  },
  {
    "objectID": "logitinterpretation.html#odds-2",
    "href": "logitinterpretation.html#odds-2",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds",
    "text": "Odds\nWe can directly compute the odds given the probabilities above, but it’s easier to continue using margins.\n. margins, at(headroom = (2.5 5)) expression(exp(xb()))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: exp(xb())\n1._at: headroom = 2.5\n2._at: headroom =   5\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at |\n          1  |   .5809764   .1638824     3.55   0.000     .2597729    .9021799\n          2  |   .0706083   .0577296     1.22   0.221    -.0425396    .1837562\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#odds-ratio-1",
    "href": "logitinterpretation.html#odds-ratio-1",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds ratio",
    "text": "Odds ratio\nThe coefficient in the logistic regression is interpreted as the odds ratio when increasing headroom by 1. In other words, if we had a collection of cars with headroom of \\(x\\) and magically change their headroom to \\(x + 1\\), we would expect for every one additional domestic car, we’d see &lt;&lt;dd_display: %9.4f exp(_b[headroom])&gt;&gt; additional foreign cars.\nWe can obtain this odds ratio by again using lincom.\n. lincom _b[headroom], or\n\n ( 1)  [foreign]headroom = 0\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |   .4304066   .1490474    -2.43   0.015     .2183295    .8484873\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "logitinterpretation.html#probabilities-3",
    "href": "logitinterpretation.html#probabilities-3",
    "title": "Logistic Models and the Margins Command",
    "section": "Probabilities",
    "text": "Probabilities\nBecause we have two categorical predictors, we can return to looking at crosstabs as a way of obtaining probabilities. The margins call will also return them.\n. table foreign highmileage highprice\n\n----------------------------------------------------\n                   |             highprice          \n                   |  Low Price   High Price   Total\n-------------------+--------------------------------\nCar origin         |                                \n  Domestic         |                                \n    highmileage    |                                \n      Low Mileage  |         23           22      45\n      High Mileage |          6            1       7\n      Total        |         29           23      52\n  Foreign          |                                \n    highmileage    |                                \n      Low Mileage  |          2           13      15\n      High Mileage |          6            1       7\n      Total        |          8           14      22\n  Total            |                                \n    highmileage    |                                \n      Low Mileage  |         25           35      60\n      High Mileage |         12            2      14\n      Total        |         37           37      74\n----------------------------------------------------\n\n. margins highprice#highmileage\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: Pr(foreign), predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   highprice#|\n highmileage |\n  Low Price #|\nLow Mileage  |        .08   .0542586     1.47   0.140     -.026345     .186345\n  Low Price #|\nHigh Mile..  |         .5   .1443376     3.46   0.001     .2171036    .7828964\n High Price #|\nLow Mileage  |   .3714286   .0816735     4.55   0.000     .2113515    .5315056\n High Price #|\nHigh Mile..  |         .5   .3535534     1.41   0.157    -.1929519    1.192952\n------------------------------------------------------------------------------\nThis is similar to the categorical predictor, where there are four groups. For example, low price and low mileage, 2 out of 25 cars are foreign, so the probability is 2/25 = .08."
  },
  {
    "objectID": "logitinterpretation.html#odds-3",
    "href": "logitinterpretation.html#odds-3",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds",
    "text": "Odds\n. margins highprice#highmileage, expression(exp(xb()))\n\nAdjusted predictions                                        Number of obs = 74\nModel VCE: OIM\n\nExpression: exp(xb())\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   highprice#|\n highmileage |\n  Low Price #|\nLow Mileage  |   .0869565   .0641052     1.36   0.175    -.0386874    .2126004\n  Low Price #|\nHigh Mile..  |          1   .5773503     1.73   0.083    -.1315857    2.131586\n High Price #|\nLow Mileage  |   .5909091   .2067148     2.86   0.004     .1857554    .9960628\n High Price #|\nHigh Mile..  |          1   1.414214     0.71   0.480    -1.771808    3.771808\n------------------------------------------------------------------------------\nIf you look at the logistic results above, the baseline categories are low mileage and low price. So, as before, the intercept is the odds of a foreign car in that subcategory, which we see here.\nWe do not obtain the odds for the other categories in the regression output."
  },
  {
    "objectID": "logitinterpretation.html#odds-ratios-1",
    "href": "logitinterpretation.html#odds-ratios-1",
    "title": "Logistic Models and the Margins Command",
    "section": "Odds ratios",
    "text": "Odds ratios\nThe odds ratios reported in the regression output only present part of the story. Let’s take a look at them again.\n. logit, or\n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(3)    =  10.54\n                                                        Prob &gt; chi2   = 0.0145\nLog likelihood = -39.763201                             Pseudo R2     = 0.1170\n\n------------------------------------------------------------------------------\n     foreign | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   highprice |\n High Price  |   6.795455    5.54509     2.35   0.019     1.372897    33.63558\n             |\n highmileage |\nHigh Mile..  |       11.5    10.7684     2.61   0.009      1.83505    72.06888\n             |\n   highprice#|\n highmileage |\n High Price #|\nHigh Mile..  |   .1471572   .2548493    -1.11   0.269     .0049392    4.384364\n             |\n       _cons |   .0869565   .0641052    -3.31   0.001     .0205016    .3688215\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\nThe coefficient on highprice is the odds ratio of being foreign between high price and low price cars, in the low mileage category.\n\\[\n    \\frac{\\textrm{OR}(\\textrm{foreign}|\\textrm{high price, low mileage})}{\\textrm{OR}(\\textrm{foreign}|\\textrm{low price, low mileage})}\n\\]\nThe coefficient on highmileage is the odds ratio of being foreign between high mileage and low mileage cars, in the low price category.\n\\[\n    \\frac{\\textrm{OR}(\\textrm{foreign}|\\textrm{low price, high mileage})}{\\textrm{OR}(\\textrm{foreign}|\\textrm{low price, low mileage})}\n\\]\nThe interaction can be interpreted in one of two ways."
  },
  {
    "objectID": "logitinterpretation.html#footnotes",
    "href": "logitinterpretation.html#footnotes",
    "title": "Logistic Models and the Margins Command",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that if you do not flag highmileage as a categorical with i., you can use instead margins, over(highmileage). If you pass a continuous variable, it will compute the probability at each discrete value of the continuous variable.↩︎\nIf I’m wrong, please let me know!↩︎"
  },
  {
    "objectID": "divideby4.html",
    "href": "divideby4.html",
    "title": "The “Divide by 4” Rule",
    "section": "",
    "text": "In addition to the odds ratio interpretation of coefficients in a logistic regression model, the “divide by 4” rule can also help with interpretation.\n. sysuse auto\n(1978 automobile data)\n\n. logit foreign mpg\n\nIteration 0:  Log likelihood =  -45.03321  \nIteration 1:  Log likelihood = -39.380959  \nIteration 2:  Log likelihood = -39.288802  \nIteration 3:  Log likelihood =  -39.28864  \nIteration 4:  Log likelihood =  -39.28864  \n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(1)    =  11.49\n                                                        Prob &gt; chi2   = 0.0007\nLog likelihood = -39.28864                              Pseudo R2     = 0.1276\n\n------------------------------------------------------------------------------\n     foreign | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |   .1597621   .0525876     3.04   0.002     .0566922     .262832\n       _cons |  -4.378866   1.211295    -3.62   0.000    -6.752961   -2.004771\n------------------------------------------------------------------------------\nThe coefficient on mpg, .1597, is the log odds. The “divide by 4” rule says that the log odds divided by 4 (so \\(.1597/4 = .0399\\)) is the maximum difference in predicted probability for a 1-unit change in mpg.\nIn other words, consider the following plot of predicted probabilities.\n. quiet margins, at(mpg = (12(1)41)) nose\n\n. marginsplot, ///\n&gt;     recast(line) ///\n&gt;     addplot(pci .5 12 .5 41, lpattern(shortdash) lcolor(black)  || ///\n&gt;             pci 0 `=4.378866/.1597621' 1 `=4.378866/.1597621', ///\n&gt;             lpattern(shortdash) lcolor(black)) ///\n&gt;     legend(off)\n\nVariables that uniquely identify margins: mpg\n\nThe line represents the predicted probability of the outcome being 1 as mpg varies. In linear regression, this would be a straight line, and the difference in predicted probability between mpg = 14 versus mpg = 15 would be equivalent to the difference in predicted probability between mpg = 30 versus mpg = 31. However, since this is a logistic curve, the difference in predicted probability varies over mpg, being sharpest near the middle. Specifically, at the point when the predicted probability is exactly .5 (the dashed lines), the slope is the largest, and it recedes from there.\nThis means that at that middle point (around mpg = 27), the increasing mpg by 1 is increasing the predicted probability by .0399. For all other places on that curve, the increased probability is less than .0399."
  },
  {
    "objectID": "responsesurfaceplot.html",
    "href": "responsesurfaceplot.html",
    "title": "Response Surface Plot",
    "section": "",
    "text": "Introduction\nResponse surface analysis is just regression with an interaction. Typically the model fit is\n\\[\n    y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2 + \\beta_4x_1^2 + \\beta_5x_2^2,\n\\]\nbut in general any interaction should work for the below visualization.\n\n\nUsing rsm::persp.lm\nThe “rsm” package allegedly does response surface modeling automatically, but we can use it just for the rsm::persp.lm function (which can just be referred to as persp after loading the package). Note that stats::persp does not work directly with a model object; you’d need to do a few extra steps to generate a matrix first.\n\nlibrary(rsm)\n\ndata(mtcars)\n\nmod1 &lt;- lm(mpg ~ disp + qsec + disp:qsec + I(disp^2) + I(qsec^2), data = mtcars)\n\nrsm::persp.lm(mod1, disp ~ qsec, zlab = \"mpg\", theta = 120, phi = 10, r = 10)\n\n\n\n\n(Note: persp.lm does not support other types of models such as mixed models from lmer, however plotly below should support all types of models that emmeans supports.)\n\n\nUsing plotly\nAlternately, plotly can be used to create an interactive plot. The data needs to be set up in a bit of an odd fashion, instead of a matrix of x-y-z triads, a list is created with the unique x values, the unique y values, and a matrix of predicted values of the outcome containing all pairwise values of x and y. We use emmeans to generate these predicted values.\n\nlibrary(plotly)\nlibrary(emmeans)\n\n\n# Set up grid\nq &lt;- 14:23\nd &lt;- (1:10)*50\n\nem &lt;- as.data.frame(emmeans(mod1, ~ qsec*disp, at = list(qsec = q, disp = d)))\n\n# x & y are unique values, z is len(x) x len(y) matrix\ndd &lt;- list(q = q,\n           d = d,\n           m = matrix(em$emmean,\n                      nrow = length(q),\n                      ncol = length(d),\n                      byrow = TRUE))\n\nplot_ly(x = dd$q, y = dd$d, z = dd$m, type = \"surface\") |&gt;\n  layout(scene = list(xaxis=list(title = \"qsec\"),\n                      yaxis=list(title = \"disp\"),\n                      zaxis=list(title = \"mpg\")))\n\n\n\n\n\n\n\nA Warning about Interpretation\nResponse surface plots can be extremely misleading due to extrapolation. Consider the plots above. One thing we would be tempted to interpret from the plot is to predict that the lowest mileage cars are around 5 mpg, and this occurs when disp and qsec are at their maximum. However, this is complete extrapolation. We can look at the predicted values for the cars in the data:\n\nsummary(predict(mod1))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  13.10   15.36   19.53   20.09   24.28   30.13 \n\n\nThe lowest predicted mileage is 13.1, yet the response surface dips below 5! What’s happening is that there are no cars in the car with both high disp and high qsec, so that area of the response curve is entirely extrapolated - we have no idea what should be occurring there.\n\nplot(mtcars$disp ~ mtcars$qsec)\ntext(21, 400, \"There's no data in this region!\", col = \"red\")\n\n\n\n\nFinally, let’s add the predicted values onto the response surface to further visualize the areas of the response surface that is pure extrapolation. (Note that I add one to the predicted values (predict(mod1) + 1) to move the points slightly above the surface, improving visualization.)\n\ndf &lt;- data.frame(qsec = mtcars$qsec,\n                 disp = mtcars$disp,\n                 predict = predict(mod1) + 1)\n\n\nplot_ly(x = dd$q, y = dd$d, z = dd$m, type = \"surface\") |&gt;\n  layout(scene = list(xaxis=list(title = \"qsec\"),\n                      yaxis=list(title = \"disp\"),\n                      zaxis=list(title = \"mpg\"))) |&gt;\n  add_trace(x = df$qsec, y = df$disp, z = df$predict, mode = \"markers\",\n            type = \"scatter3d\",\n            marker = list(size = 10, color = \"red\", symbol = 104))"
  },
  {
    "objectID": "xtsetvsmixed.html",
    "href": "xtsetvsmixed.html",
    "title": "Stata’s mixed versus xtreg",
    "section": "",
    "text": "In Stata, panel data (repeated measures) can be modeled using mixed (and its siblings e.g. melogit, mepoisson) or using the xt toolkit, including xtset and xtreg.\nThis document is an attempt to show the equivalency of the models between the two commands. There will be slight differences due to the algorithms used in the backend but the results should generally be equivalent."
  },
  {
    "objectID": "xtsetvsmixed.html#the-math",
    "href": "xtsetvsmixed.html#the-math",
    "title": "Stata’s mixed versus xtreg",
    "section": "The Math",
    "text": "The Math\nPicture a typical mixed model setup:\n\\[\n    y\\_{it} = \\alpha + x\\_{it}\\beta + \\nu\\_i + \\epsilon\\_{it}.\n\\]\nHere \\(i\\) is an index for individuals, \\(t\\) is an index for time. \\(y\\_{it}\\) and \\(x\\_{it}\\) are some outcome and predictor which are both time and individual varying. \\(\\nu\\_{i}\\) is an error associated with each individual and \\(\\epsilon\\_{it}\\) is an additional error per observation.\nIf this model is true, then the following must be true:\n\\[\n    \\overline{y}\\_i = \\alpha + \\overline{x}\\_i\\beta + \\nu\\_i + \\overline{\\epsilon\\_i}.\n\\]\nEach bar’d variable is average over each individual. In this model, \\(\\nu\\_i\\) and \\(\\overline{\\epsilon}\\_i\\) are indistinguishable, so this is just a linear model.\nSince we have that both models are equivalent, if we difference them, we remain equivalent:\n\\[\n    (y\\_{it} - \\overline{y}\\_i) = (x\\_{it} - \\overline{x}\\_i)\\beta + (\\epsilon\\_{it} - \\overline{\\epsilon}\\_i).\n\\]\nAgain, we have just a linear model.\nFinally, the random effects model doesn’t add much clarity, but it essentially is a weighted combination of the other two, with the weight being a function of the variance of \\(\\nu\\_i\\) and \\(\\epsilon\\_i\\). If the variance of \\(\\nu\\_i\\) is 0, then there’s no individual level effect and the first model can be fit lineally (because \\(\\nu\\_i\\) is constant and folds into the intercept)."
  },
  {
    "objectID": "xtsetvsmixed.html#assumptions",
    "href": "xtsetvsmixed.html#assumptions",
    "title": "Stata’s mixed versus xtreg",
    "section": "Assumptions",
    "text": "Assumptions\nThere is one key different assumption between the models:\nThe random effects model assumes that unobservable variables are uncorrelated with other covariates. The other models don’t.\nThe between effects and random effects models assume that \\(\\nu\\_i\\) and \\(\\overline{x}\\_i\\) are uncorrelated (individual intercepts are independent of predictors)."
  },
  {
    "objectID": "xtsetvsmixed.html#overall-variation",
    "href": "xtsetvsmixed.html#overall-variation",
    "title": "Stata’s mixed versus xtreg",
    "section": "Overall variation",
    "text": "Overall variation\nEasy:\n. summ ln_wage\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     ln_wage |     28,534    1.674907    .4780935          0   5.263916"
  },
  {
    "objectID": "xtsetvsmixed.html#within-variation",
    "href": "xtsetvsmixed.html#within-variation",
    "title": "Stata’s mixed versus xtreg",
    "section": "Within variation",
    "text": "Within variation\nTaking our cue from the notes in the theory, to obtain within variation we will center the variable by individual.\n. egen meanln_wage = mean(ln_wage), by(idcode)\n\n. gen cln_wage = ln_wage - meanln_wage\n\n. summ cln_wage\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n    cln_wage |     28,534   -2.83e-10      .29266  -2.082629   3.108763\nNote that the mean is 0 (within rounding error), as we’d expect. To get the mean/min/max back into the same scale as the raw data we can re-add the overall mean to\n. egen overallmean = mean(ln_wage)\n\n. gen cln_wage2 = cln_wage + overallmean\n\n. summ cln_wage2\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   cln_wage2 |     28,534    1.674907      .29266  -.4077221   4.783669\nThis works because each individual has mean of 0 or in the second case, overallmean, in either case, since the means are constant, we’ve removed any between variance and isolated the within variance."
  },
  {
    "objectID": "xtsetvsmixed.html#between-variation",
    "href": "xtsetvsmixed.html#between-variation",
    "title": "Stata’s mixed versus xtreg",
    "section": "Between variation",
    "text": "Between variation\nWe simply collapse by id.\n. preserve\n\n. collapse (mean) ln_wage, by(idcode)\n\n. summ ln_wage\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     ln_wage |      4,711    1.650605     .424569          0   3.912023\n\n. restore\nDoing this works because each subject now has a single observation, hence the within variance is identically 0, so the remaining variance is between-variance."
  },
  {
    "objectID": "xtsetvsmixed.html#xtreg-fe-fixed-effect-model-within-variance",
    "href": "xtsetvsmixed.html#xtreg-fe-fixed-effect-model-within-variance",
    "title": "Stata’s mixed versus xtreg",
    "section": "xtreg, fe: Fixed Effect model (Within variance)",
    "text": "xtreg, fe: Fixed Effect model (Within variance)\nThe fixed effects results are\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, fe\nnote: grade omitted because of collinearity.\n\nFixed-effects (within) regression               Number of obs     =     28,091\nGroup variable: idcode                          Number of groups  =      4,697\n\nR-squared:                                      Obs per group:\n     Within  = 0.1491                                         min =          1\n     Between = 0.3526                                         avg =        6.0\n     Overall = 0.2517                                         max =         15\n\n                                                F(5, 23389)       =     819.94\ncorr(u_i, Xb) = 0.2348                          Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |          0  (omitted)\n         age |  -.0026787    .000863    -3.10   0.002    -.0043703   -.0009871\n     ttl_exp |   .0287709   .0014474    19.88   0.000     .0259339    .0316079\n      tenure |   .0114355   .0009229    12.39   0.000     .0096265    .0132445\n    not_smsa |  -.0921689   .0096641    -9.54   0.000    -.1111112   -.0732266\n       south |  -.0633396   .0110819    -5.72   0.000    -.0850608   -.0416184\n       _cons |   1.591678   .0186849    85.19   0.000     1.555054    1.628302\n-------------+----------------------------------------------------------------\n     sigma_u |  .36167618\n     sigma_e |  .29477563\n         rho |  .60086475   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(4696, 23389) = 6.63                 Prob &gt; F = 0.0000\nTo replicate, we’ll include idcode as a categorical variable. One of the benefits of xtreg ..., fe is efficiency; since there are over 4000 idcode, the regression model will fail to run. Consequently, we’ll demostrate on a subset of the data\n. preserve\n\n. keep if idcode &lt; 100\n(27,959 observations deleted)\n\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, fe\nnote: grade omitted because of collinearity.\n\nFixed-effects (within) regression               Number of obs     =        567\nGroup variable: idcode                          Number of groups  =         89\n\nR-squared:                                      Obs per group:\n     Within  = 0.1895                                         min =          1\n     Between = 0.2312                                         avg =        6.4\n     Overall = 0.1900                                         max =         15\n\n                                                F(5, 473)         =      22.12\ncorr(u_i, Xb) = 0.1420                          Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |          0  (omitted)\n         age |  -.0045718   .0054274    -0.84   0.400    -.0152366    .0060931\n     ttl_exp |   .0313655   .0091803     3.42   0.001     .0133263    .0494046\n      tenure |   .0149681   .0061975     2.42   0.016     .0027901    .0271462\n    not_smsa |   .0003284   .0743637     0.00   0.996    -.1457957    .1464524\n       south |   .1353543    .170972     0.79   0.429    -.2006042    .4713129\n       _cons |   1.770705   .1125176    15.74   0.000     1.549609    1.991801\n-------------+----------------------------------------------------------------\n     sigma_u |  .35036238\n     sigma_e |  .26779328\n         rho |   .6312319   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(88, 473) = 9.01                     Prob &gt; F = 0.0000\n\n. reg ln_wage grade age ttl_exp tenure not_smsa south i.idcode, noconstant\n\n      Source |       SS           df       MS      Number of obs   =       567\n-------------+----------------------------------   F(94, 473)      =    305.03\n       Model |  2056.18728        94  21.8743328   Prob &gt; F        =    0.0000\n    Residual |  33.9203619       473  .071713239   R-squared       =    0.9838\n-------------+----------------------------------   Adj R-squared   =    0.9805\n       Total |  2090.10764       567  3.68625687   Root MSE        =    .26779\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .1664948   .0111166    14.98   0.000     .1446507    .1883389\n         age |  -.0045718   .0054274    -0.84   0.400    -.0152366    .0060931\n     ttl_exp |   .0313655   .0091803     3.42   0.001     .0133263    .0494046\n      tenure |   .0149681   .0061975     2.42   0.016     .0027901    .0271462\n    not_smsa |   .0003284   .0743637     0.00   0.996    -.1457957    .1464524\n       south |   .1353543    .170972     0.79   0.429    -.2006042    .4713129\n             |\n      idcode |\n          2  |  -.4226575   .1098026    -3.85   0.000    -.6384187   -.2068963\n          3  |  -.5725985    .104336    -5.49   0.000    -.7776178   -.3675791\n          4  |  -.9436944   .1389879    -6.79   0.000    -1.216805   -.6705843\n          5  |   -.222494   .1118964    -1.99   0.047    -.4423696   -.0026184\n          6  |  -.4007454   .1096849    -3.65   0.000    -.6162754   -.1852154\n          7  |  -.6883352   .1225737    -5.62   0.000    -.9291914   -.4474789\n          9  |  -.2376803   .1165492    -2.04   0.042    -.4666986    -.008662\n         10  |  -.3435751   .1106681    -3.10   0.002    -.5610371   -.1261131\n         12  |  -.5649688   .2342601    -2.41   0.016    -1.025288   -.1046496\n         13  |  -.1909402   .1248158    -1.53   0.127    -.4362023    .0543219\n         14  |   -.455463   .2143522    -2.12   0.034    -.8766633   -.0342628\n         15  |  -.4232663   .1481695    -2.86   0.004    -.7144181   -.1321146\n         16  |  -.5371115   .1264239    -4.25   0.000    -.7855334   -.2886895\n         17  |  -.3603291   .1480851    -2.43   0.015    -.6513151   -.0693431\n         18  |  -1.048504   .1680767    -6.24   0.000    -1.378773   -.7182346\n         19  |  -.6236019    .126136    -4.94   0.000    -.8714581   -.3757457\n         20  |  -.5473341   .1259303    -4.35   0.000    -.7947861   -.2998822\n         21  |  -.7002773   .1569423    -4.46   0.000    -1.008668   -.3918869\n         22  |  -.4564266    .124073    -3.68   0.000    -.7002291   -.2126242\n         23  |  -.8079256    .166224    -4.86   0.000    -1.134554   -.4812968\n         24  |  -.4104822   .1151672    -3.56   0.000    -.6367848   -.1841795\n         25  |  -.3723879   .1189486    -3.13   0.002     -.606121   -.1386548\n         26  |  -.5195303   .1469161    -3.54   0.000    -.8082192   -.2308414\n         27  |  -.2435413    .162567    -1.50   0.135    -.5629841    .0759016\n         29  |  -.3157487   .1642662    -1.92   0.055    -.6385305    .0070331\n         30  |    .080972   .1171126     0.69   0.490    -.1491533    .3110972\n         33  |  -.3586822   .2790194    -1.29   0.199    -.9069531    .1895887\n         35  |  -.6745962    .279494    -2.41   0.016      -1.2238   -.1253927\n         36  |  -.3269244   .1273912    -2.57   0.011    -.5772471   -.0766016\n         37  |  -.6471198   .1735604    -3.73   0.000    -.9881646   -.3060751\n         38  |  -.4596228   .1274658    -3.61   0.000    -.7100921   -.2091535\n         39  |  -.4201852    .163985    -2.56   0.011    -.7424145    -.097956\n         40  |  -.5339167   .2840895    -1.88   0.061     -1.09215    .0243168\n         41  |  -.3757769    .122508    -3.07   0.002     -.616504   -.1350497\n         43  |  -1.311499   .2226841    -5.89   0.000    -1.749072   -.8739265\n         44  |  -.2583422   .1631146    -1.58   0.114     -.578861    .0621766\n         45  |  -.1339062   .1198129    -1.12   0.264    -.3693376    .1015253\n         46  |  -1.736985   .2252408    -7.71   0.000    -2.179582   -1.294389\n         47  |  -.6256071   .2058861    -3.04   0.003    -1.030172   -.2210425\n         48  |  -.9859271   .1824752    -5.40   0.000     -1.34449   -.6273647\n         49  |  -.3247521   .1438057    -2.26   0.024     -.607329   -.0421751\n         50  |  -.2738516   .1866851    -1.47   0.143    -.6406863    .0929831\n         51  |  -.6200922   .1274217    -4.87   0.000    -.8704748   -.3697096\n         53  |  -.6259007   .1428582    -4.38   0.000     -.906616   -.3451854\n         54  |  -.5365555   .2812581    -1.91   0.057    -1.089225    .0161143\n         55  |  -.4445126   .1231406    -3.61   0.000    -.6864829   -.2025422\n         56  |   .3443307   .1570771     2.19   0.029     .0356754    .6529859\n         57  |  -.6570438    .114101    -5.76   0.000    -.8812514   -.4328363\n         58  |   -.435127   .1799124    -2.42   0.016    -.7886535   -.0816005\n         59  |   -.628225   .1364025    -4.61   0.000    -.8962548   -.3601951\n         60  |   -.475028   .1440273    -3.30   0.001    -.7580406   -.1920155\n         61  |  -.7261986   .1510088    -4.81   0.000     -1.02293   -.4294675\n         62  |  -.3044535   .1221368    -2.49   0.013    -.5444515   -.0644556\n         63  |   .0037803   .1285151     0.03   0.977    -.2487508    .2563115\n         64  |  -.5483459   .1340122    -4.09   0.000    -.8116787   -.2850131\n         65  |  -.3321434    .141554    -2.35   0.019    -.6102959    -.053991\n         66  |  -.8689924   .1212238    -7.17   0.000    -1.107196   -.6307886\n         67  |  -.3807511   .1543163    -2.47   0.014    -.6839814   -.0775208\n         68  |  -.6198262   .1756128    -3.53   0.000    -.9649039   -.2747485\n         69  |  -.4311974   .1330101    -3.24   0.001    -.6925613   -.1698336\n         70  |   .0557999     .23286     0.24   0.811    -.4017682    .5133679\n         71  |   -.541011   .1282886    -4.22   0.000    -.7930971   -.2889249\n         72  |  -.4935751   .1234947    -4.00   0.000    -.7362413    -.250909\n         73  |  -.6795435   .1474628    -4.61   0.000    -.9693067   -.3897804\n         75  |  -.0205057    .126925    -0.16   0.872    -.2699124     .228901\n         76  |  -.0420092   .2920378    -0.14   0.886    -.6158611    .5318427\n         77  |   -.284598   .1677175    -1.70   0.090    -.6141614    .0449655\n         78  |   .1533827   .1098172     1.40   0.163    -.0624072    .3691726\n         79  |   .4001081   .2748834     1.46   0.146    -.1400355    .9402517\n         80  |   .0210369   .1955429     0.11   0.914    -.3632033     .405277\n         81  |  -.2832579   .2152753    -1.32   0.189    -.7062723    .1397564\n         82  |   .0315568   .1432313     0.22   0.826    -.2498916    .3130052\n         83  |  -.4661398   .1293009    -3.61   0.000     -.720215   -.2120646\n         84  |   .0525394   .2789779     0.19   0.851      -.49565    .6007288\n         85  |    .206903   .1590657     1.30   0.194      -.10566    .5194659\n         86  |  -.7807072   .1429434    -5.46   0.000     -1.06159   -.4998246\n         87  |  -.5240905   .2246382    -2.33   0.020    -.9655027   -.0826783\n         88  |  -.1660402   .2047806    -0.81   0.418    -.5684324    .2363519\n         89  |   .0577245   .2107879     0.27   0.784     -.356472    .4719209\n         91  |   .1126238   .2082437     0.54   0.589    -.2965734     .521821\n         92  |    .488832   .2785641     1.75   0.080    -.0585442    1.036208\n         93  |  -.3047289   .2808968    -1.08   0.279    -.8566889    .2472311\n         94  |  -.2315187   .1288964    -1.80   0.073    -.4847991    .0217617\n         95  |  -.5423296   .1395023    -3.89   0.000    -.8164505   -.2682088\n         96  |  -.0991001   .2025628    -0.49   0.625    -.4971343    .2989342\n         97  |   .0848356   .1679777     0.51   0.614    -.2452392    .4149104\n         98  |  -.0877863   .1183023    -0.74   0.458    -.3202494    .1446767\n         99  |   -.127322   .1556316    -0.82   0.414    -.4331369    .1784928\n------------------------------------------------------------------------------\n\n. restore\ngrade is not estimated in the fixed effects model because it is time-invarying; within each individual it is constant. In the regress model, we are able to estimate it.\nxtreg reports 3 R-squared statistics; this is a within variance model so we can use that value (which agrees with the regression R-squared). Note that the regress R-squared estimate is artificially inflated due to the massive amount of predictors."
  },
  {
    "objectID": "xtsetvsmixed.html#xtreg-be-between-effect-model-between-variance",
    "href": "xtsetvsmixed.html#xtreg-be-between-effect-model-between-variance",
    "title": "Stata’s mixed versus xtreg",
    "section": "xtreg, be: Between Effect model (Between variance)",
    "text": "xtreg, be: Between Effect model (Between variance)\nThe between effects results are\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, be\n\nBetween regression (regression on group means)  Number of obs     =     28,091\nGroup variable: idcode                          Number of groups  =      4,697\n\nR-squared:                                      Obs per group:\n     Within  = 0.1427                                         min =          1\n     Between = 0.4787                                         avg =        6.0\n     Overall = 0.3562                                         max =         15\n\n                                                F(6,4690)         =     717.89\nsd(u_i + avg(e_i.)) = .3068161                  Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |    .064188   .0019539    32.85   0.000     .0603575    .0680185\n         age |  -.0041071   .0010618    -3.87   0.000    -.0061886   -.0020255\n     ttl_exp |   .0287514    .002033    14.14   0.000     .0247658    .0327369\n      tenure |   .0286782   .0022174    12.93   0.000      .024331    .0330254\n    not_smsa |   -.175568   .0111952   -15.68   0.000    -.1975159   -.1536202\n       south |  -.1086271   .0098438   -11.04   0.000    -.1279256   -.0893287\n       _cons |   .8066724   .0329873    24.45   0.000     .7420017     .871343\n------------------------------------------------------------------------------\nTo replicate, collapse over idcode and run a regression:\n. preserve\n\n. collapse (mean) ln_wage grade age ttl_exp tenure not_smsa south, by(idcode)\n\n. reg ln_wage grade age ttl_exp tenure not_smsa south\n\n      Source |       SS           df       MS      Number of obs   =     4,697\n-------------+----------------------------------   F(6, 4690)      =    724.96\n       Model |  406.076398         6  67.6793996   Prob &gt; F        =    0.0000\n    Residual |  437.838178     4,690  .093355688   R-squared       =    0.4812\n-------------+----------------------------------   Adj R-squared   =    0.4805\n       Total |  843.914575     4,696  .179709237   Root MSE        =    .30554\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0642015   .0019467    32.98   0.000     .0603849     .068018\n         age |  -.0039009   .0010611    -3.68   0.000    -.0059812   -.0018206\n     ttl_exp |   .0285569   .0020267    14.09   0.000     .0245836    .0325301\n      tenure |   .0288392   .0022077    13.06   0.000      .024511    .0331673\n    not_smsa |  -.1758847   .0111494   -15.78   0.000    -.1977428   -.1540266\n       south |  -.1080377   .0098055   -11.02   0.000    -.1272611   -.0888142\n       _cons |   .8000955   .0328792    24.33   0.000     .7356368    .8645542\n------------------------------------------------------------------------------\n\n. restore\nAgain, the coefficients agree to three decimals and the between R-square agrees.\nAll predictors here are estimated; if we had any time-variant by individual-invariant predictors (e.g. time), they would not be estimable here."
  },
  {
    "objectID": "xtsetvsmixed.html#xtreg-re-random-effect-model-both-variances",
    "href": "xtsetvsmixed.html#xtreg-re-random-effect-model-both-variances",
    "title": "Stata’s mixed versus xtreg",
    "section": "xtreg, re: Random Effect model (Both variances)",
    "text": "xtreg, re: Random Effect model (Both variances)\nThe random effects results are\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, re\n\nRandom-effects GLS regression                   Number of obs     =     28,091\nGroup variable: idcode                          Number of groups  =      4,697\n\nR-squared:                                      Obs per group:\n     Within  = 0.1483                                         min =          1\n     Between = 0.4701                                         avg =        6.0\n     Overall = 0.3569                                         max =         15\n\n                                                Wald chi2(6)      =    8304.62\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0691836   .0017689    39.11   0.000     .0657166    .0726506\n         age |  -.0038386   .0006544    -5.87   0.000    -.0051212   -.0025559\n     ttl_exp |   .0301313   .0011215    26.87   0.000     .0279331    .0323294\n      tenure |   .0134656   .0008442    15.95   0.000      .011811    .0151202\n    not_smsa |   -.128591   .0072246   -17.80   0.000     -.142751    -.114431\n       south |  -.0932646    .007231   -12.90   0.000     -.107437   -.0790921\n       _cons |   .7544109   .0273445    27.59   0.000     .7008168    .8080051\n-------------+----------------------------------------------------------------\n     sigma_u |  .26027808\n     sigma_e |  .29477563\n         rho |  .43808743   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nJust fit a regular mixed model:\n. mixed ln_wage grade age ttl_exp tenure not_smsa south || idcode:\n\nPerforming EM optimization ...\n\nPerforming gradient-based optimization: \nIteration 0:  Log likelihood = -9218.9773  \nIteration 1:  Log likelihood = -9218.9773  \n\nComputing standard errors ...\n\nMixed-effects ML regression                         Number of obs    =  28,091\nGroup variable: idcode                              Number of groups =   4,697\n                                                    Obs per group:\n                                                                 min =       1\n                                                                 avg =     6.0\n                                                                 max =      15\n                                                    Wald chi2(6)     = 8496.81\nLog likelihood = -9218.9773                         Prob &gt; chi2      =  0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0691186   .0017231    40.11   0.000     .0657414    .0724959\n         age |   -.003869   .0006491    -5.96   0.000    -.0051411   -.0025969\n     ttl_exp |    .030151   .0011135    27.08   0.000     .0279687    .0323334\n      tenure |    .013591   .0008441    16.10   0.000     .0119365    .0152454\n    not_smsa |  -.1299789    .007154   -18.17   0.000    -.1440004   -.1159575\n       south |  -.0941264   .0071291   -13.20   0.000    -.1080991   -.0801537\n       _cons |   .7566548   .0267655    28.27   0.000     .7041954    .8091142\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n  Random-effects parameters  |   Estimate   Std. err.     [95% conf. interval]\n-----------------------------+------------------------------------------------\nidcode: Identity             |\n                  var(_cons) |   .0626522   .0017678      .0592815    .0662147\n-----------------------------+------------------------------------------------\n               var(Residual) |    .087569    .000811      .0859938    .0891732\n------------------------------------------------------------------------------\nLR test vs. linear model: chibar2(01) = 7277.75       Prob &gt;= chibar2 = 0.0000\nThe results are very close; we can get even closer by fitting the xtreg model with the mle option, which uses a different estimation strategy.\n. xtreg ln_wage grade age ttl_exp tenure not_smsa south, re mle\n\nFitting constant-only model:\nIteration 0:  Log likelihood = -12663.954\nIteration 1:  Log likelihood = -12649.756\nIteration 2:  Log likelihood = -12649.614\nIteration 3:  Log likelihood = -12649.614\n\nFitting full model:\nIteration 0:  Log likelihood = -9271.8615\nIteration 1:  Log likelihood = -9219.1214\nIteration 2:  Log likelihood = -9218.9773\nIteration 3:  Log likelihood = -9218.9773\n\nRandom-effects ML regression                        Number of obs    =  28,091\nGroup variable: idcode                              Number of groups =   4,697\n\nRandom effects u_i ~ Gaussian                       Obs per group:\n                                                                 min =       1\n                                                                 avg =     6.0\n                                                                 max =      15\n\n                                                    LR chi2(6)       = 6861.27\nLog likelihood = -9218.9773                         Prob &gt; chi2      =  0.0000\n\n------------------------------------------------------------------------------\n     ln_wage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       grade |   .0691186   .0017233    40.11   0.000     .0657411    .0724962\n         age |   -.003869   .0006491    -5.96   0.000    -.0051413   -.0025967\n     ttl_exp |    .030151   .0011135    27.08   0.000     .0279687    .0323334\n      tenure |    .013591   .0008454    16.08   0.000     .0119341    .0152478\n    not_smsa |  -.1299789   .0071711   -18.13   0.000     -.144034   -.1159239\n       south |  -.0941264   .0071356   -13.19   0.000    -.1081119   -.0801409\n       _cons |   .7566548   .0267773    28.26   0.000     .7041722    .8091374\n-------------+----------------------------------------------------------------\n    /sigma_u |   .2503043   .0035313                      .2434779    .2573221\n    /sigma_e |   .2959207   .0013704                       .293247    .2986188\n         rho |   .4170663   .0074745                      .4024774    .4317704\n------------------------------------------------------------------------------\nLR test of sigma_u=0: chibar2(01) = 7277.75            Prob &gt;= chibar2 = 0.000"
  },
  {
    "objectID": "splinesvsinteraction.html",
    "href": "splinesvsinteraction.html",
    "title": "Splines vs Interaction models",
    "section": "",
    "text": "Linear splines are sometimes used when looking at interrupted time series models. For example, consider the scatter plot below.\nThe slope amongst the red points (x &lt; 5) is clearly different from the slope amongst the blue points (x &gt; 5). The best fit line fails to capture this at all.\nImagine that x is time, and at x = 5, some intervention took place. The goal is to capture the change in slope that occurs after the intervention. One easy approach would be to fit separate pre and post models, and test for equality of coefficients. However, we can also address this with a single model.\nA linear spline model (as fit by Stata’s mkspline) can capture that change in trend. Including an indicator for pre/post even allows a discontinuity at x = 5 instead of the typical continuous spline. However, splines can be harder to interpret and more complicated to work with. This document will demonstrate that an interaction model is equivalent to the linear spline model, and with a simple re-scaling, easier to interpret."
  },
  {
    "objectID": "splinesvsinteraction.html#data-generation",
    "href": "splinesvsinteraction.html#data-generation",
    "title": "Splines vs Interaction models",
    "section": "Data generation",
    "text": "Data generation\nLet’s create a slightly more general data set where there is a “jump” (discontinuity) at intervention in addition to the change in trend.\n. clear\n\n. set obs 100\nNumber of observations (_N) was 0, now 100.\n\n. gen x = runiform(0, 10)\n\n. sort x // To ease plotting later\n\n. gen z = x &gt; 5\n\n. gen y = x + z - x*z + rnormal()\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0), legend(off)\n\nNow there’s a drop of around 4 at the intervention addition to a flattening of the slope."
  },
  {
    "objectID": "splinesvsinteraction.html#spline-model-1---continuous-at-intervention",
    "href": "splinesvsinteraction.html#spline-model-1---continuous-at-intervention",
    "title": "Splines vs Interaction models",
    "section": "Spline Model 1 - Continuous at intervention",
    "text": "Spline Model 1 - Continuous at intervention\nFirst, we’ll predict y using only the splines. This forces a continuity at intervention.\n. reg y x0 x1\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(2, 97)        =      9.48\n       Model |  47.3187332         2  23.6593666   Prob &gt; F        =    0.0002\n    Residual |  242.134186        97  2.49622872   R-squared       =    0.1635\n-------------+----------------------------------   Adj R-squared   =    0.1462\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.5799\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x0 |   .4201304   .1111038     3.78   0.000     .1996201    .6406408\n          x1 |  -.9088959   .2087659    -4.35   0.000    -1.323238   -.4945534\n       _cons |   .7302661   .3768498     1.94   0.056    -.0176763    1.478209\n------------------------------------------------------------------------------\n\n. est store spline1\n\n. predict y_spline1\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_spline1 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_spline1 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\nThe continuity1 at x = 5 makes this a poor fit."
  },
  {
    "objectID": "splinesvsinteraction.html#spline-model-2---discontinuous-at-intervention",
    "href": "splinesvsinteraction.html#spline-model-2---discontinuous-at-intervention",
    "title": "Splines vs Interaction models",
    "section": "Spline Model 2 - Discontinuous at intervention",
    "text": "Spline Model 2 - Discontinuous at intervention\nSimply adding z to the model will allow a discontinuity.\n. reg y x0 x1 z\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(3, 96)        =     38.83\n       Model |  158.673925         3  52.8913084   Prob &gt; F        =    0.0000\n    Residual |  130.778994        96  1.36228119   R-squared       =    0.5482\n-------------+----------------------------------   Adj R-squared   =    0.5341\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.1672\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x0 |   .9983998   .1040552     9.59   0.000      .791852    1.204948\n          x1 |  -.8769945   .1542639    -5.69   0.000    -1.183206   -.5707831\n           z |  -4.057394   .4487716    -9.04   0.000    -4.948199    -3.16659\n       _cons |  -.1003115   .2931596    -0.34   0.733    -.6822287    .4816058\n------------------------------------------------------------------------------\n\n. est store spline2\n\n. predict y_spline2\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_spline2 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_spline2 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\nWe capture the model much better here. Note that the coefficient on x0 is the marginal slope we obtained before and x1 is the difference between the slopes.\nAdditionally (and one of the major benefits that linear spline proponents point to) is that the coefficient on z, -4.06, captures the drop that occurs at x = 5 - in the pre-period, the best fit line is approaching ~5, and in the post-period, the best fit line is approaching ~1."
  },
  {
    "objectID": "splinesvsinteraction.html#without-marginal",
    "href": "splinesvsinteraction.html#without-marginal",
    "title": "Splines vs Interaction models",
    "section": "Without marginal",
    "text": "Without marginal\nLet’s generate the splines without the marginal option to show the results are the same.\n. mkspline x0a 5 x1a = x\n. reg y x0a x1a z\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(3, 96)        =     38.83\n       Model |  158.673925         3  52.8913084   Prob &gt; F        =    0.0000\n    Residual |  130.778994        96  1.36228119   R-squared       =    0.5482\n-------------+----------------------------------   Adj R-squared   =    0.5341\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.1672\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         x0a |   .9983998   .1040552     9.59   0.000      .791852    1.204948\n         x1a |   .1214053   .1138854     1.07   0.289    -.1046554    .3474659\n           z |  -4.057394   .4487716    -9.04   0.000    -4.948199    -3.16659\n       _cons |  -.1003115   .2931596    -0.34   0.733    -.6822287    .4816058\n------------------------------------------------------------------------------\n\n. est store spline3\n\n. predict y_spline3\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_spline3 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_spline3 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\nThe model is identical, but the coefficient on x1a is now the slope in the post period."
  },
  {
    "objectID": "splinesvsinteraction.html#interaction-model-1---continuity-at-intervention",
    "href": "splinesvsinteraction.html#interaction-model-1---continuity-at-intervention",
    "title": "Splines vs Interaction models",
    "section": "Interaction model 1 - Continuity at intervention",
    "text": "Interaction model 1 - Continuity at intervention\nIf we use a version of x which is re-centered around the intervention point (a linear transformation, not affecting the model fit), we can instead obtain a coefficient on the interaction that’s interpretable.\n. gen xc = x - 5\nFirst we’ll fit the model forcing continuity at the intervention. We fit this model by including a main effect for xc, the interaction of xc and z, but crucially, not a main effect for z.\n. reg y c.xc c.xc#i.z\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(2, 97)        =      9.48\n       Model |  47.3187335         2  23.6593667   Prob &gt; F        =    0.0002\n    Residual |  242.134186        97  2.49622872   R-squared       =    0.1635\n-------------+----------------------------------   Adj R-squared   =    0.1462\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.5799\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          xc |   .4201304   .1111038     3.78   0.000     .1996201    .6406408\n             |\n      z#c.xc |\n          1  |  -.9088959   .2087659    -4.35   0.000    -1.323238   -.4945535\n             |\n       _cons |   2.830918   .3037036     9.32   0.000     2.228151    3.433686\n------------------------------------------------------------------------------\n\n. est store int1\n\n. predict y_int1\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_int1 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_int1 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\n. est table spline1 int1\n\n----------------------------------------\n    Variable |  spline1        int1     \n-------------+--------------------------\n          x0 |  .42013044               \n          x1 | -.90889588               \n          xc |               .42013044  \n             |\n      z#c.xc |\n          1  |              -.90889589  \n             |\n       _cons |   .7302661    2.8309183  \n----------------------------------------\nAs you can see, we get identical results. (The y-intercept differs - in the spline model, it is the value estimated when x = 0; in the interaction model, it is the value estimated when x approaches 5 from the left.)"
  },
  {
    "objectID": "splinesvsinteraction.html#interaction-model-2---discontinuous-at-intervention",
    "href": "splinesvsinteraction.html#interaction-model-2---discontinuous-at-intervention",
    "title": "Splines vs Interaction models",
    "section": "Interaction Model 2 - Discontinuous at intervention",
    "text": "Interaction Model 2 - Discontinuous at intervention\nNow, relax the continuity assumption.\n. reg y c.xc##i.z\n\n      Source |       SS           df       MS      Number of obs   =       100\n-------------+----------------------------------   F(3, 96)        =     38.83\n       Model |  158.673926         3  52.8913088   Prob &gt; F        =    0.0000\n    Residual |  130.778993        96  1.36228118   R-squared       =    0.5482\n-------------+----------------------------------   Adj R-squared   =    0.5341\n       Total |  289.452919        99  2.92376686   Root MSE        =    1.1672\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          xc |   .9983998   .1040552     9.59   0.000      .791852    1.204948\n         1.z |  -4.057394   .4487716    -9.04   0.000    -4.948199    -3.16659\n             |\n      z#c.xc |\n          1  |  -.8769945   .1542639    -5.69   0.000    -1.183206   -.5707831\n             |\n       _cons |   4.891688    .319828    15.29   0.000     4.256834    5.526541\n------------------------------------------------------------------------------\n\n. est store int2\n\n. predict y_int2\n(option xb assumed; fitted values)\n\n. twoway (scatter y x if z == 1) (scatter y x if z == 0) ///\n&gt;        (line y_int2 x if z == 1, lcolor(navy)) ///\n&gt;        (line y_int2 x if z == 0, lcolor(maroon)), ///\n&gt;          legend(off)\n\n. est table spline2 int2\n\n----------------------------------------\n    Variable |  spline2        int2     \n-------------+--------------------------\n          x0 |  .99839981               \n          x1 | -.87699452               \n           z | -4.0573944               \n          xc |               .99839982  \n             |\n           z |\n          1  |              -4.0573945  \n             |\n      z#c.xc |\n          1  |              -.87699454  \n             |\n       _cons | -.10031147    4.8916876  \n----------------------------------------\nAgain, we get the same results."
  },
  {
    "objectID": "splinesvsinteraction.html#obtaining-both-slopes",
    "href": "splinesvsinteraction.html#obtaining-both-slopes",
    "title": "Splines vs Interaction models",
    "section": "Obtaining both slopes",
    "text": "Obtaining both slopes\nAs mentioned before, the one downside of the interaction model is that we don’t directly get the post-slope, instead obtaining the pre-slope and and the difference in slopes. This is easily remedied:\n. margins z, dydx(xc)\n\nAverage marginal effects                                   Number of obs = 100\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  xc\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxc           |\n           z |\n          0  |   .9983998   .1040552     9.59   0.000      .791852    1.204948\n          1  |   .1214053   .1138854     1.07   0.289    -.1046554    .3474659\n------------------------------------------------------------------------------\nOnce again, agreeing with the slopes obtained before of 0.998 and 0.121."
  },
  {
    "objectID": "splinesvsinteraction.html#footnotes",
    "href": "splinesvsinteraction.html#footnotes",
    "title": "Splines vs Interaction models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe visual discontinity is due the way the plot is generated and is not real.↩︎"
  },
  {
    "objectID": "iccirr.html",
    "href": "iccirr.html",
    "title": "Inter-rater Reliability",
    "section": "",
    "text": "A few notes on agreement between raters."
  },
  {
    "objectID": "iccirr.html#sources",
    "href": "iccirr.html#sources",
    "title": "Inter-rater Reliability",
    "section": "Sources",
    "text": "Sources\n\nhttps://en.wikipedia.org/wiki/Cohen%27s_kappa\nhttp://john-uebersax.com/stat/kappa.htm\nhttp://www.stata.com/manuals14/rkappa.pdf"
  },
  {
    "objectID": "iccirr.html#icc-in-r",
    "href": "iccirr.html#icc-in-r",
    "title": "Inter-rater Reliability",
    "section": "ICC in R",
    "text": "ICC in R\nUse the Orthodont data from nlme as our example. Look at distance measurements and look at correlation by Subject.\n\nlibrary(nlme)\nlibrary(lme4)\ndata(Orthodont)\n\n\nWith nlme\nUsing the nlme package, we fit the model:\n\nfm1 &lt;- lme(distance ~ 1, random = ~ 1 | Subject, data = Orthodont)\nsummary(fm1)\n\nLinear mixed-effects model fit by REML\n  Data: Orthodont \n       AIC      BIC    logLik\n  521.3618 529.3803 -257.6809\n\nRandom effects:\n Formula: ~1 | Subject\n        (Intercept) Residual\nStdDev:    1.937002 2.220312\n\nFixed effects:  distance ~ 1 \n               Value Std.Error DF  t-value p-value\n(Intercept) 24.02315 0.4296606 81 55.91192       0\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-3.2400448 -0.5277439 -0.1072888  0.4731815  2.7687301 \n\nNumber of Observations: 108\nNumber of Groups: 27 \n\n\nThe between-effect standard deviation is reported as the Residual StdDev. To obtain the ICC, we compute each \\(\\sigma\\):\n\ns2w &lt;- getVarCov(fm1)[[1]]\ns2b &lt;- fm1$s^2\nc(sigma2_w = s2w, sigma2_b = s2b, icc = s2w/(s2w + s2b))\n\n sigma2_w  sigma2_b       icc \n3.7519762 4.9297832 0.4321677 \n\n\n\n\nWith lme4\nUsing the lme4 package, we fit the model:\n\nfm2 &lt;- lmer(distance ~ (1 | Subject), data = Orthodont)\nsummary(fm2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: distance ~ (1 | Subject)\n   Data: Orthodont\n\nREML criterion at convergence: 515.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2400 -0.5277 -0.1073  0.4732  2.7687 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 3.752    1.937   \n Residual             4.930    2.220   \nNumber of obs: 108, groups:  Subject, 27\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  24.0231     0.4297   55.91\n\n\nThe Variance column of the Random Effects table gives the within-subject (Subject) and between-subject (Residual) variances.\n\ns2w &lt;- summary(fm2)$varcor$Subject[1]\ns2b &lt;- summary(fm2)$sigma^2\nc(sigma2_w = s2w, sigma2_b = s2b, icc = s2w/(s2w + s2b))\n\n sigma2_w  sigma2_b       icc \n3.7519736 4.9297839 0.4321675"
  },
  {
    "objectID": "iccirr.html#sources-1",
    "href": "iccirr.html#sources-1",
    "title": "Inter-rater Reliability",
    "section": "Sources",
    "text": "Sources\n\nhttps://en.wikipedia.org/wiki/Intraclass_correlation\nhttp://stats.stackexchange.com/questions/14976/intraclass-correlation-coefficients-icc-with-multiple-variables\nhttp://john-uebersax.com/stat/icc.htm"
  },
  {
    "objectID": "randomselection.html",
    "href": "randomselection.html",
    "title": "Choosing a Random Sample",
    "section": "",
    "text": "A common task is to select a random subset of rows from your data set. This document discusses an easy way to do this, including sampling from subsamples."
  },
  {
    "objectID": "randomselection.html#data",
    "href": "randomselection.html#data",
    "title": "Choosing a Random Sample",
    "section": "Data",
    "text": "Data\nWe’ll load up the “auto” data set and shrink it down substantially in order to be able to print out the results.\n. sysuse auto\n(1978 automobile data)\n\n. keep make foreign\n\n. bysort foreign: gen row = _n\n\n. keep if row &lt;= 4\n(66 observations deleted)\n\n. drop row\n\n. list, sep(0)\n\n     +--------------------------+\n     | make             foreign |\n     |--------------------------|\n  1. | AMC Concord     Domestic |\n  2. | AMC Pacer       Domestic |\n  3. | AMC Spirit      Domestic |\n  4. | Buick Century   Domestic |\n  5. | Audi 5000        Foreign |\n  6. | Audi Fox         Foreign |\n  7. | BMW 320i         Foreign |\n  8. | Datsun 200       Foreign |\n     +--------------------------+"
  },
  {
    "objectID": "randomselection.html#simple-random-sample",
    "href": "randomselection.html#simple-random-sample",
    "title": "Choosing a Random Sample",
    "section": "Simple Random Sample",
    "text": "Simple Random Sample\nLet’s say we want to select 4 rows, as a simple random sample. That is, the probability of any row being included in the sample is equal.\nFirst, we’ll generate a random number per row. You can use any distribution you want; uniform or normal are common.\n. generate rand = rnormal()\n\n. list, sep(0)\n\n     +--------------------------------------+\n     | make             foreign        rand |\n     |--------------------------------------|\n  1. | AMC Concord     Domestic   -.4705035 |\n  2. | AMC Pacer       Domestic   -.3938664 |\n  3. | AMC Spirit      Domestic   -.2524172 |\n  4. | Buick Century   Domestic   -1.404408 |\n  5. | Audi 5000        Foreign   -.8082101 |\n  6. | Audi Fox         Foreign   -.0387205 |\n  7. | BMW 320i         Foreign    1.185362 |\n  8. | Datsun 200       Foreign   -.2958094 |\n     +--------------------------------------+\nrnormal takes in 2 optional arguments of a mean and standard deviation; the defaults are 0 and 1 respectively.\nIf you prefer uniform, you call generate rand = runiform(a, b) where a and b are upper and lower bounds, e.g. generate rand = runiform(0, 1).\nNow we simply sort by this new variable.\n. sort rand\n\n. list, sep(0)\n\n     +--------------------------------------+\n     | make             foreign        rand |\n     |--------------------------------------|\n  1. | Buick Century   Domestic   -1.404408 |\n  2. | Audi 5000        Foreign   -.8082101 |\n  3. | AMC Concord     Domestic   -.4705035 |\n  4. | AMC Pacer       Domestic   -.3938664 |\n  5. | Datsun 200       Foreign   -.2958094 |\n  6. | AMC Spirit      Domestic   -.2524172 |\n  7. | Audi Fox         Foreign   -.0387205 |\n  8. | BMW 320i         Foreign    1.185362 |\n     +--------------------------------------+\nFinally, we can identify our sample.\n. gen insample = _n &lt;= 4\n\n. list, sep(0)\n\n     +-------------------------------------------------+\n     | make             foreign        rand   insample |\n     |-------------------------------------------------|\n  1. | Buick Century   Domestic   -1.404408          1 |\n  2. | Audi 5000        Foreign   -.8082101          1 |\n  3. | AMC Concord     Domestic   -.4705035          1 |\n  4. | AMC Pacer       Domestic   -.3938664          1 |\n  5. | Datsun 200       Foreign   -.2958094          0 |\n  6. | AMC Spirit      Domestic   -.2524172          0 |\n  7. | Audi Fox         Foreign   -.0387205          0 |\n  8. | BMW 320i         Foreign    1.185362          0 |\n     +-------------------------------------------------+\nRecall that _n refers to the current row number, so this is just flagging all rows 4 and below!"
  },
  {
    "objectID": "randomselection.html#sample-by-subgroup",
    "href": "randomselection.html#sample-by-subgroup",
    "title": "Choosing a Random Sample",
    "section": "Sample by Subgroup",
    "text": "Sample by Subgroup\nConsider the sample we obtained above, and notice that we sampled 3 domestic cars and 1 foreign car. Since it was a simple random sample, that split is random; we could have just as easily obtained all foreign cars or any other combination. Perhaps we want to force some balance, for example, that our random sample is exactly 2 foreign and 2 domestic.\nWe’ll generate a new random number first just as before.\n. drop rand insample\n\n. generate rand = rnormal()\n\n. list, sep(0)\n\n     +--------------------------------------+\n     | make             foreign        rand |\n     |--------------------------------------|\n  1. | Buick Century   Domestic   -1.179235 |\n  2. | Audi 5000        Foreign    1.503948 |\n  3. | AMC Concord     Domestic    .0767283 |\n  4. | AMC Pacer       Domestic    -.627642 |\n  5. | Datsun 200       Foreign   -1.122534 |\n  6. | AMC Spirit      Domestic   -1.491838 |\n  7. | Audi Fox         Foreign    .0291835 |\n  8. | BMW 320i         Foreign   -.7714012 |\n     +--------------------------------------+\nNow when we sort, we’ll sort by foreign first.\n. sort foreign rand\n\n. list, sep(0)\n\n     +--------------------------------------+\n     | make             foreign        rand |\n     |--------------------------------------|\n  1. | AMC Spirit      Domestic   -1.491838 |\n  2. | Buick Century   Domestic   -1.179235 |\n  3. | AMC Pacer       Domestic    -.627642 |\n  4. | AMC Concord     Domestic    .0767283 |\n  5. | Datsun 200       Foreign   -1.122534 |\n  6. | BMW 320i         Foreign   -.7714012 |\n  7. | Audi Fox         Foreign    .0291835 |\n  8. | Audi 5000        Foreign    1.503948 |\n     +--------------------------------------+\nSo we have two separate randomly sorted lists here. To select a fixed number from each, we can use the bysort prefix.\n. bysort foreign (rand): gen rownumber = _n\n\n. gen insample = rownumber &lt;= 2\n\n. list, sep(0)\n\n     +------------------------------------------------------------+\n     | make             foreign        rand   rownum~r   insample |\n     |------------------------------------------------------------|\n  1. | AMC Spirit      Domestic   -1.491838          1          1 |\n  2. | Buick Century   Domestic   -1.179235          2          1 |\n  3. | AMC Pacer       Domestic    -.627642          3          0 |\n  4. | AMC Concord     Domestic    .0767283          4          0 |\n  5. | Datsun 200       Foreign   -1.122534          1          1 |\n  6. | BMW 320i         Foreign   -.7714012          2          1 |\n  7. | Audi Fox         Foreign    .0291835          3          0 |\n  8. | Audi 5000        Foreign    1.503948          4          0 |\n     +------------------------------------------------------------+\n(Recall that when calling bysort, any argument in parentheses is used for sorting, not for by’ing. Since I sorted by foreign and rand above I probably could have just used the prefix by foreign:, however, I prefer always using bysort with full sorting just to avoid any issues.)\nWe could have also enforced an unequal split in foreign:\n. gen insample2 = rownumber &lt;= 3 if foreign == 0\n(4 missing values generated)\n\n. replace insample2 = rownumber &lt;= 1 if foreign == 1\n(4 real changes made)\n\n. list, sep(0)\n\n     +-----------------------------------------------------------------------+\n     | make             foreign        rand   rownum~r   insample   insamp~2 |\n     |-----------------------------------------------------------------------|\n  1. | AMC Spirit      Domestic   -1.491838          1          1          1 |\n  2. | Buick Century   Domestic   -1.179235          2          1          1 |\n  3. | AMC Pacer       Domestic    -.627642          3          0          1 |\n  4. | AMC Concord     Domestic    .0767283          4          0          0 |\n  5. | Datsun 200       Foreign   -1.122534          1          1          1 |\n  6. | BMW 320i         Foreign   -.7714012          2          1          0 |\n  7. | Audi Fox         Foreign    .0291835          3          0          0 |\n  8. | Audi 5000        Foreign    1.503948          4          0          0 |\n     +-----------------------------------------------------------------------+"
  },
  {
    "objectID": "svy_gsem_teffects.html",
    "href": "svy_gsem_teffects.html",
    "title": "Mediation with svyset data",
    "section": "",
    "text": "Stata Versioning\n\n\n\nCurrent versions of Stata support svy: sem and estat teffects without complaint. There is no longer a need to use gsem and manual calculation. However, this document may still be useful as either a guide to using estat teffects or manually calculating them if you need to use gsem for another reason.\n\n\nMediation models in Stata are fit with the sem command. sem does not support svyset data, so instead you use gsem (e.g. svy: gsem ...). However, gsem does not support estat teffects which calculates direct, indirect and total effects.\nThis document shows how to manually calculate these effects using nlcom.\nNote that this is a case where all variables are continuous and all models are linear - we are only using gsem for it’s support of svy:, not it’s support of GLMs. Indirect effects are a more complicated topic in those models which we do not address here. Additionally, we’ll trust Stata to compute standard errors rather than getting into any sticky issues of bootstrapping.\n\nStandard Mediation\nFirst, let’s estimate the direct, indirect and total effects without the use of the survey design to show equivalence.\n. webuse gsem_multmed\n(Fictional job-performance data)\nThe model we’ll be fitting is\n\n\n\nmediation\n\n\nHere, “satis” is a potential mediator between “support” and “perform”. The direct effect is the arrow between “support” and “perform”, the indirect effect is the arrows from “support” to “perform” which passes through “satis”, and the total effect is the sum of the direct and indirect effects.\n. sem (perform &lt;- satis support) (satis &lt;- support)\n\nEndogenous variables\n  Observed: perform satis\n\nExogenous variables\n  Observed: support\n\nFitting target model:\nIteration 0:  Log likelihood = -3779.9224  \nIteration 1:  Log likelihood = -3779.9224  \n\nStructural equation model                                Number of obs = 1,500\nEstimation method: ml\n\nLog likelihood = -3779.9224\n\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .6161077   .0303143    20.32   0.000     .5566927    .6755227\n       _cons |   4.981054   .0150589   330.77   0.000     4.951539    5.010569\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n       _cons |    .019262   .0154273     1.25   0.212    -.0109749    .0494989\n-------------+----------------------------------------------------------------\nvar(e.perf~m)|   .3397087   .0124044                      .3162461     .364912\n var(e.satis)|   .3569007   .0130322                      .3322507    .3833795\n------------------------------------------------------------------------------\nLR test of model vs. saturated: chi2(0) = 0.00                 Prob &gt; chi2 = .\nThe direct, indirect and total effects can be estimated via estat teffects.\n. estat teffects\n\n\nDirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .6161077   .0303143    20.32   0.000     .5566927    .6755227\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n------------------------------------------------------------------------------\n\n\nIndirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |          0  (no path)\n     support |    .205648   .0280066     7.34   0.000      .150756      .26054\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |          0  (no path)\n------------------------------------------------------------------------------\n\n\nTotal effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .8217557   .0404579    20.31   0.000     .7424597    .9010516\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n------------------------------------------------------------------------------\nLet’s calculate them manually. First we’ll re-display the SEM results with the coeflegend to obtain the names to access the coefficients.\n. sem, coeflegend\n\nStructural equation model                                Number of obs = 1,500\nEstimation method: ml\n\nLog likelihood = -3779.9224\n\n------------------------------------------------------------------------------\n             | Coefficient  Legend\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401  _b[perform:satis]\n     support |   .6161077  _b[perform:support]\n       _cons |   4.981054  _b[perform:_cons]\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945  _b[satis:support]\n       _cons |    .019262  _b[satis:_cons]\n-------------+----------------------------------------------------------------\nvar(e.perf~m)|   .3397087  _b[/var(e.perform)]\n var(e.satis)|   .3569007  _b[/var(e.satis)]\n------------------------------------------------------------------------------\nLR test of model vs. saturated: chi2(0) = 0.00                 Prob &gt; chi2 = .\nThe main effects are directly from the model, but for completeness let’s obtain it.\n. estat teffects, noindirect nototal\n\n\nDirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .6161077   .0303143    20.32   0.000     .5566927    .6755227\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n------------------------------------------------------------------------------\n\n. nlcom _b[perform:support]\n\n       _nl_1: _b[perform:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .6161077   .0303143    20.32   0.000     .5566927    .6755227\n------------------------------------------------------------------------------\nFor the indirect effect, we’ll simply multiply the path from “support” to “satis” and from “satis” to “perform”.\n. estat teffects, nodirect nototal\n\n\nIndirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |          0  (no path)\n     support |    .205648   .0280066     7.34   0.000      .150756      .26054\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |          0  (no path)\n------------------------------------------------------------------------------\n\n. nlcom _b[perform:satis]*_b[satis:support]\n\n       _nl_1: _b[perform:satis]*_b[satis:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |    .205648   .0280066     7.34   0.000      .150756      .26054\n------------------------------------------------------------------------------\nFinally, we can sum those for the direct effect.\n. estat teffects, nodirect noindirect\n\n\nTotal effects\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  perform    |\n       satis |   .8984401   .0251903    35.67   0.000      .849068    .9478123\n     support |   .8217557   .0404579    20.31   0.000     .7424597    .9010516\n  -----------+----------------------------------------------------------------\n  satis      |\n     support |   .2288945   .0305047     7.50   0.000     .1691064    .2886826\n------------------------------------------------------------------------------\n\n. nlcom _b[perform:satis]*_b[satis:support] + _b[perform:support]\n\n       _nl_1: _b[perform:satis]*_b[satis:support] + _b[perform:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .8217557   .0404579    20.31   0.000     .7424597    .9010516\n------------------------------------------------------------------------------\n\n\nWith survey data\nWe’ll reproduce the above results with survey set data. The actually svyset here is nonsense, this test data is not actual survey data.\n. svyset branch [pweight = perform]\n\nSampling weights: perform\n             VCE: linearized\n     Single unit: missing\n        Strata 1: &lt;one&gt;\n Sampling unit 1: branch\n           FPC 1: &lt;zero&gt;\nTo use the svy prefix, we switch from sem to gsem.\n. svy: gsem (perform &lt;- satis support) (satis &lt;- support)\n(running gsem on estimation sample)\n\nSurvey: Generalized structural equation model\n\nNumber of strata =  1                              Number of obs   =     1,500\nNumber of PSUs   = 75                              Population size = 7,507.976\n                                                   Design df       =        74\n\nResponse: perform \nFamily:   Gaussian\nLink:     Identity\n\nResponse: satis   \nFamily:   Gaussian\nLink:     Identity\n\n------------------------------------------------------------------------------\n             |             Linearized\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nperform      |\n       satis |   .8768337   .0478296    18.33   0.000      .781531    .9721363\n     support |   .6105411   .0276199    22.11   0.000     .5555072     .665575\n       _cons |   5.051254   .0414936   121.74   0.000     4.968576    5.133932\n-------------+----------------------------------------------------------------\nsatis        |\n     support |    .212986   .0261195     8.15   0.000     .1609418    .2650301\n       _cons |   .0841272   .0583215     1.44   0.153     -.032081    .2003353\n-------------+----------------------------------------------------------------\nvar(e.perf~m)|   .3284831   .0241911                      .2836511    .3804009\n var(e.satis)|   .3570689   .0353162                      .2931998    .4348508\n------------------------------------------------------------------------------\nFinally, all three effects can be calculated via the same nlcom commands.\n. * main effect\n. nlcom _b[perform:support]\n\n       _nl_1: _b[perform:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .6105411   .0276199    22.11   0.000      .556407    .6646751\n------------------------------------------------------------------------------\n\n. \n. * Indirect effect\n. nlcom _b[perform:satis]*_b[satis:support]\n\n       _nl_1: _b[perform:satis]*_b[satis:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .1867533   .0253151     7.38   0.000     .1371366    .2363699\n------------------------------------------------------------------------------\n\n. \n. * Total effect\n. nlcom _b[perform:satis]*_b[satis:support] + _b[perform:support]\n\n       _nl_1: _b[perform:satis]*_b[satis:support] + _b[perform:support]\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   .7972943   .0308642    25.83   0.000     .7368017     .857787\n------------------------------------------------------------------------------\n\n."
  }
]