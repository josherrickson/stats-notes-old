---
title: "Visualizing the issue with Collinear Predictors"
author: "Josh Errickson"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

# Introduction

While there are many resources out there to describe the issues arising with
multicollinearity in independent variables, there's a visualization for the
problem that I came across in a book once and haven't found replicated online.
This replicates the visualization.

# Set-up

Let $Y$ be a response and $X_1$ and $X_2$ be the predictors, such that

$$
    Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \epsilon_i
$$

for individual $i$.

For simplicity, let's say that

$$
    \beta_0 = 0,\\
    \beta_1 = 1,\\
    \beta_2 = 1.
$$

I carry out a simulation by generating 1,000 simulations with a given
correlation between predictors and obtain their coefficients.

```{r}
#| eval: false
reps <- 1000
n <- 100
save <- matrix(nrow = reps, ncol = 3)

for (i in 1:reps) {
  x1 <- rnorm(n)
  x2 <- rnorm(n)
  y <- x1 + x2 + rnorm(n)

  mod <- lm(y ~ x1 + x2)
  save[i,] <- c(coef(mod)[-1], cor(x1, x2))
}
```

The line `x2 <- rnorm(n)` gets replaced with `x2 <- x1 + rnorm(n, sd = _)`,
where the `_` is replaced with difference values to induce more correlation
between `x1` and `x2`.

Following these simulations, the coefficients are plotted against each out.

# Simulation

::: {.panel-tabset}

## No Collinearity

```{r}
#| echo: false
reps <- 1000
n <- 100
save <- matrix(nrow = reps, ncol = 3)

for (i in 1:reps) {
  x1 <- rnorm(n)
  x2 <- rnorm(n)
  y <- x1 + x2 + rnorm(n)

  mod <- lm(y ~ x1 + x2)
  save[i,] <- c(coef(mod)[-1], cor(x1, x2))
}
save <- data.frame(save)
names(save) <- c("b_1", "b_2")
```

The average correlation between $X_1$ and $X_2$ is `r round(mean(save[,3]), 3)`.

```{r}
#| echo: false
#| fig.height: 7
#| fig.width: 7
ggplot(save, aes(x = b_1, y = b_2)) +
  geom_point() +
  geom_point(aes(x = 1, y = 1), col = "red", size = 3) +
  ylab(bquote(hat(beta)[2])) +
  xlab(bquote(hat(beta)[1])) +
  theme(text = element_text(size=15))
```

The red dot represents the true coefficients. We see no relationship between the
estimated coefficients, and each are well centered around the truth.

## Moderate Collinearity

```{r, echo = FALSE}
reps <- 1000
n <- 100
save <- matrix(nrow = reps, ncol = 3)

for (i in 1:reps) {
  x1 <- rnorm(n)
  x2 <- x1 + rnorm(n, sd = 1.5)
  y <- x1 + x2 + rnorm(n)

  mod <- lm(y ~ x1 + x2)
  save[i,] <- c(coef(mod)[-1], cor(x1, x2))
}
save <- data.frame(save)
names(save) <- c("b_1", "b_2")
```

The average correlation between $X_1$ and $X_2$ is `r round(mean(save[,3]), 3)`.

```{r, echo = FALSE, fig.height = 7, fig.width = 7}
ggplot(save, aes(x = b_1, y = b_2)) +
  geom_point() +
  geom_point(aes(x = 1, y = 1), col = "red", size = 3) +
  ylab(bquote(hat(beta)[2])) +
  xlab(bquote(hat(beta)[1])) +
  theme(text = element_text(size=15))
```

While there is some semblance of a relationship, it is not very strong.

## High Collinearity

```{r, echo = FALSE}
reps <- 1000
n <- 100
save <- matrix(nrow = reps, ncol = 3)

for (i in 1:reps) {
  x1 <- rnorm(n)
  x2 <- x1 + rnorm(n, sd = .5)
  y <- x1 + x2 + rnorm(n)

  mod <- lm(y ~ x1 + x2)
  save[i,] <- c(coef(mod)[-1], cor(x1, x2))
}
save <- data.frame(save)
names(save) <- c("b_1", "b_2")
```

The average correlation between $X_1$ and $X_2$ is `r round(mean(save[,3]), 3)`.

```{r, echo = FALSE, fig.height = 7, fig.width = 7}
ggplot(save, aes(x = b_1, y = b_2)) +
  geom_point() +
  geom_point(aes(x = 1, y = 1), col = "red", size = 3) +
  ylab(bquote(hat(beta)[2])) +
  xlab(bquote(hat(beta)[1])) +
  theme(text = element_text(size=15))
```

There's a strong relationship now, such that if one coefficient is incorrectly
high, the other is incorrectly low.

## Extremely High Collinearity

```{r, echo = FALSE}
reps <- 1000
n <- 100
save <- matrix(nrow = reps, ncol = 3)

for (i in 1:reps) {
  x1 <- rnorm(n)
  x2 <- x1 + rnorm(n, sd = .1)
  y <- x1 + x2 + rnorm(n)

  mod <- lm(y ~ x1 + x2)
  save[i,] <- c(coef(mod)[-1], cor(x1, x2))
}
save <- data.frame(save)
names(save) <- c("b_1", "b_2")
```

The average correlation between $X_1$ and $X_2$ is `r round(mean(save[,3]), 3)`.

```{r, echo = FALSE, fig.height = 7, fig.width = 7}
ggplot(save, aes(x = b_1, y = b_2)) +
  geom_point() +
  geom_point(aes(x = 1, y = 1), col = "red", size = 3) +
  ylab(bquote(hat(beta)[2])) +
  xlab(bquote(hat(beta)[1])) +
  theme(text = element_text(size=15))
```

We now see a very strong relationship between the coefficients.
:::

# Why is this a problem?

Consider the "extremely high correlation" results. With such high correlation,
essentially $X_1 = X_2$, which is represented by the red line (slope of -1). We
can approximate our model:

\begin{aligned}
  Y_i &= \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \epsilon_i\\
  &\approx \beta_0 + (\beta_1 + \beta_2)X_{1i} + \epsilon_i\\
  &\approx \beta_0 + (\beta_1 + \beta_2)X_{2i} + \epsilon_i
\end{aligned}

In other words, the model has that $\beta_1 + \beta_2 = 2$ (since we assumed
above that both coefficients have values of 1.. So while all of those models
would have the same predictive power for $Y$, they would have drastically
different interpretations depending on where along that red line they fall.
